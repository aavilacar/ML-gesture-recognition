{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCZBFzjClURz"
      },
      "source": [
        "# Train of a TensorFlow Lite model for Microcontrollers\n",
        "\n",
        "This notebook demonstrates the process of training a model using TensorFlow and converting it for use with TensorFlow Lite for microcontrollers. The application is gesture recognition using features extracted from sEMG signals."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UQblnrLd_ET"
      },
      "source": [
        "## Configure Defaults"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5PYwRFppd-WB"
      },
      "outputs": [],
      "source": [
        "# Define paths to model files\n",
        "import os\n",
        "MODELS_DIR = 'models/'\n",
        "if not os.path.exists(MODELS_DIR):\n",
        "    os.mkdir(MODELS_DIR)\n",
        "MODEL_TF = MODELS_DIR + 'model'\n",
        "MODEL_NO_QUANT_TFLITE = MODELS_DIR + 'model_no_quant.tflite'\n",
        "MODEL_TFLITE = MODELS_DIR + 'model.tflite'\n",
        "MODEL_TFLITE_MICRO = MODELS_DIR + 'model.cc'\n",
        "\n",
        "# Defining path to save the entire keras model to a HDF5 file (for size \n",
        "# comparison with the TensorFlow Lite models later)\n",
        "MODEL_TF_H5 = MODELS_DIR + 'model.h5'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dh4AXGuHWeu1"
      },
      "source": [
        "## Setup Environment\n",
        "\n",
        "Install Dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cr1VLfotanf6",
        "outputId": "8414541c-cb16-49c3-9e2b-9dabcf172cc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==2.4.0\n",
            "  Downloading tensorflow-2.4.0-cp37-cp37m-manylinux2010_x86_64.whl (394.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 394.7 MB 18 kB/s \n",
            "\u001b[?25hCollecting absl-py~=0.10\n",
            "  Downloading absl_py-0.15.0-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 63.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (1.6.3)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (0.37.1)\n",
            "Collecting tensorflow-estimator<2.5.0,>=2.4.0rc0\n",
            "  Downloading tensorflow_estimator-2.4.0-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |████████████████████████████████| 462 kB 42.4 MB/s \n",
            "\u001b[?25hCollecting wrapt~=1.12.1\n",
            "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
            "Collecting grpcio~=1.32.0\n",
            "  Downloading grpcio-1.32.0-cp37-cp37m-manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 48.6 MB/s \n",
            "\u001b[?25hCollecting gast==0.3.3\n",
            "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (3.17.3)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (0.2.0)\n",
            "Collecting h5py~=2.10.0\n",
            "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 42.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (1.1.2)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (1.1.0)\n",
            "Collecting typing-extensions~=3.7.4\n",
            "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
            "Collecting flatbuffers~=1.12.0\n",
            "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (1.15.0)\n",
            "Collecting numpy~=1.19.2\n",
            "  Downloading numpy-1.19.5-cp37-cp37m-manylinux2010_x86_64.whl (14.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.8 MB 39.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (3.3.0)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (2.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (3.3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (1.35.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (0.6.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.0) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4.0) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4.0) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.0) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.0) (3.2.0)\n",
            "Building wheels for collected packages: wrapt\n",
            "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wrapt: filename=wrapt-1.12.1-cp37-cp37m-linux_x86_64.whl size=68720 sha256=23e185fa87ea61d0dfa66b24266e61e382ad74bd280b58728dc50ba533c123ef\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/76/4c/aa25851149f3f6d9785f6c869387ad82b3fd37582fa8147ac6\n",
            "Successfully built wrapt\n",
            "Installing collected packages: typing-extensions, numpy, grpcio, absl-py, wrapt, tensorflow-estimator, h5py, gast, flatbuffers, tensorflow\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 4.2.0\n",
            "    Uninstalling typing-extensions-4.2.0:\n",
            "      Successfully uninstalled typing-extensions-4.2.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.44.0\n",
            "    Uninstalling grpcio-1.44.0:\n",
            "      Successfully uninstalled grpcio-1.44.0\n",
            "  Attempting uninstall: absl-py\n",
            "    Found existing installation: absl-py 1.0.0\n",
            "    Uninstalling absl-py-1.0.0:\n",
            "      Successfully uninstalled absl-py-1.0.0\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.14.0\n",
            "    Uninstalling wrapt-1.14.0:\n",
            "      Successfully uninstalled wrapt-1.14.0\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.3\n",
            "    Uninstalling gast-0.5.3:\n",
            "      Successfully uninstalled gast-0.5.3\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 2.0\n",
            "    Uninstalling flatbuffers-2.0:\n",
            "      Successfully uninstalled flatbuffers-2.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.0\n",
            "    Uninstalling tensorflow-2.8.0:\n",
            "      Successfully uninstalled tensorflow-2.8.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed absl-py-0.15.0 flatbuffers-1.12 gast-0.3.3 grpcio-1.32.0 h5py-2.10.0 numpy-1.19.5 tensorflow-2.4.0 tensorflow-estimator-2.4.0 typing-extensions-3.7.4.3 wrapt-1.12.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "! pip install tensorflow==2.4.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tx9lOPWh9grN"
      },
      "source": [
        "Import Dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53PBJBv1jEtJ"
      },
      "outputs": [],
      "source": [
        "# TensorFlow is an open source machine learning library\n",
        "import tensorflow as tf\n",
        "\n",
        "# Keras is TensorFlow's high-level API for deep learning\n",
        "from tensorflow import keras\n",
        "# Numpy is a math library\n",
        "import numpy as np\n",
        "# Pandas is a data manipulation library \n",
        "import pandas as pd\n",
        "# Matplotlib is a graphing library\n",
        "import matplotlib.pyplot as plt\n",
        "# Math is Python's math library\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-PuBEb6CMeo"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gB0-dlNmLT-"
      },
      "source": [
        "### 1. Import and Process Data\n",
        "\n",
        "In order to train a ML model, it is necessary to have a dataset. A dataset is a collection of labelled feature vectors, where a feature is a value that describes a sample of data somehow. In our case, the features are the MAV, ZC, SSC and WL of the Channels 1 and 2 data. A label is an informative value (or values) used to identify a particular feature vector. In our case, we use one-hot encoding for labelling, where each bit represents the probability from 0 to 1 the features belong to a particular gesture. For example, for Gesture 1 samples, there would be a probability of 1 in the first bit and a probability of 0 in the remaining bits, so the label would be [1, 0].\n",
        "\n",
        "The code in the following cell will import the feature vectors of each sample (obtained from MATLAB), assign them a label ([1, 0] for Gesture 1 and [0, 1] for Gesture 2), and then mix the Gesture 1 and 2 datasets together. After that, the code splits the dataset into `x` (features) and `y` (labels) to appropriately feed it into the ML algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKjg7QeMDsDx"
      },
      "outputs": [],
      "source": [
        "#Importing .txt files containing the combined Channel 1 and Channel 2 sEMG \n",
        "#features for each gesture; each row corresponding to a different sample\n",
        "data_gest1 = np.loadtxt('gest1.txt')\n",
        "data_gest2 = np.loadtxt('gest2.txt')\n",
        "\n",
        "#Creating labels for each gesture using one-hot encoding. Each gesture has a \n",
        "#label of 1 for its corresponding column and a label of 0 for all other \n",
        "#columns. Make sure the number of columns in each label is the same.\n",
        "label_gest1 = np.tile(np.array([1,0], dtype=float), (len(data_gest1), 1))\n",
        "label_gest2 = np.tile(np.array([0,1], dtype=float), (len(data_gest2), 1))\n",
        "\n",
        "#Combining data from each gesture with its corresponding label\n",
        "gest1 = np.concatenate((data_gest1, label_gest1), axis=1)\n",
        "gest2 = np.concatenate((data_gest2, label_gest2), axis=1)\n",
        "\n",
        "#Mixing the Gestures 1 and 2 data\n",
        "if len(gest1) != len(gest2):        #Checking gest1 and gest2 have the same length\n",
        "  print(\"Data for gestures 1 and 2 isn't the same size\")\n",
        "else:\n",
        "    counter = 0    #Counter to be used in the for loop\n",
        "    gestCombined = np.zeros((2*len(gest1),len(gest1[0])))   #Pre-allocating size for speed\n",
        "    for n in range(0,len(gest1)):       #Combining data into array gestCombined\n",
        "        gestCombined[counter] = gest1[n]\n",
        "        counter = counter + 1\n",
        "        gestCombined[counter] = gest2[n]\n",
        "        counter = counter + 1\n",
        "\n",
        "#Getting x and y values to feed into the algorithm. x has our features and y\n",
        "#has the corresponding labels\n",
        "gest_num = len(label_gest1[0])      #Number of gestures\n",
        "x_values = np.zeros((len(gestCombined),len(gestCombined[0])-gest_num))     #Pre-allocating size for speed\n",
        "y_values = np.zeros((len(gestCombined),gest_num))      #Pre-allocating size for speed\n",
        "for k in range(0,len(gestCombined)):\n",
        "    x_values[k] = gestCombined[k][0:len(gestCombined[n])-gest_num]\n",
        "    y_values[k] = gestCombined[k][len(gestCombined[n])-gest_num:len(gestCombined[n])]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Up8Xk_pMH4Rt"
      },
      "source": [
        "### 2. Split the Data\n",
        "\n",
        "To evaluate the accuracy of the model, it is necessary to compare its predictions to real data and check how well they match up. This evaluation happens during training (where it is referred to as validation) and after training (referred to as testing). It is important in both cases to use fresh data that was not already used to train the model.\n",
        "\n",
        "The data is split as follows:\n",
        "  1. Training: 60%\n",
        "  2. Validation: 20%\n",
        "  3. Testing: 20%\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNYko5L1keqZ"
      },
      "outputs": [],
      "source": [
        "# Number of sample datapoints\n",
        "samples = len(x_values)\n",
        "\n",
        "# We'll use 60% of our data for training and 20% for testing. The remaining 20%\n",
        "# will be used for validation. Calculating the indices of each section\n",
        "train_split =  int(0.6 * samples)\n",
        "test_split = int(0.2 * samples + train_split)\n",
        "\n",
        "# Choping x and y data into three parts\n",
        "x_train = np.float32(x_values[0:train_split])\n",
        "x_validate = np.float32(x_values[train_split:test_split])\n",
        "x_test = np.float32(x_values[test_split:len(x_values)])\n",
        "\n",
        "y_train = np.float32(y_values[0:train_split])\n",
        "y_validate = np.float32(y_values[train_split:test_split])\n",
        "y_test = np.float32(y_values[test_split:len(y_values)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2SqvxP5At3y",
        "outputId": "96934845-27c7-4ec4-8ec1-103c17a1a9e6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.float32"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "type(x_train[1][1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wfdelu1TmgPk"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5McVnHmNiDw"
      },
      "source": [
        "### 1. Design the Model\n",
        "We build a neural network model that takes `x` as an input value and uses it to predict the probability scores for each gesture (i.e. the `y`). This type of problem is called a regression, and the algorithm uses the layers of neurons to attempt to learn any patterns underlying the training data, so it can make predictions.\n",
        "\n",
        "There are 3 layers of neurons. The first layer takes an input vector of length 6 (the `x` feature vector) and runs it through 26 neurons. Based on this input, each neuron will become activated to a certain degree based on its internal state (its _weight_ and _bias_ values). A neuron's degree of activation is expressed as a number.\n",
        "\n",
        "The activation numbers from the first layer are then fed as inputs to the second layer, 17 neurons long, which applies its own _weights_ and _biases_ to these inputs to calculate its own activation. The activation numbers from the second layer are then fed as inputs to the third layer, and the process repeats until the final layer is reached. The last layer outputs a vector of length 2 (the `y` vector of probabilities).\n",
        "\n",
        "The code in the following cell defines the model using [Keras](https://www.tensorflow.org/guide/keras), TensorFlow's high-level API for creating deep learning networks. Once the network is defined, it is compiled with Categorical Crossentropy as the loss parameter and (Categorical) Accuracy as the performance metric. These parameters are normally used to measure the error in multi-class classification problems which use one-hot encoding, like it is our case ([How to solve Classification Problems in Deep Learning with Tensorflow & Keras?](https://medium.com/deep-learning-with-keras/how-to-solve-classification-problems-in-deep-learning-with-tensorflow-keras-6e39c5b09501))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gD60bE8cXQId",
        "outputId": "c61f7dfb-b8a8-4512-9720-ccc5fd13d88d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_6 (Dense)              (None, 26)                182       \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 17)                459       \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 2)                 36        \n",
            "=================================================================\n",
            "Total params: 677\n",
            "Trainable params: 677\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "#--- Creating the model architecture using Keras ---#\n",
        "# Defining the model\n",
        "model = tf.keras.Sequential()\n",
        "\n",
        "# Adding the first layer. The neurons decide whether to activate based on the \n",
        "# 'relu' activation function.\n",
        "model.add(keras.layers.Dense(26, activation='relu', input_shape=(6,)))\n",
        "\n",
        "# Including additional layers. These layers will help the network learn more \n",
        "# complex representations\n",
        "model.add(keras.layers.Dense(17, activation='relu'))\n",
        "\n",
        "# Incorporating the final layer, 2 neurons long, in order to output a vector of\n",
        "# length 2. The activation function used is 'softmax'\n",
        "model.add(keras.layers.Dense(2, activation='softmax'))\n",
        "\n",
        "# Compiling the model using the standard 'adam' optimizer, the\n",
        "# 'categorical_crossentropy' loss function, and the 'accuracy' metric for \n",
        "# regression.\n",
        "model.compile(optimizer='adam', loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Printing some summary information about the model's architecture\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0idLyRLQeGj"
      },
      "source": [
        "### 2. Train the Model\n",
        "Once the model is defined, we can use our data to train it. Training involves passing an `x` feature vector into the neural network, checking how far the network's output deviates from the expected `y` value, and adjusting the neurons' _weights_ and _biases_ so that the output is more likely to be correct the next time.\n",
        "\n",
        "Training runs this process on the full dataset multiple times, and each full run-through is known as an epoch. During each epoch, data is run through the network in multiple batches. Each batch involves several pieces of data, which are passed into the network, producing output values. These outputs' correctness is measured in aggregate and the network's _weights_ and _biases_ are adjusted accordingly, once per batch.\n",
        "\n",
        "The code in the following cell uses the `x` and `y` from the training data to train the model. It runs for 1000 epochs, with 8 pieces of data in each batch. The validation data is also passed in for validation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8hQKr4cVOdE",
        "outputId": "c41f32ea-4e42-4e43-c5ba-70d1ba6cab91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "68/68 [==============================] - 1s 4ms/step - loss: 93.3916 - accuracy: 0.4988 - val_loss: 9.7512 - val_accuracy: 0.6278\n",
            "Epoch 2/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 8.2141 - accuracy: 0.6383 - val_loss: 3.3687 - val_accuracy: 0.7278\n",
            "Epoch 3/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 3.1153 - accuracy: 0.6834 - val_loss: 3.6575 - val_accuracy: 0.7222\n",
            "Epoch 4/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 2.2906 - accuracy: 0.7513 - val_loss: 1.5491 - val_accuracy: 0.7889\n",
            "Epoch 5/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 1.9811 - accuracy: 0.7631 - val_loss: 1.4381 - val_accuracy: 0.8056\n",
            "Epoch 6/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 3.7861 - accuracy: 0.6994 - val_loss: 2.1081 - val_accuracy: 0.7833\n",
            "Epoch 7/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 2.4383 - accuracy: 0.7355 - val_loss: 2.7173 - val_accuracy: 0.7722\n",
            "Epoch 8/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 1.5622 - accuracy: 0.7796 - val_loss: 4.8152 - val_accuracy: 0.7278\n",
            "Epoch 9/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 2.3738 - accuracy: 0.7254 - val_loss: 1.1685 - val_accuracy: 0.8000\n",
            "Epoch 10/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 1.3864 - accuracy: 0.7501 - val_loss: 1.2130 - val_accuracy: 0.7889\n",
            "Epoch 11/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 1.1495 - accuracy: 0.7722 - val_loss: 1.0955 - val_accuracy: 0.7944\n",
            "Epoch 12/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 1.5722 - accuracy: 0.7444 - val_loss: 1.5914 - val_accuracy: 0.6944\n",
            "Epoch 13/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 1.5735 - accuracy: 0.7471 - val_loss: 1.3571 - val_accuracy: 0.8111\n",
            "Epoch 14/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 1.1604 - accuracy: 0.7842 - val_loss: 2.5355 - val_accuracy: 0.7500\n",
            "Epoch 15/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 2.3001 - accuracy: 0.7627 - val_loss: 2.5416 - val_accuracy: 0.7500\n",
            "Epoch 16/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 1.5025 - accuracy: 0.7790 - val_loss: 0.9754 - val_accuracy: 0.7778\n",
            "Epoch 17/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 1.1916 - accuracy: 0.7721 - val_loss: 1.6705 - val_accuracy: 0.7889\n",
            "Epoch 18/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 1.4033 - accuracy: 0.7449 - val_loss: 1.2550 - val_accuracy: 0.8000\n",
            "Epoch 19/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.8989 - accuracy: 0.7418 - val_loss: 0.8944 - val_accuracy: 0.7944\n",
            "Epoch 20/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 1.4366 - accuracy: 0.7703 - val_loss: 0.8436 - val_accuracy: 0.7944\n",
            "Epoch 21/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 1.2368 - accuracy: 0.7569 - val_loss: 0.7886 - val_accuracy: 0.7944\n",
            "Epoch 22/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.8436 - accuracy: 0.7587 - val_loss: 0.8452 - val_accuracy: 0.8056\n",
            "Epoch 23/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 1.2774 - accuracy: 0.7498 - val_loss: 0.7935 - val_accuracy: 0.8167\n",
            "Epoch 24/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 1.3449 - accuracy: 0.7653 - val_loss: 0.8914 - val_accuracy: 0.8111\n",
            "Epoch 25/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 1.3385 - accuracy: 0.7137 - val_loss: 3.2154 - val_accuracy: 0.7333\n",
            "Epoch 26/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 1.5650 - accuracy: 0.7545 - val_loss: 0.7090 - val_accuracy: 0.8056\n",
            "Epoch 27/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 1.2492 - accuracy: 0.7511 - val_loss: 0.7681 - val_accuracy: 0.7667\n",
            "Epoch 28/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.8019 - accuracy: 0.7522 - val_loss: 1.5029 - val_accuracy: 0.5722\n",
            "Epoch 29/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 1.4816 - accuracy: 0.6842 - val_loss: 1.3684 - val_accuracy: 0.7500\n",
            "Epoch 30/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.8593 - accuracy: 0.7862 - val_loss: 0.9984 - val_accuracy: 0.7889\n",
            "Epoch 31/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 1.0886 - accuracy: 0.7968 - val_loss: 0.6717 - val_accuracy: 0.8167\n",
            "Epoch 32/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.8143 - accuracy: 0.8116 - val_loss: 0.6851 - val_accuracy: 0.7889\n",
            "Epoch 33/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 1.4163 - accuracy: 0.7333 - val_loss: 1.2846 - val_accuracy: 0.7667\n",
            "Epoch 34/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.9944 - accuracy: 0.7277 - val_loss: 1.8143 - val_accuracy: 0.5389\n",
            "Epoch 35/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 1.2942 - accuracy: 0.6990 - val_loss: 0.7102 - val_accuracy: 0.8000\n",
            "Epoch 36/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 1.0350 - accuracy: 0.7485 - val_loss: 1.4785 - val_accuracy: 0.5778\n",
            "Epoch 37/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.9600 - accuracy: 0.7215 - val_loss: 0.7267 - val_accuracy: 0.7556\n",
            "Epoch 38/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.6391 - accuracy: 0.8143 - val_loss: 0.9897 - val_accuracy: 0.7611\n",
            "Epoch 39/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.6730 - accuracy: 0.7798 - val_loss: 0.5775 - val_accuracy: 0.8000\n",
            "Epoch 40/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.6389 - accuracy: 0.8130 - val_loss: 0.7562 - val_accuracy: 0.8111\n",
            "Epoch 41/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 1.0789 - accuracy: 0.7571 - val_loss: 0.6037 - val_accuracy: 0.8000\n",
            "Epoch 42/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 1.1378 - accuracy: 0.7734 - val_loss: 1.8998 - val_accuracy: 0.7444\n",
            "Epoch 43/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.8117 - accuracy: 0.8044 - val_loss: 0.5495 - val_accuracy: 0.8167\n",
            "Epoch 44/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.6857 - accuracy: 0.7989 - val_loss: 2.0896 - val_accuracy: 0.5000\n",
            "Epoch 45/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.9014 - accuracy: 0.7439 - val_loss: 1.3807 - val_accuracy: 0.5556\n",
            "Epoch 46/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 1.0284 - accuracy: 0.7143 - val_loss: 3.5617 - val_accuracy: 0.7444\n",
            "Epoch 47/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 2.6210 - accuracy: 0.7404 - val_loss: 0.7197 - val_accuracy: 0.7556\n",
            "Epoch 48/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.7424 - accuracy: 0.7604 - val_loss: 0.5507 - val_accuracy: 0.8389\n",
            "Epoch 49/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.7815 - accuracy: 0.7820 - val_loss: 1.4190 - val_accuracy: 0.7556\n",
            "Epoch 50/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.6477 - accuracy: 0.8249 - val_loss: 0.7025 - val_accuracy: 0.8000\n",
            "Epoch 51/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.9144 - accuracy: 0.7544 - val_loss: 0.6429 - val_accuracy: 0.7667\n",
            "Epoch 52/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.6151 - accuracy: 0.7786 - val_loss: 1.9995 - val_accuracy: 0.7278\n",
            "Epoch 53/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.7948 - accuracy: 0.7563 - val_loss: 1.0339 - val_accuracy: 0.5889\n",
            "Epoch 54/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.9255 - accuracy: 0.7618 - val_loss: 1.7800 - val_accuracy: 0.5167\n",
            "Epoch 55/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 1.0116 - accuracy: 0.7256 - val_loss: 0.7877 - val_accuracy: 0.8000\n",
            "Epoch 56/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 1.3594 - accuracy: 0.7159 - val_loss: 0.8032 - val_accuracy: 0.7167\n",
            "Epoch 57/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.8461 - accuracy: 0.8068 - val_loss: 1.1885 - val_accuracy: 0.5889\n",
            "Epoch 58/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 1.0236 - accuracy: 0.7350 - val_loss: 1.0881 - val_accuracy: 0.6278\n",
            "Epoch 59/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.7916 - accuracy: 0.7586 - val_loss: 2.7202 - val_accuracy: 0.7278\n",
            "Epoch 60/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 1.0241 - accuracy: 0.7469 - val_loss: 1.5073 - val_accuracy: 0.7389\n",
            "Epoch 61/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 1.3705 - accuracy: 0.7076 - val_loss: 0.5952 - val_accuracy: 0.7889\n",
            "Epoch 62/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.5844 - accuracy: 0.8148 - val_loss: 1.2820 - val_accuracy: 0.7722\n",
            "Epoch 63/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.8239 - accuracy: 0.7830 - val_loss: 0.5972 - val_accuracy: 0.8222\n",
            "Epoch 64/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.6664 - accuracy: 0.7826 - val_loss: 4.1246 - val_accuracy: 0.5000\n",
            "Epoch 65/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 1.6729 - accuracy: 0.6833 - val_loss: 0.7734 - val_accuracy: 0.8000\n",
            "Epoch 66/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.7504 - accuracy: 0.8062 - val_loss: 0.7501 - val_accuracy: 0.7889\n",
            "Epoch 67/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.8995 - accuracy: 0.7536 - val_loss: 1.1148 - val_accuracy: 0.7611\n",
            "Epoch 68/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.6612 - accuracy: 0.7935 - val_loss: 0.6454 - val_accuracy: 0.7722\n",
            "Epoch 69/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.7340 - accuracy: 0.7973 - val_loss: 1.5457 - val_accuracy: 0.5222\n",
            "Epoch 70/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.7245 - accuracy: 0.7694 - val_loss: 0.7435 - val_accuracy: 0.7778\n",
            "Epoch 71/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.8838 - accuracy: 0.7469 - val_loss: 0.5385 - val_accuracy: 0.8333\n",
            "Epoch 72/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.7521 - accuracy: 0.7653 - val_loss: 1.1161 - val_accuracy: 0.5611\n",
            "Epoch 73/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.6496 - accuracy: 0.7847 - val_loss: 0.9502 - val_accuracy: 0.7722\n",
            "Epoch 74/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.8071 - accuracy: 0.7592 - val_loss: 0.5388 - val_accuracy: 0.8167\n",
            "Epoch 75/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.6904 - accuracy: 0.7763 - val_loss: 0.5190 - val_accuracy: 0.8278\n",
            "Epoch 76/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.5576 - accuracy: 0.8163 - val_loss: 0.7137 - val_accuracy: 0.8000\n",
            "Epoch 77/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.5657 - accuracy: 0.7889 - val_loss: 0.5563 - val_accuracy: 0.8000\n",
            "Epoch 78/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.6368 - accuracy: 0.7594 - val_loss: 0.5001 - val_accuracy: 0.8333\n",
            "Epoch 79/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.6583 - accuracy: 0.7891 - val_loss: 0.8962 - val_accuracy: 0.6333\n",
            "Epoch 80/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.8461 - accuracy: 0.7690 - val_loss: 0.7227 - val_accuracy: 0.7833\n",
            "Epoch 81/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.7688 - accuracy: 0.7413 - val_loss: 1.2236 - val_accuracy: 0.7389\n",
            "Epoch 82/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.6178 - accuracy: 0.7878 - val_loss: 0.5930 - val_accuracy: 0.7944\n",
            "Epoch 83/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.5325 - accuracy: 0.8076 - val_loss: 0.6888 - val_accuracy: 0.7167\n",
            "Epoch 84/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.6331 - accuracy: 0.8146 - val_loss: 0.6112 - val_accuracy: 0.8000\n",
            "Epoch 85/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.6364 - accuracy: 0.7540 - val_loss: 2.5080 - val_accuracy: 0.7278\n",
            "Epoch 86/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.9223 - accuracy: 0.7978 - val_loss: 1.5516 - val_accuracy: 0.5167\n",
            "Epoch 87/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.7212 - accuracy: 0.7452 - val_loss: 0.6387 - val_accuracy: 0.8000\n",
            "Epoch 88/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.7329 - accuracy: 0.8145 - val_loss: 0.5113 - val_accuracy: 0.8167\n",
            "Epoch 89/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.5258 - accuracy: 0.7912 - val_loss: 0.6416 - val_accuracy: 0.7944\n",
            "Epoch 90/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.8370 - accuracy: 0.7657 - val_loss: 0.6917 - val_accuracy: 0.7389\n",
            "Epoch 91/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.4552 - accuracy: 0.8066 - val_loss: 1.4359 - val_accuracy: 0.7444\n",
            "Epoch 92/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.9713 - accuracy: 0.7308 - val_loss: 0.9291 - val_accuracy: 0.7444\n",
            "Epoch 93/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.5688 - accuracy: 0.7865 - val_loss: 0.5555 - val_accuracy: 0.8056\n",
            "Epoch 94/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.8724 - accuracy: 0.7475 - val_loss: 1.0173 - val_accuracy: 0.6111\n",
            "Epoch 95/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.8380 - accuracy: 0.7367 - val_loss: 0.5260 - val_accuracy: 0.8278\n",
            "Epoch 96/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.9440 - accuracy: 0.7453 - val_loss: 1.7389 - val_accuracy: 0.7333\n",
            "Epoch 97/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 1.3727 - accuracy: 0.7066 - val_loss: 0.6248 - val_accuracy: 0.8056\n",
            "Epoch 98/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.7485 - accuracy: 0.7828 - val_loss: 0.9525 - val_accuracy: 0.7611\n",
            "Epoch 99/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.6050 - accuracy: 0.8075 - val_loss: 0.5969 - val_accuracy: 0.8000\n",
            "Epoch 100/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.5439 - accuracy: 0.8107 - val_loss: 0.5540 - val_accuracy: 0.8278\n",
            "Epoch 101/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.6659 - accuracy: 0.7921 - val_loss: 0.5522 - val_accuracy: 0.8278\n",
            "Epoch 102/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.6685 - accuracy: 0.7696 - val_loss: 0.6189 - val_accuracy: 0.8000\n",
            "Epoch 103/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.5996 - accuracy: 0.7843 - val_loss: 1.0275 - val_accuracy: 0.7667\n",
            "Epoch 104/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.7535 - accuracy: 0.7843 - val_loss: 0.5612 - val_accuracy: 0.7944\n",
            "Epoch 105/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.5914 - accuracy: 0.7978 - val_loss: 0.5364 - val_accuracy: 0.8444\n",
            "Epoch 106/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.8486 - accuracy: 0.7338 - val_loss: 0.5439 - val_accuracy: 0.8167\n",
            "Epoch 107/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.5325 - accuracy: 0.8138 - val_loss: 1.4481 - val_accuracy: 0.7444\n",
            "Epoch 108/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.6884 - accuracy: 0.7919 - val_loss: 1.0407 - val_accuracy: 0.7611\n",
            "Epoch 109/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.5953 - accuracy: 0.7793 - val_loss: 0.6499 - val_accuracy: 0.7833\n",
            "Epoch 110/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.6377 - accuracy: 0.8039 - val_loss: 0.9040 - val_accuracy: 0.7556\n",
            "Epoch 111/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.6258 - accuracy: 0.7963 - val_loss: 0.7981 - val_accuracy: 0.7833\n",
            "Epoch 112/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.6286 - accuracy: 0.7844 - val_loss: 0.5237 - val_accuracy: 0.7889\n",
            "Epoch 113/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.4979 - accuracy: 0.8094 - val_loss: 0.7542 - val_accuracy: 0.7722\n",
            "Epoch 114/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 1.6068 - accuracy: 0.7414 - val_loss: 0.5454 - val_accuracy: 0.8278\n",
            "Epoch 115/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.5546 - accuracy: 0.8186 - val_loss: 1.5681 - val_accuracy: 0.7444\n",
            "Epoch 116/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 1.4940 - accuracy: 0.7351 - val_loss: 0.8781 - val_accuracy: 0.6944\n",
            "Epoch 117/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.8545 - accuracy: 0.7326 - val_loss: 1.9897 - val_accuracy: 0.5111\n",
            "Epoch 118/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 1.7047 - accuracy: 0.7069 - val_loss: 1.3207 - val_accuracy: 0.5944\n",
            "Epoch 119/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 1.2076 - accuracy: 0.7156 - val_loss: 1.7466 - val_accuracy: 0.7611\n",
            "Epoch 120/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.8611 - accuracy: 0.7754 - val_loss: 0.8633 - val_accuracy: 0.7556\n",
            "Epoch 121/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.6430 - accuracy: 0.8060 - val_loss: 1.5603 - val_accuracy: 0.5667\n",
            "Epoch 122/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.8029 - accuracy: 0.7621 - val_loss: 0.6132 - val_accuracy: 0.7889\n",
            "Epoch 123/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.4970 - accuracy: 0.8144 - val_loss: 1.2156 - val_accuracy: 0.6111\n",
            "Epoch 124/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.6342 - accuracy: 0.7962 - val_loss: 0.5780 - val_accuracy: 0.8167\n",
            "Epoch 125/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.8078 - accuracy: 0.7618 - val_loss: 1.1122 - val_accuracy: 0.6278\n",
            "Epoch 126/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.6860 - accuracy: 0.7610 - val_loss: 0.6992 - val_accuracy: 0.7444\n",
            "Epoch 127/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.4503 - accuracy: 0.8372 - val_loss: 0.5595 - val_accuracy: 0.8500\n",
            "Epoch 128/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.7502 - accuracy: 0.7899 - val_loss: 1.3017 - val_accuracy: 0.7444\n",
            "Epoch 129/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.9108 - accuracy: 0.7502 - val_loss: 1.4539 - val_accuracy: 0.7444\n",
            "Epoch 130/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 2.3699 - accuracy: 0.6896 - val_loss: 0.7343 - val_accuracy: 0.7944\n",
            "Epoch 131/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 1.0532 - accuracy: 0.7319 - val_loss: 0.6408 - val_accuracy: 0.8111\n",
            "Epoch 132/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.4701 - accuracy: 0.8513 - val_loss: 1.0276 - val_accuracy: 0.6722\n",
            "Epoch 133/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.5349 - accuracy: 0.8315 - val_loss: 0.6291 - val_accuracy: 0.8111\n",
            "Epoch 134/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.4947 - accuracy: 0.8041 - val_loss: 0.5987 - val_accuracy: 0.7833\n",
            "Epoch 135/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.4847 - accuracy: 0.8199 - val_loss: 0.5913 - val_accuracy: 0.8056\n",
            "Epoch 136/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.7801 - accuracy: 0.7421 - val_loss: 0.5481 - val_accuracy: 0.8056\n",
            "Epoch 137/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.4872 - accuracy: 0.8348 - val_loss: 1.1718 - val_accuracy: 0.7556\n",
            "Epoch 138/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.5504 - accuracy: 0.8074 - val_loss: 0.5246 - val_accuracy: 0.8278\n",
            "Epoch 139/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.4279 - accuracy: 0.8436 - val_loss: 1.7750 - val_accuracy: 0.5278\n",
            "Epoch 140/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 1.1046 - accuracy: 0.7643 - val_loss: 0.5839 - val_accuracy: 0.8333\n",
            "Epoch 141/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.4497 - accuracy: 0.8401 - val_loss: 0.6265 - val_accuracy: 0.7889\n",
            "Epoch 142/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.5149 - accuracy: 0.8077 - val_loss: 0.5050 - val_accuracy: 0.8111\n",
            "Epoch 143/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.5389 - accuracy: 0.8121 - val_loss: 0.5728 - val_accuracy: 0.8111\n",
            "Epoch 144/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.4855 - accuracy: 0.8394 - val_loss: 0.9045 - val_accuracy: 0.7611\n",
            "Epoch 145/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 1.0436 - accuracy: 0.7444 - val_loss: 0.8463 - val_accuracy: 0.6944\n",
            "Epoch 146/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.6021 - accuracy: 0.7915 - val_loss: 0.9799 - val_accuracy: 0.6722\n",
            "Epoch 147/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.6863 - accuracy: 0.7850 - val_loss: 0.9584 - val_accuracy: 0.6611\n",
            "Epoch 148/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.6629 - accuracy: 0.7880 - val_loss: 1.0154 - val_accuracy: 0.7556\n",
            "Epoch 149/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.4829 - accuracy: 0.8110 - val_loss: 0.5652 - val_accuracy: 0.8444\n",
            "Epoch 150/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.8005 - accuracy: 0.7522 - val_loss: 0.9216 - val_accuracy: 0.6833\n",
            "Epoch 151/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.8728 - accuracy: 0.7793 - val_loss: 0.5371 - val_accuracy: 0.8389\n",
            "Epoch 152/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.5934 - accuracy: 0.8019 - val_loss: 0.6418 - val_accuracy: 0.8111\n",
            "Epoch 153/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.4503 - accuracy: 0.8262 - val_loss: 0.9098 - val_accuracy: 0.6889\n",
            "Epoch 154/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.4508 - accuracy: 0.8214 - val_loss: 0.5271 - val_accuracy: 0.8444\n",
            "Epoch 155/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.6255 - accuracy: 0.7968 - val_loss: 0.7255 - val_accuracy: 0.7389\n",
            "Epoch 156/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.6035 - accuracy: 0.7832 - val_loss: 1.0968 - val_accuracy: 0.6389\n",
            "Epoch 157/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.8471 - accuracy: 0.7936 - val_loss: 0.6682 - val_accuracy: 0.8167\n",
            "Epoch 158/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.4550 - accuracy: 0.8208 - val_loss: 0.8160 - val_accuracy: 0.7778\n",
            "Epoch 159/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 1.0361 - accuracy: 0.7456 - val_loss: 0.9688 - val_accuracy: 0.7722\n",
            "Epoch 160/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.5795 - accuracy: 0.8445 - val_loss: 0.4568 - val_accuracy: 0.8556\n",
            "Epoch 161/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.4707 - accuracy: 0.8297 - val_loss: 0.5537 - val_accuracy: 0.7889\n",
            "Epoch 162/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.5889 - accuracy: 0.7818 - val_loss: 0.7039 - val_accuracy: 0.7778\n",
            "Epoch 163/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.5194 - accuracy: 0.8151 - val_loss: 0.5804 - val_accuracy: 0.7778\n",
            "Epoch 164/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.5898 - accuracy: 0.7985 - val_loss: 0.6319 - val_accuracy: 0.8000\n",
            "Epoch 165/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.6234 - accuracy: 0.7741 - val_loss: 1.4911 - val_accuracy: 0.5778\n",
            "Epoch 166/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.5095 - accuracy: 0.8359 - val_loss: 0.8937 - val_accuracy: 0.7667\n",
            "Epoch 167/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.6263 - accuracy: 0.8235 - val_loss: 0.5725 - val_accuracy: 0.7722\n",
            "Epoch 168/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3785 - accuracy: 0.8641 - val_loss: 0.6818 - val_accuracy: 0.7778\n",
            "Epoch 169/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3692 - accuracy: 0.8719 - val_loss: 0.6258 - val_accuracy: 0.8056\n",
            "Epoch 170/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.6918 - accuracy: 0.7850 - val_loss: 0.8916 - val_accuracy: 0.7667\n",
            "Epoch 171/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.6325 - accuracy: 0.7945 - val_loss: 0.6462 - val_accuracy: 0.7778\n",
            "Epoch 172/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.5990 - accuracy: 0.8163 - val_loss: 0.6577 - val_accuracy: 0.8056\n",
            "Epoch 173/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.5535 - accuracy: 0.8093 - val_loss: 1.3458 - val_accuracy: 0.5722\n",
            "Epoch 174/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.8984 - accuracy: 0.7526 - val_loss: 0.4529 - val_accuracy: 0.8389\n",
            "Epoch 175/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.5788 - accuracy: 0.8116 - val_loss: 0.4210 - val_accuracy: 0.7944\n",
            "Epoch 176/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.4156 - accuracy: 0.8302 - val_loss: 0.4424 - val_accuracy: 0.8111\n",
            "Epoch 177/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3372 - accuracy: 0.8633 - val_loss: 0.5639 - val_accuracy: 0.7611\n",
            "Epoch 178/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.6180 - accuracy: 0.7907 - val_loss: 0.7141 - val_accuracy: 0.7667\n",
            "Epoch 179/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.5384 - accuracy: 0.8162 - val_loss: 0.5019 - val_accuracy: 0.8167\n",
            "Epoch 180/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2949 - accuracy: 0.8687 - val_loss: 0.4057 - val_accuracy: 0.7889\n",
            "Epoch 181/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3718 - accuracy: 0.8637 - val_loss: 0.4366 - val_accuracy: 0.7889\n",
            "Epoch 182/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.4017 - accuracy: 0.8452 - val_loss: 1.0864 - val_accuracy: 0.7556\n",
            "Epoch 183/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.7952 - accuracy: 0.7747 - val_loss: 1.2598 - val_accuracy: 0.5722\n",
            "Epoch 184/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.5609 - accuracy: 0.7921 - val_loss: 0.4193 - val_accuracy: 0.7944\n",
            "Epoch 185/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.4447 - accuracy: 0.8394 - val_loss: 0.3816 - val_accuracy: 0.8056\n",
            "Epoch 186/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3340 - accuracy: 0.8615 - val_loss: 0.5471 - val_accuracy: 0.7778\n",
            "Epoch 187/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.4302 - accuracy: 0.8199 - val_loss: 0.4619 - val_accuracy: 0.8444\n",
            "Epoch 188/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.6193 - accuracy: 0.8125 - val_loss: 0.6334 - val_accuracy: 0.7722\n",
            "Epoch 189/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.4115 - accuracy: 0.8322 - val_loss: 0.6764 - val_accuracy: 0.7333\n",
            "Epoch 190/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.6023 - accuracy: 0.8045 - val_loss: 0.4616 - val_accuracy: 0.8222\n",
            "Epoch 191/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.4953 - accuracy: 0.8135 - val_loss: 0.3926 - val_accuracy: 0.8278\n",
            "Epoch 192/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3998 - accuracy: 0.8501 - val_loss: 0.4231 - val_accuracy: 0.8167\n",
            "Epoch 193/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.4732 - accuracy: 0.8123 - val_loss: 0.6157 - val_accuracy: 0.7722\n",
            "Epoch 194/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.8391 - accuracy: 0.7493 - val_loss: 0.3694 - val_accuracy: 0.8389\n",
            "Epoch 195/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3695 - accuracy: 0.8729 - val_loss: 0.8722 - val_accuracy: 0.7611\n",
            "Epoch 196/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.7048 - accuracy: 0.7820 - val_loss: 0.6573 - val_accuracy: 0.7833\n",
            "Epoch 197/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.5181 - accuracy: 0.8087 - val_loss: 0.6746 - val_accuracy: 0.7556\n",
            "Epoch 198/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.7381 - accuracy: 0.7753 - val_loss: 0.3816 - val_accuracy: 0.8333\n",
            "Epoch 199/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.4948 - accuracy: 0.8104 - val_loss: 1.0644 - val_accuracy: 0.7833\n",
            "Epoch 200/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.5174 - accuracy: 0.8320 - val_loss: 0.5392 - val_accuracy: 0.8056\n",
            "Epoch 201/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.4989 - accuracy: 0.8274 - val_loss: 2.2338 - val_accuracy: 0.7444\n",
            "Epoch 202/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 1.3913 - accuracy: 0.7661 - val_loss: 0.4335 - val_accuracy: 0.8222\n",
            "Epoch 203/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3319 - accuracy: 0.8571 - val_loss: 0.3625 - val_accuracy: 0.8500\n",
            "Epoch 204/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.5321 - accuracy: 0.8303 - val_loss: 0.3549 - val_accuracy: 0.8667\n",
            "Epoch 205/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.6383 - accuracy: 0.8356 - val_loss: 0.4437 - val_accuracy: 0.8333\n",
            "Epoch 206/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3307 - accuracy: 0.8585 - val_loss: 0.3451 - val_accuracy: 0.8444\n",
            "Epoch 207/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.5806 - accuracy: 0.7661 - val_loss: 0.6755 - val_accuracy: 0.7889\n",
            "Epoch 208/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.5415 - accuracy: 0.8210 - val_loss: 0.8652 - val_accuracy: 0.7611\n",
            "Epoch 209/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.5899 - accuracy: 0.8017 - val_loss: 0.4987 - val_accuracy: 0.7889\n",
            "Epoch 210/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3815 - accuracy: 0.8289 - val_loss: 0.3866 - val_accuracy: 0.8333\n",
            "Epoch 211/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.4868 - accuracy: 0.8412 - val_loss: 0.3739 - val_accuracy: 0.8444\n",
            "Epoch 212/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.4563 - accuracy: 0.8279 - val_loss: 0.3294 - val_accuracy: 0.8389\n",
            "Epoch 213/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.4158 - accuracy: 0.8604 - val_loss: 0.9033 - val_accuracy: 0.6278\n",
            "Epoch 214/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.7389 - accuracy: 0.7604 - val_loss: 1.0173 - val_accuracy: 0.7722\n",
            "Epoch 215/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.7829 - accuracy: 0.8029 - val_loss: 0.4610 - val_accuracy: 0.8056\n",
            "Epoch 216/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.5182 - accuracy: 0.8193 - val_loss: 0.3705 - val_accuracy: 0.8333\n",
            "Epoch 217/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.6544 - accuracy: 0.8150 - val_loss: 0.5760 - val_accuracy: 0.7833\n",
            "Epoch 218/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.4657 - accuracy: 0.8450 - val_loss: 0.3813 - val_accuracy: 0.8500\n",
            "Epoch 219/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3036 - accuracy: 0.8695 - val_loss: 0.4502 - val_accuracy: 0.8111\n",
            "Epoch 220/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.6053 - accuracy: 0.8029 - val_loss: 0.4530 - val_accuracy: 0.8222\n",
            "Epoch 221/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.4011 - accuracy: 0.8496 - val_loss: 0.3767 - val_accuracy: 0.8333\n",
            "Epoch 222/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2469 - accuracy: 0.8942 - val_loss: 0.5147 - val_accuracy: 0.7778\n",
            "Epoch 223/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2797 - accuracy: 0.8770 - val_loss: 0.5019 - val_accuracy: 0.7611\n",
            "Epoch 224/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.4072 - accuracy: 0.8519 - val_loss: 0.7734 - val_accuracy: 0.6944\n",
            "Epoch 225/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3865 - accuracy: 0.8333 - val_loss: 0.7217 - val_accuracy: 0.7778\n",
            "Epoch 226/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3863 - accuracy: 0.8359 - val_loss: 0.4053 - val_accuracy: 0.8167\n",
            "Epoch 227/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3696 - accuracy: 0.8416 - val_loss: 0.6009 - val_accuracy: 0.7556\n",
            "Epoch 228/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.4098 - accuracy: 0.8438 - val_loss: 0.5249 - val_accuracy: 0.7889\n",
            "Epoch 229/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3582 - accuracy: 0.8384 - val_loss: 0.5022 - val_accuracy: 0.8056\n",
            "Epoch 230/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.5814 - accuracy: 0.8049 - val_loss: 0.8367 - val_accuracy: 0.7722\n",
            "Epoch 231/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.4944 - accuracy: 0.8265 - val_loss: 0.3831 - val_accuracy: 0.8333\n",
            "Epoch 232/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.5300 - accuracy: 0.8152 - val_loss: 0.5865 - val_accuracy: 0.7944\n",
            "Epoch 233/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3284 - accuracy: 0.8558 - val_loss: 0.3446 - val_accuracy: 0.8444\n",
            "Epoch 234/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3400 - accuracy: 0.8541 - val_loss: 0.9980 - val_accuracy: 0.7556\n",
            "Epoch 235/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.6502 - accuracy: 0.7987 - val_loss: 0.5304 - val_accuracy: 0.8056\n",
            "Epoch 236/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3619 - accuracy: 0.8586 - val_loss: 0.3693 - val_accuracy: 0.8278\n",
            "Epoch 237/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.4286 - accuracy: 0.8450 - val_loss: 0.4434 - val_accuracy: 0.7944\n",
            "Epoch 238/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3606 - accuracy: 0.8830 - val_loss: 0.7051 - val_accuracy: 0.7611\n",
            "Epoch 239/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3313 - accuracy: 0.8558 - val_loss: 0.7627 - val_accuracy: 0.7056\n",
            "Epoch 240/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3759 - accuracy: 0.8516 - val_loss: 0.4173 - val_accuracy: 0.8167\n",
            "Epoch 241/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2942 - accuracy: 0.8826 - val_loss: 1.2523 - val_accuracy: 0.7556\n",
            "Epoch 242/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.7595 - accuracy: 0.8013 - val_loss: 0.5840 - val_accuracy: 0.7889\n",
            "Epoch 243/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.4312 - accuracy: 0.8130 - val_loss: 0.7024 - val_accuracy: 0.7889\n",
            "Epoch 244/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.4897 - accuracy: 0.8223 - val_loss: 0.3512 - val_accuracy: 0.8556\n",
            "Epoch 245/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2666 - accuracy: 0.8883 - val_loss: 1.4178 - val_accuracy: 0.7444\n",
            "Epoch 246/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.6273 - accuracy: 0.8227 - val_loss: 0.4149 - val_accuracy: 0.8500\n",
            "Epoch 247/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.4111 - accuracy: 0.8420 - val_loss: 0.4890 - val_accuracy: 0.8167\n",
            "Epoch 248/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.6238 - accuracy: 0.7835 - val_loss: 0.6786 - val_accuracy: 0.7389\n",
            "Epoch 249/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.5095 - accuracy: 0.8282 - val_loss: 0.4263 - val_accuracy: 0.8667\n",
            "Epoch 250/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3731 - accuracy: 0.8455 - val_loss: 0.4650 - val_accuracy: 0.7889\n",
            "Epoch 251/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.4022 - accuracy: 0.8429 - val_loss: 0.6851 - val_accuracy: 0.7556\n",
            "Epoch 252/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.4219 - accuracy: 0.8447 - val_loss: 0.9473 - val_accuracy: 0.6722\n",
            "Epoch 253/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3953 - accuracy: 0.8109 - val_loss: 0.9889 - val_accuracy: 0.7611\n",
            "Epoch 254/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.5752 - accuracy: 0.8162 - val_loss: 0.5061 - val_accuracy: 0.8111\n",
            "Epoch 255/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3334 - accuracy: 0.8552 - val_loss: 0.5105 - val_accuracy: 0.8056\n",
            "Epoch 256/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2612 - accuracy: 0.8966 - val_loss: 0.9557 - val_accuracy: 0.6556\n",
            "Epoch 257/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.4402 - accuracy: 0.8503 - val_loss: 0.5105 - val_accuracy: 0.8000\n",
            "Epoch 258/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3754 - accuracy: 0.8515 - val_loss: 0.4654 - val_accuracy: 0.8333\n",
            "Epoch 259/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.5234 - accuracy: 0.8296 - val_loss: 0.3986 - val_accuracy: 0.8444\n",
            "Epoch 260/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3085 - accuracy: 0.8583 - val_loss: 0.9791 - val_accuracy: 0.7611\n",
            "Epoch 261/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.4196 - accuracy: 0.8538 - val_loss: 0.7455 - val_accuracy: 0.7778\n",
            "Epoch 262/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3847 - accuracy: 0.8735 - val_loss: 0.4540 - val_accuracy: 0.8111\n",
            "Epoch 263/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.4048 - accuracy: 0.8454 - val_loss: 0.5256 - val_accuracy: 0.7778\n",
            "Epoch 264/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.4025 - accuracy: 0.8394 - val_loss: 0.4489 - val_accuracy: 0.8222\n",
            "Epoch 265/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3521 - accuracy: 0.8541 - val_loss: 0.3757 - val_accuracy: 0.8556\n",
            "Epoch 266/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.4788 - accuracy: 0.8301 - val_loss: 0.4439 - val_accuracy: 0.8056\n",
            "Epoch 267/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.5588 - accuracy: 0.8307 - val_loss: 0.4363 - val_accuracy: 0.8333\n",
            "Epoch 268/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2620 - accuracy: 0.8989 - val_loss: 0.7126 - val_accuracy: 0.7833\n",
            "Epoch 269/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.4388 - accuracy: 0.8189 - val_loss: 0.3468 - val_accuracy: 0.8611\n",
            "Epoch 270/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3066 - accuracy: 0.8666 - val_loss: 0.3746 - val_accuracy: 0.8222\n",
            "Epoch 271/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3051 - accuracy: 0.8751 - val_loss: 0.3850 - val_accuracy: 0.8444\n",
            "Epoch 272/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.4391 - accuracy: 0.8517 - val_loss: 0.5551 - val_accuracy: 0.7889\n",
            "Epoch 273/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.4722 - accuracy: 0.8550 - val_loss: 0.8182 - val_accuracy: 0.7778\n",
            "Epoch 274/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.4760 - accuracy: 0.8360 - val_loss: 0.3886 - val_accuracy: 0.8278\n",
            "Epoch 275/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2788 - accuracy: 0.8901 - val_loss: 0.4908 - val_accuracy: 0.7722\n",
            "Epoch 276/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2804 - accuracy: 0.8700 - val_loss: 0.4407 - val_accuracy: 0.7833\n",
            "Epoch 277/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3629 - accuracy: 0.8615 - val_loss: 0.7336 - val_accuracy: 0.7500\n",
            "Epoch 278/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.4391 - accuracy: 0.8500 - val_loss: 0.4154 - val_accuracy: 0.8278\n",
            "Epoch 279/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3098 - accuracy: 0.8882 - val_loss: 0.4223 - val_accuracy: 0.8056\n",
            "Epoch 280/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3038 - accuracy: 0.9016 - val_loss: 0.3725 - val_accuracy: 0.8222\n",
            "Epoch 281/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.4215 - accuracy: 0.8220 - val_loss: 0.4071 - val_accuracy: 0.8444\n",
            "Epoch 282/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3147 - accuracy: 0.8679 - val_loss: 0.6044 - val_accuracy: 0.7667\n",
            "Epoch 283/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.4725 - accuracy: 0.7892 - val_loss: 0.3696 - val_accuracy: 0.8444\n",
            "Epoch 284/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3384 - accuracy: 0.8645 - val_loss: 0.8655 - val_accuracy: 0.7667\n",
            "Epoch 285/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.4035 - accuracy: 0.8502 - val_loss: 0.3540 - val_accuracy: 0.8722\n",
            "Epoch 286/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3620 - accuracy: 0.8678 - val_loss: 0.5289 - val_accuracy: 0.8111\n",
            "Epoch 287/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3992 - accuracy: 0.8670 - val_loss: 0.6761 - val_accuracy: 0.7833\n",
            "Epoch 288/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.4120 - accuracy: 0.8503 - val_loss: 0.5237 - val_accuracy: 0.7944\n",
            "Epoch 289/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.4177 - accuracy: 0.8706 - val_loss: 0.3867 - val_accuracy: 0.8444\n",
            "Epoch 290/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3071 - accuracy: 0.8762 - val_loss: 0.4447 - val_accuracy: 0.8167\n",
            "Epoch 291/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3864 - accuracy: 0.8526 - val_loss: 0.4432 - val_accuracy: 0.8222\n",
            "Epoch 292/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3066 - accuracy: 0.8798 - val_loss: 0.5805 - val_accuracy: 0.7833\n",
            "Epoch 293/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.4696 - accuracy: 0.8557 - val_loss: 0.3753 - val_accuracy: 0.8444\n",
            "Epoch 294/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3083 - accuracy: 0.8562 - val_loss: 0.4429 - val_accuracy: 0.8167\n",
            "Epoch 295/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3153 - accuracy: 0.8772 - val_loss: 2.0971 - val_accuracy: 0.7333\n",
            "Epoch 296/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.8715 - accuracy: 0.7816 - val_loss: 0.5416 - val_accuracy: 0.7833\n",
            "Epoch 297/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3204 - accuracy: 0.8522 - val_loss: 0.4251 - val_accuracy: 0.8000\n",
            "Epoch 298/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3277 - accuracy: 0.8653 - val_loss: 0.6344 - val_accuracy: 0.7389\n",
            "Epoch 299/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3445 - accuracy: 0.8422 - val_loss: 0.4605 - val_accuracy: 0.8222\n",
            "Epoch 300/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3403 - accuracy: 0.8803 - val_loss: 0.5538 - val_accuracy: 0.7611\n",
            "Epoch 301/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3942 - accuracy: 0.8339 - val_loss: 0.7143 - val_accuracy: 0.7444\n",
            "Epoch 302/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3491 - accuracy: 0.8532 - val_loss: 0.4131 - val_accuracy: 0.8500\n",
            "Epoch 303/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2944 - accuracy: 0.8915 - val_loss: 0.4828 - val_accuracy: 0.7889\n",
            "Epoch 304/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3422 - accuracy: 0.8524 - val_loss: 0.3971 - val_accuracy: 0.8278\n",
            "Epoch 305/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3019 - accuracy: 0.8591 - val_loss: 0.3597 - val_accuracy: 0.8278\n",
            "Epoch 306/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2749 - accuracy: 0.8880 - val_loss: 0.7813 - val_accuracy: 0.7056\n",
            "Epoch 307/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3467 - accuracy: 0.8369 - val_loss: 0.4140 - val_accuracy: 0.8444\n",
            "Epoch 308/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2653 - accuracy: 0.8964 - val_loss: 0.3683 - val_accuracy: 0.8333\n",
            "Epoch 309/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3608 - accuracy: 0.8599 - val_loss: 0.6006 - val_accuracy: 0.7611\n",
            "Epoch 310/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3925 - accuracy: 0.8396 - val_loss: 0.5446 - val_accuracy: 0.8000\n",
            "Epoch 311/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3538 - accuracy: 0.8584 - val_loss: 0.6039 - val_accuracy: 0.8056\n",
            "Epoch 312/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.5136 - accuracy: 0.8079 - val_loss: 0.6708 - val_accuracy: 0.7611\n",
            "Epoch 313/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3851 - accuracy: 0.8316 - val_loss: 0.3923 - val_accuracy: 0.8278\n",
            "Epoch 314/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3991 - accuracy: 0.8388 - val_loss: 0.9550 - val_accuracy: 0.7000\n",
            "Epoch 315/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.4821 - accuracy: 0.8363 - val_loss: 0.3851 - val_accuracy: 0.8333\n",
            "Epoch 316/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3710 - accuracy: 0.8508 - val_loss: 0.3633 - val_accuracy: 0.8222\n",
            "Epoch 317/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3040 - accuracy: 0.8680 - val_loss: 0.5023 - val_accuracy: 0.8056\n",
            "Epoch 318/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2471 - accuracy: 0.8794 - val_loss: 0.5074 - val_accuracy: 0.8056\n",
            "Epoch 319/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.4675 - accuracy: 0.7975 - val_loss: 0.5141 - val_accuracy: 0.7889\n",
            "Epoch 320/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3938 - accuracy: 0.8100 - val_loss: 0.4079 - val_accuracy: 0.8056\n",
            "Epoch 321/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.5136 - accuracy: 0.7892 - val_loss: 0.4090 - val_accuracy: 0.7944\n",
            "Epoch 322/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3799 - accuracy: 0.8572 - val_loss: 0.8375 - val_accuracy: 0.7667\n",
            "Epoch 323/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.4437 - accuracy: 0.8194 - val_loss: 0.3896 - val_accuracy: 0.8444\n",
            "Epoch 324/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3275 - accuracy: 0.8631 - val_loss: 0.3511 - val_accuracy: 0.8500\n",
            "Epoch 325/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.4708 - accuracy: 0.8413 - val_loss: 0.8194 - val_accuracy: 0.6944\n",
            "Epoch 326/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.4074 - accuracy: 0.8034 - val_loss: 0.4154 - val_accuracy: 0.8722\n",
            "Epoch 327/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2807 - accuracy: 0.8989 - val_loss: 0.6574 - val_accuracy: 0.7833\n",
            "Epoch 328/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3570 - accuracy: 0.8430 - val_loss: 0.5029 - val_accuracy: 0.7778\n",
            "Epoch 329/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3283 - accuracy: 0.8642 - val_loss: 0.3575 - val_accuracy: 0.8444\n",
            "Epoch 330/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2882 - accuracy: 0.8719 - val_loss: 0.3677 - val_accuracy: 0.8500\n",
            "Epoch 331/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3369 - accuracy: 0.8772 - val_loss: 0.3794 - val_accuracy: 0.8667\n",
            "Epoch 332/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2926 - accuracy: 0.8768 - val_loss: 0.4463 - val_accuracy: 0.8056\n",
            "Epoch 333/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3170 - accuracy: 0.8887 - val_loss: 0.6096 - val_accuracy: 0.7556\n",
            "Epoch 334/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3456 - accuracy: 0.8441 - val_loss: 0.4595 - val_accuracy: 0.8056\n",
            "Epoch 335/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3057 - accuracy: 0.8425 - val_loss: 0.9074 - val_accuracy: 0.6389\n",
            "Epoch 336/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.5085 - accuracy: 0.8029 - val_loss: 0.3947 - val_accuracy: 0.8222\n",
            "Epoch 337/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3429 - accuracy: 0.8559 - val_loss: 0.5635 - val_accuracy: 0.8000\n",
            "Epoch 338/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3386 - accuracy: 0.8277 - val_loss: 0.4311 - val_accuracy: 0.8333\n",
            "Epoch 339/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3187 - accuracy: 0.8599 - val_loss: 0.3693 - val_accuracy: 0.8389\n",
            "Epoch 340/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2893 - accuracy: 0.8904 - val_loss: 1.2499 - val_accuracy: 0.7500\n",
            "Epoch 341/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.4940 - accuracy: 0.8301 - val_loss: 0.4322 - val_accuracy: 0.8333\n",
            "Epoch 342/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.4239 - accuracy: 0.8140 - val_loss: 0.3757 - val_accuracy: 0.8167\n",
            "Epoch 343/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3405 - accuracy: 0.8640 - val_loss: 0.6598 - val_accuracy: 0.7389\n",
            "Epoch 344/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.4609 - accuracy: 0.8432 - val_loss: 0.5193 - val_accuracy: 0.7833\n",
            "Epoch 345/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3382 - accuracy: 0.8697 - val_loss: 0.3836 - val_accuracy: 0.8611\n",
            "Epoch 346/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3037 - accuracy: 0.8780 - val_loss: 0.3623 - val_accuracy: 0.8722\n",
            "Epoch 347/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2831 - accuracy: 0.8768 - val_loss: 0.4187 - val_accuracy: 0.8111\n",
            "Epoch 348/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3888 - accuracy: 0.8500 - val_loss: 0.7460 - val_accuracy: 0.7278\n",
            "Epoch 349/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.6293 - accuracy: 0.7941 - val_loss: 0.4946 - val_accuracy: 0.8056\n",
            "Epoch 350/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3139 - accuracy: 0.8793 - val_loss: 0.3697 - val_accuracy: 0.8556\n",
            "Epoch 351/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2820 - accuracy: 0.8764 - val_loss: 0.6133 - val_accuracy: 0.7556\n",
            "Epoch 352/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3151 - accuracy: 0.8792 - val_loss: 0.3987 - val_accuracy: 0.8444\n",
            "Epoch 353/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3219 - accuracy: 0.8875 - val_loss: 0.4093 - val_accuracy: 0.8278\n",
            "Epoch 354/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.4103 - accuracy: 0.8247 - val_loss: 0.3782 - val_accuracy: 0.8278\n",
            "Epoch 355/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3136 - accuracy: 0.8782 - val_loss: 0.3562 - val_accuracy: 0.8722\n",
            "Epoch 356/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3183 - accuracy: 0.8687 - val_loss: 0.4328 - val_accuracy: 0.8222\n",
            "Epoch 357/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3639 - accuracy: 0.8384 - val_loss: 0.3530 - val_accuracy: 0.8444\n",
            "Epoch 358/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3102 - accuracy: 0.8783 - val_loss: 0.3797 - val_accuracy: 0.8111\n",
            "Epoch 359/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.4856 - accuracy: 0.8212 - val_loss: 0.8478 - val_accuracy: 0.7111\n",
            "Epoch 360/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.4810 - accuracy: 0.8131 - val_loss: 0.3689 - val_accuracy: 0.8500\n",
            "Epoch 361/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2476 - accuracy: 0.8863 - val_loss: 0.5939 - val_accuracy: 0.7056\n",
            "Epoch 362/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3454 - accuracy: 0.8291 - val_loss: 0.3554 - val_accuracy: 0.8333\n",
            "Epoch 363/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2503 - accuracy: 0.8934 - val_loss: 0.3651 - val_accuracy: 0.8333\n",
            "Epoch 364/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2944 - accuracy: 0.8684 - val_loss: 0.3669 - val_accuracy: 0.8222\n",
            "Epoch 365/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3750 - accuracy: 0.8346 - val_loss: 0.4065 - val_accuracy: 0.8389\n",
            "Epoch 366/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2528 - accuracy: 0.9001 - val_loss: 0.5785 - val_accuracy: 0.8000\n",
            "Epoch 367/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3738 - accuracy: 0.8476 - val_loss: 0.3544 - val_accuracy: 0.8333\n",
            "Epoch 368/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3145 - accuracy: 0.8581 - val_loss: 0.4777 - val_accuracy: 0.8000\n",
            "Epoch 369/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2966 - accuracy: 0.8608 - val_loss: 0.8153 - val_accuracy: 0.7556\n",
            "Epoch 370/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.4655 - accuracy: 0.8153 - val_loss: 0.3708 - val_accuracy: 0.8500\n",
            "Epoch 371/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3351 - accuracy: 0.8694 - val_loss: 0.3852 - val_accuracy: 0.8556\n",
            "Epoch 372/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3810 - accuracy: 0.8383 - val_loss: 0.4544 - val_accuracy: 0.7889\n",
            "Epoch 373/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3601 - accuracy: 0.8579 - val_loss: 0.3449 - val_accuracy: 0.8722\n",
            "Epoch 374/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3170 - accuracy: 0.8547 - val_loss: 0.3809 - val_accuracy: 0.8222\n",
            "Epoch 375/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2942 - accuracy: 0.8976 - val_loss: 0.5429 - val_accuracy: 0.7889\n",
            "Epoch 376/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.4400 - accuracy: 0.8398 - val_loss: 0.6624 - val_accuracy: 0.8111\n",
            "Epoch 377/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3336 - accuracy: 0.8689 - val_loss: 0.3356 - val_accuracy: 0.8500\n",
            "Epoch 378/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2825 - accuracy: 0.8774 - val_loss: 0.3625 - val_accuracy: 0.8333\n",
            "Epoch 379/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2091 - accuracy: 0.9223 - val_loss: 0.4809 - val_accuracy: 0.7833\n",
            "Epoch 380/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2872 - accuracy: 0.8652 - val_loss: 0.3576 - val_accuracy: 0.8833\n",
            "Epoch 381/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.5455 - accuracy: 0.8371 - val_loss: 0.3717 - val_accuracy: 0.8389\n",
            "Epoch 382/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.3079 - accuracy: 0.8830 - val_loss: 0.6731 - val_accuracy: 0.7500\n",
            "Epoch 383/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3888 - accuracy: 0.8359 - val_loss: 0.3906 - val_accuracy: 0.8444\n",
            "Epoch 384/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.3387 - accuracy: 0.8709 - val_loss: 0.4060 - val_accuracy: 0.8389\n",
            "Epoch 385/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2641 - accuracy: 0.8845 - val_loss: 0.3444 - val_accuracy: 0.8556\n",
            "Epoch 386/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3306 - accuracy: 0.8750 - val_loss: 1.1664 - val_accuracy: 0.7500\n",
            "Epoch 387/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.4129 - accuracy: 0.8498 - val_loss: 0.3489 - val_accuracy: 0.8556\n",
            "Epoch 388/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2750 - accuracy: 0.8616 - val_loss: 0.4794 - val_accuracy: 0.8056\n",
            "Epoch 389/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2623 - accuracy: 0.8793 - val_loss: 0.4150 - val_accuracy: 0.8278\n",
            "Epoch 390/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2603 - accuracy: 0.9037 - val_loss: 0.4070 - val_accuracy: 0.8222\n",
            "Epoch 391/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3460 - accuracy: 0.8634 - val_loss: 0.4071 - val_accuracy: 0.8222\n",
            "Epoch 392/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2426 - accuracy: 0.9032 - val_loss: 0.6430 - val_accuracy: 0.7333\n",
            "Epoch 393/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3866 - accuracy: 0.8296 - val_loss: 0.4358 - val_accuracy: 0.8167\n",
            "Epoch 394/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3092 - accuracy: 0.8467 - val_loss: 0.3599 - val_accuracy: 0.8222\n",
            "Epoch 395/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2836 - accuracy: 0.8936 - val_loss: 0.3915 - val_accuracy: 0.8278\n",
            "Epoch 396/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2761 - accuracy: 0.8870 - val_loss: 0.5032 - val_accuracy: 0.7944\n",
            "Epoch 397/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3108 - accuracy: 0.8672 - val_loss: 0.3594 - val_accuracy: 0.8389\n",
            "Epoch 398/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2624 - accuracy: 0.9024 - val_loss: 0.4316 - val_accuracy: 0.8278\n",
            "Epoch 399/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2520 - accuracy: 0.9136 - val_loss: 0.4337 - val_accuracy: 0.8056\n",
            "Epoch 400/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3227 - accuracy: 0.8547 - val_loss: 0.9688 - val_accuracy: 0.7611\n",
            "Epoch 401/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.4828 - accuracy: 0.8513 - val_loss: 0.4635 - val_accuracy: 0.8111\n",
            "Epoch 402/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2664 - accuracy: 0.8940 - val_loss: 0.4376 - val_accuracy: 0.8333\n",
            "Epoch 403/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2957 - accuracy: 0.8843 - val_loss: 0.4761 - val_accuracy: 0.8333\n",
            "Epoch 404/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.3217 - accuracy: 0.8417 - val_loss: 0.4892 - val_accuracy: 0.8278\n",
            "Epoch 405/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2916 - accuracy: 0.8730 - val_loss: 0.4314 - val_accuracy: 0.8333\n",
            "Epoch 406/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2910 - accuracy: 0.8823 - val_loss: 0.8667 - val_accuracy: 0.7500\n",
            "Epoch 407/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.5265 - accuracy: 0.8294 - val_loss: 0.7333 - val_accuracy: 0.7222\n",
            "Epoch 408/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.6075 - accuracy: 0.8116 - val_loss: 0.4548 - val_accuracy: 0.8222\n",
            "Epoch 409/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2630 - accuracy: 0.8934 - val_loss: 0.7461 - val_accuracy: 0.7500\n",
            "Epoch 410/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3825 - accuracy: 0.8435 - val_loss: 0.5995 - val_accuracy: 0.8056\n",
            "Epoch 411/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.4995 - accuracy: 0.8295 - val_loss: 0.3466 - val_accuracy: 0.8444\n",
            "Epoch 412/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3663 - accuracy: 0.8543 - val_loss: 0.4569 - val_accuracy: 0.8222\n",
            "Epoch 413/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2507 - accuracy: 0.8908 - val_loss: 0.3738 - val_accuracy: 0.8389\n",
            "Epoch 414/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3060 - accuracy: 0.8712 - val_loss: 0.3911 - val_accuracy: 0.8333\n",
            "Epoch 415/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2922 - accuracy: 0.8944 - val_loss: 0.4617 - val_accuracy: 0.8333\n",
            "Epoch 416/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3596 - accuracy: 0.8770 - val_loss: 0.3722 - val_accuracy: 0.8500\n",
            "Epoch 417/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3440 - accuracy: 0.8722 - val_loss: 0.3928 - val_accuracy: 0.8389\n",
            "Epoch 418/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2835 - accuracy: 0.8741 - val_loss: 0.5383 - val_accuracy: 0.8111\n",
            "Epoch 419/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3102 - accuracy: 0.8766 - val_loss: 0.6645 - val_accuracy: 0.7111\n",
            "Epoch 420/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3253 - accuracy: 0.8554 - val_loss: 0.5102 - val_accuracy: 0.7944\n",
            "Epoch 421/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2483 - accuracy: 0.8917 - val_loss: 0.4929 - val_accuracy: 0.8056\n",
            "Epoch 422/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3034 - accuracy: 0.8675 - val_loss: 0.6475 - val_accuracy: 0.7889\n",
            "Epoch 423/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3364 - accuracy: 0.8847 - val_loss: 0.3396 - val_accuracy: 0.8556\n",
            "Epoch 424/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3556 - accuracy: 0.8425 - val_loss: 0.5587 - val_accuracy: 0.7667\n",
            "Epoch 425/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2315 - accuracy: 0.8897 - val_loss: 0.3712 - val_accuracy: 0.8278\n",
            "Epoch 426/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2803 - accuracy: 0.8769 - val_loss: 0.3828 - val_accuracy: 0.8222\n",
            "Epoch 427/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3081 - accuracy: 0.8721 - val_loss: 0.5102 - val_accuracy: 0.7611\n",
            "Epoch 428/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2747 - accuracy: 0.8686 - val_loss: 0.3842 - val_accuracy: 0.8556\n",
            "Epoch 429/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.4010 - accuracy: 0.8416 - val_loss: 0.3940 - val_accuracy: 0.8500\n",
            "Epoch 430/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3489 - accuracy: 0.8477 - val_loss: 0.4483 - val_accuracy: 0.8222\n",
            "Epoch 431/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2834 - accuracy: 0.8759 - val_loss: 0.4179 - val_accuracy: 0.8278\n",
            "Epoch 432/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3490 - accuracy: 0.8629 - val_loss: 0.6061 - val_accuracy: 0.8000\n",
            "Epoch 433/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3985 - accuracy: 0.8419 - val_loss: 0.3867 - val_accuracy: 0.8389\n",
            "Epoch 434/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3057 - accuracy: 0.8756 - val_loss: 0.3511 - val_accuracy: 0.8722\n",
            "Epoch 435/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2420 - accuracy: 0.9166 - val_loss: 0.3850 - val_accuracy: 0.8333\n",
            "Epoch 436/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3152 - accuracy: 0.8793 - val_loss: 0.3463 - val_accuracy: 0.8722\n",
            "Epoch 437/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2616 - accuracy: 0.8847 - val_loss: 0.4399 - val_accuracy: 0.8389\n",
            "Epoch 438/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3113 - accuracy: 0.8614 - val_loss: 0.3950 - val_accuracy: 0.8444\n",
            "Epoch 439/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2746 - accuracy: 0.8946 - val_loss: 0.3614 - val_accuracy: 0.8556\n",
            "Epoch 440/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2247 - accuracy: 0.9029 - val_loss: 0.4546 - val_accuracy: 0.8056\n",
            "Epoch 441/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3106 - accuracy: 0.8686 - val_loss: 0.3491 - val_accuracy: 0.8556\n",
            "Epoch 442/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3203 - accuracy: 0.8578 - val_loss: 0.3779 - val_accuracy: 0.8278\n",
            "Epoch 443/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2722 - accuracy: 0.8875 - val_loss: 0.3569 - val_accuracy: 0.8389\n",
            "Epoch 444/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2952 - accuracy: 0.8912 - val_loss: 0.3817 - val_accuracy: 0.8278\n",
            "Epoch 445/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3357 - accuracy: 0.8656 - val_loss: 0.3459 - val_accuracy: 0.8333\n",
            "Epoch 446/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2246 - accuracy: 0.9142 - val_loss: 0.3603 - val_accuracy: 0.8556\n",
            "Epoch 447/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3029 - accuracy: 0.8639 - val_loss: 0.3558 - val_accuracy: 0.8333\n",
            "Epoch 448/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2614 - accuracy: 0.8803 - val_loss: 0.3964 - val_accuracy: 0.8500\n",
            "Epoch 449/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2898 - accuracy: 0.8630 - val_loss: 0.3702 - val_accuracy: 0.8333\n",
            "Epoch 450/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2358 - accuracy: 0.8980 - val_loss: 0.3606 - val_accuracy: 0.8667\n",
            "Epoch 451/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2948 - accuracy: 0.8644 - val_loss: 0.5231 - val_accuracy: 0.7889\n",
            "Epoch 452/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2918 - accuracy: 0.8615 - val_loss: 0.7744 - val_accuracy: 0.7556\n",
            "Epoch 453/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3556 - accuracy: 0.8577 - val_loss: 0.3426 - val_accuracy: 0.8500\n",
            "Epoch 454/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2484 - accuracy: 0.9012 - val_loss: 0.7568 - val_accuracy: 0.7333\n",
            "Epoch 455/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3774 - accuracy: 0.8550 - val_loss: 0.5858 - val_accuracy: 0.7611\n",
            "Epoch 456/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3182 - accuracy: 0.8662 - val_loss: 0.3915 - val_accuracy: 0.8500\n",
            "Epoch 457/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3310 - accuracy: 0.8580 - val_loss: 0.3853 - val_accuracy: 0.8556\n",
            "Epoch 458/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2564 - accuracy: 0.8706 - val_loss: 0.3671 - val_accuracy: 0.8444\n",
            "Epoch 459/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2983 - accuracy: 0.8960 - val_loss: 0.3794 - val_accuracy: 0.8389\n",
            "Epoch 460/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2880 - accuracy: 0.8851 - val_loss: 0.4375 - val_accuracy: 0.8444\n",
            "Epoch 461/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2168 - accuracy: 0.9230 - val_loss: 0.4180 - val_accuracy: 0.8111\n",
            "Epoch 462/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.3738 - accuracy: 0.8529 - val_loss: 0.3529 - val_accuracy: 0.8556\n",
            "Epoch 463/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2959 - accuracy: 0.8879 - val_loss: 0.3859 - val_accuracy: 0.8167\n",
            "Epoch 464/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3073 - accuracy: 0.8759 - val_loss: 0.3703 - val_accuracy: 0.8111\n",
            "Epoch 465/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2400 - accuracy: 0.8877 - val_loss: 0.3487 - val_accuracy: 0.8444\n",
            "Epoch 466/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2767 - accuracy: 0.8531 - val_loss: 0.3859 - val_accuracy: 0.8167\n",
            "Epoch 467/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3297 - accuracy: 0.8562 - val_loss: 0.4272 - val_accuracy: 0.8389\n",
            "Epoch 468/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.4287 - accuracy: 0.8469 - val_loss: 0.3948 - val_accuracy: 0.8278\n",
            "Epoch 469/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2580 - accuracy: 0.8885 - val_loss: 0.4179 - val_accuracy: 0.8222\n",
            "Epoch 470/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3561 - accuracy: 0.8730 - val_loss: 0.3970 - val_accuracy: 0.8444\n",
            "Epoch 471/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.3190 - accuracy: 0.8755 - val_loss: 0.4126 - val_accuracy: 0.8278\n",
            "Epoch 472/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2733 - accuracy: 0.8837 - val_loss: 0.4215 - val_accuracy: 0.8222\n",
            "Epoch 473/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3364 - accuracy: 0.8592 - val_loss: 0.5619 - val_accuracy: 0.8056\n",
            "Epoch 474/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2621 - accuracy: 0.8789 - val_loss: 0.4059 - val_accuracy: 0.8500\n",
            "Epoch 475/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2088 - accuracy: 0.9221 - val_loss: 0.4117 - val_accuracy: 0.8278\n",
            "Epoch 476/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2396 - accuracy: 0.8928 - val_loss: 0.3802 - val_accuracy: 0.8556\n",
            "Epoch 477/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2755 - accuracy: 0.8651 - val_loss: 0.3886 - val_accuracy: 0.8278\n",
            "Epoch 478/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2172 - accuracy: 0.9188 - val_loss: 0.3779 - val_accuracy: 0.8333\n",
            "Epoch 479/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2499 - accuracy: 0.8793 - val_loss: 0.3895 - val_accuracy: 0.8500\n",
            "Epoch 480/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.3087 - accuracy: 0.8630 - val_loss: 0.4146 - val_accuracy: 0.8167\n",
            "Epoch 481/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2990 - accuracy: 0.8590 - val_loss: 0.4262 - val_accuracy: 0.8222\n",
            "Epoch 482/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2520 - accuracy: 0.8877 - val_loss: 0.3856 - val_accuracy: 0.8444\n",
            "Epoch 483/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2357 - accuracy: 0.9078 - val_loss: 0.4393 - val_accuracy: 0.8167\n",
            "Epoch 484/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3101 - accuracy: 0.8614 - val_loss: 0.3767 - val_accuracy: 0.8500\n",
            "Epoch 485/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2743 - accuracy: 0.8855 - val_loss: 0.3586 - val_accuracy: 0.8444\n",
            "Epoch 486/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2595 - accuracy: 0.8917 - val_loss: 0.4033 - val_accuracy: 0.8278\n",
            "Epoch 487/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2488 - accuracy: 0.8940 - val_loss: 0.3690 - val_accuracy: 0.8556\n",
            "Epoch 488/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2633 - accuracy: 0.9034 - val_loss: 0.3889 - val_accuracy: 0.8167\n",
            "Epoch 489/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3232 - accuracy: 0.8527 - val_loss: 0.3681 - val_accuracy: 0.8222\n",
            "Epoch 490/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2727 - accuracy: 0.8880 - val_loss: 0.3481 - val_accuracy: 0.8667\n",
            "Epoch 491/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2646 - accuracy: 0.8994 - val_loss: 0.5634 - val_accuracy: 0.7722\n",
            "Epoch 492/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2998 - accuracy: 0.8764 - val_loss: 0.3956 - val_accuracy: 0.8111\n",
            "Epoch 493/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2978 - accuracy: 0.8691 - val_loss: 0.3511 - val_accuracy: 0.8389\n",
            "Epoch 494/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2710 - accuracy: 0.8923 - val_loss: 0.3406 - val_accuracy: 0.8778\n",
            "Epoch 495/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3167 - accuracy: 0.8854 - val_loss: 0.3803 - val_accuracy: 0.8222\n",
            "Epoch 496/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2339 - accuracy: 0.9057 - val_loss: 0.4021 - val_accuracy: 0.8333\n",
            "Epoch 497/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3547 - accuracy: 0.8533 - val_loss: 0.3777 - val_accuracy: 0.8611\n",
            "Epoch 498/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2389 - accuracy: 0.8941 - val_loss: 0.3703 - val_accuracy: 0.8111\n",
            "Epoch 499/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2066 - accuracy: 0.9176 - val_loss: 0.3602 - val_accuracy: 0.8389\n",
            "Epoch 500/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2705 - accuracy: 0.8793 - val_loss: 0.3635 - val_accuracy: 0.8667\n",
            "Epoch 501/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2587 - accuracy: 0.8805 - val_loss: 0.3841 - val_accuracy: 0.8667\n",
            "Epoch 502/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3605 - accuracy: 0.8847 - val_loss: 0.4329 - val_accuracy: 0.8389\n",
            "Epoch 503/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2629 - accuracy: 0.9090 - val_loss: 0.3985 - val_accuracy: 0.8167\n",
            "Epoch 504/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2618 - accuracy: 0.8834 - val_loss: 0.3766 - val_accuracy: 0.8389\n",
            "Epoch 505/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2512 - accuracy: 0.8881 - val_loss: 0.3573 - val_accuracy: 0.8722\n",
            "Epoch 506/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3137 - accuracy: 0.8678 - val_loss: 0.4905 - val_accuracy: 0.8056\n",
            "Epoch 507/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2619 - accuracy: 0.9076 - val_loss: 0.3827 - val_accuracy: 0.8500\n",
            "Epoch 508/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2605 - accuracy: 0.9110 - val_loss: 0.4587 - val_accuracy: 0.8111\n",
            "Epoch 509/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3366 - accuracy: 0.8535 - val_loss: 0.4224 - val_accuracy: 0.8278\n",
            "Epoch 510/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2502 - accuracy: 0.9012 - val_loss: 0.4049 - val_accuracy: 0.8278\n",
            "Epoch 511/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2439 - accuracy: 0.8754 - val_loss: 0.3892 - val_accuracy: 0.8389\n",
            "Epoch 512/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2525 - accuracy: 0.8868 - val_loss: 0.4173 - val_accuracy: 0.8278\n",
            "Epoch 513/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2665 - accuracy: 0.8717 - val_loss: 0.3981 - val_accuracy: 0.8278\n",
            "Epoch 514/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2464 - accuracy: 0.9058 - val_loss: 0.4189 - val_accuracy: 0.8167\n",
            "Epoch 515/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.4237 - accuracy: 0.8012 - val_loss: 0.4123 - val_accuracy: 0.8222\n",
            "Epoch 516/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3203 - accuracy: 0.8516 - val_loss: 0.4788 - val_accuracy: 0.8222\n",
            "Epoch 517/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2882 - accuracy: 0.8839 - val_loss: 0.4008 - val_accuracy: 0.8556\n",
            "Epoch 518/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2950 - accuracy: 0.8525 - val_loss: 0.3647 - val_accuracy: 0.8056\n",
            "Epoch 519/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3433 - accuracy: 0.8536 - val_loss: 0.5200 - val_accuracy: 0.7722\n",
            "Epoch 520/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3170 - accuracy: 0.8566 - val_loss: 0.3813 - val_accuracy: 0.8000\n",
            "Epoch 521/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2621 - accuracy: 0.8858 - val_loss: 0.6273 - val_accuracy: 0.7611\n",
            "Epoch 522/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3003 - accuracy: 0.8634 - val_loss: 0.3722 - val_accuracy: 0.8389\n",
            "Epoch 523/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2473 - accuracy: 0.9115 - val_loss: 0.4181 - val_accuracy: 0.8278\n",
            "Epoch 524/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2685 - accuracy: 0.8820 - val_loss: 0.5177 - val_accuracy: 0.8000\n",
            "Epoch 525/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3364 - accuracy: 0.8377 - val_loss: 0.4433 - val_accuracy: 0.8444\n",
            "Epoch 526/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2359 - accuracy: 0.8850 - val_loss: 0.3634 - val_accuracy: 0.8444\n",
            "Epoch 527/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3368 - accuracy: 0.8759 - val_loss: 0.3877 - val_accuracy: 0.8500\n",
            "Epoch 528/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2619 - accuracy: 0.8806 - val_loss: 0.4094 - val_accuracy: 0.8556\n",
            "Epoch 529/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2413 - accuracy: 0.8968 - val_loss: 0.3467 - val_accuracy: 0.8556\n",
            "Epoch 530/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2341 - accuracy: 0.9173 - val_loss: 0.3571 - val_accuracy: 0.8556\n",
            "Epoch 531/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2080 - accuracy: 0.9032 - val_loss: 0.4099 - val_accuracy: 0.8500\n",
            "Epoch 532/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2106 - accuracy: 0.9054 - val_loss: 0.3949 - val_accuracy: 0.8611\n",
            "Epoch 533/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3298 - accuracy: 0.8449 - val_loss: 0.4107 - val_accuracy: 0.8389\n",
            "Epoch 534/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2556 - accuracy: 0.8983 - val_loss: 0.4475 - val_accuracy: 0.8333\n",
            "Epoch 535/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3189 - accuracy: 0.8468 - val_loss: 0.4249 - val_accuracy: 0.8111\n",
            "Epoch 536/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2864 - accuracy: 0.8531 - val_loss: 0.3709 - val_accuracy: 0.8167\n",
            "Epoch 537/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2517 - accuracy: 0.8924 - val_loss: 0.3956 - val_accuracy: 0.8167\n",
            "Epoch 538/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2402 - accuracy: 0.9124 - val_loss: 0.3803 - val_accuracy: 0.8278\n",
            "Epoch 539/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2640 - accuracy: 0.8849 - val_loss: 0.4302 - val_accuracy: 0.8167\n",
            "Epoch 540/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2623 - accuracy: 0.8741 - val_loss: 0.3906 - val_accuracy: 0.8222\n",
            "Epoch 541/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2230 - accuracy: 0.8976 - val_loss: 0.4512 - val_accuracy: 0.8167\n",
            "Epoch 542/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2525 - accuracy: 0.8872 - val_loss: 0.4155 - val_accuracy: 0.8278\n",
            "Epoch 543/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.1984 - accuracy: 0.9105 - val_loss: 0.4505 - val_accuracy: 0.8111\n",
            "Epoch 544/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2784 - accuracy: 0.8862 - val_loss: 0.4001 - val_accuracy: 0.8278\n",
            "Epoch 545/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2431 - accuracy: 0.9020 - val_loss: 0.3735 - val_accuracy: 0.8556\n",
            "Epoch 546/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2674 - accuracy: 0.8862 - val_loss: 0.3734 - val_accuracy: 0.8333\n",
            "Epoch 547/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2284 - accuracy: 0.9169 - val_loss: 0.3920 - val_accuracy: 0.8333\n",
            "Epoch 548/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2576 - accuracy: 0.8907 - val_loss: 0.4460 - val_accuracy: 0.8111\n",
            "Epoch 549/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2734 - accuracy: 0.8771 - val_loss: 0.4279 - val_accuracy: 0.8444\n",
            "Epoch 550/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2397 - accuracy: 0.9137 - val_loss: 0.3837 - val_accuracy: 0.8611\n",
            "Epoch 551/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2393 - accuracy: 0.8969 - val_loss: 0.3542 - val_accuracy: 0.8389\n",
            "Epoch 552/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2362 - accuracy: 0.9073 - val_loss: 0.4749 - val_accuracy: 0.8222\n",
            "Epoch 553/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2336 - accuracy: 0.8995 - val_loss: 0.3789 - val_accuracy: 0.8611\n",
            "Epoch 554/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2172 - accuracy: 0.9211 - val_loss: 0.3563 - val_accuracy: 0.8333\n",
            "Epoch 555/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2712 - accuracy: 0.8794 - val_loss: 0.3598 - val_accuracy: 0.8500\n",
            "Epoch 556/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2965 - accuracy: 0.8725 - val_loss: 0.3922 - val_accuracy: 0.8556\n",
            "Epoch 557/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2292 - accuracy: 0.8945 - val_loss: 0.3892 - val_accuracy: 0.8278\n",
            "Epoch 558/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2948 - accuracy: 0.8700 - val_loss: 0.3984 - val_accuracy: 0.8389\n",
            "Epoch 559/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2561 - accuracy: 0.9043 - val_loss: 0.3723 - val_accuracy: 0.8278\n",
            "Epoch 560/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2354 - accuracy: 0.9131 - val_loss: 0.3743 - val_accuracy: 0.8444\n",
            "Epoch 561/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2553 - accuracy: 0.9049 - val_loss: 0.3959 - val_accuracy: 0.8333\n",
            "Epoch 562/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3390 - accuracy: 0.8734 - val_loss: 0.5629 - val_accuracy: 0.8000\n",
            "Epoch 563/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2801 - accuracy: 0.8687 - val_loss: 0.4410 - val_accuracy: 0.8111\n",
            "Epoch 564/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2576 - accuracy: 0.8955 - val_loss: 0.7348 - val_accuracy: 0.7278\n",
            "Epoch 565/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.4924 - accuracy: 0.8151 - val_loss: 0.3660 - val_accuracy: 0.8556\n",
            "Epoch 566/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2673 - accuracy: 0.8807 - val_loss: 0.3614 - val_accuracy: 0.8444\n",
            "Epoch 567/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2635 - accuracy: 0.8856 - val_loss: 0.4926 - val_accuracy: 0.8111\n",
            "Epoch 568/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2706 - accuracy: 0.8938 - val_loss: 0.4124 - val_accuracy: 0.8444\n",
            "Epoch 569/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2744 - accuracy: 0.8718 - val_loss: 0.4433 - val_accuracy: 0.8222\n",
            "Epoch 570/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2856 - accuracy: 0.8845 - val_loss: 0.3813 - val_accuracy: 0.8222\n",
            "Epoch 571/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2775 - accuracy: 0.8953 - val_loss: 0.3633 - val_accuracy: 0.8611\n",
            "Epoch 572/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2281 - accuracy: 0.9052 - val_loss: 0.4234 - val_accuracy: 0.8000\n",
            "Epoch 573/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2517 - accuracy: 0.8814 - val_loss: 0.4108 - val_accuracy: 0.8278\n",
            "Epoch 574/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2763 - accuracy: 0.8754 - val_loss: 0.3882 - val_accuracy: 0.8611\n",
            "Epoch 575/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2519 - accuracy: 0.9047 - val_loss: 0.4972 - val_accuracy: 0.8222\n",
            "Epoch 576/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2638 - accuracy: 0.8840 - val_loss: 0.3981 - val_accuracy: 0.8222\n",
            "Epoch 577/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2712 - accuracy: 0.8704 - val_loss: 0.4989 - val_accuracy: 0.8056\n",
            "Epoch 578/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2685 - accuracy: 0.8879 - val_loss: 0.3726 - val_accuracy: 0.8111\n",
            "Epoch 579/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2863 - accuracy: 0.8720 - val_loss: 0.4079 - val_accuracy: 0.8444\n",
            "Epoch 580/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2211 - accuracy: 0.9067 - val_loss: 0.3699 - val_accuracy: 0.8556\n",
            "Epoch 581/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2337 - accuracy: 0.9149 - val_loss: 0.3954 - val_accuracy: 0.8222\n",
            "Epoch 582/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2627 - accuracy: 0.8914 - val_loss: 0.4509 - val_accuracy: 0.8222\n",
            "Epoch 583/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2654 - accuracy: 0.8921 - val_loss: 0.4240 - val_accuracy: 0.8389\n",
            "Epoch 584/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2437 - accuracy: 0.8857 - val_loss: 0.4709 - val_accuracy: 0.8167\n",
            "Epoch 585/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2376 - accuracy: 0.9071 - val_loss: 0.4170 - val_accuracy: 0.8278\n",
            "Epoch 586/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2483 - accuracy: 0.8819 - val_loss: 0.3719 - val_accuracy: 0.8500\n",
            "Epoch 587/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2599 - accuracy: 0.9020 - val_loss: 0.4894 - val_accuracy: 0.8333\n",
            "Epoch 588/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2528 - accuracy: 0.9093 - val_loss: 0.3590 - val_accuracy: 0.8333\n",
            "Epoch 589/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2736 - accuracy: 0.8768 - val_loss: 0.6145 - val_accuracy: 0.7556\n",
            "Epoch 590/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2612 - accuracy: 0.8994 - val_loss: 0.3947 - val_accuracy: 0.8111\n",
            "Epoch 591/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2837 - accuracy: 0.8897 - val_loss: 0.3946 - val_accuracy: 0.8333\n",
            "Epoch 592/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3549 - accuracy: 0.8462 - val_loss: 0.4928 - val_accuracy: 0.7944\n",
            "Epoch 593/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3200 - accuracy: 0.8784 - val_loss: 0.5025 - val_accuracy: 0.8111\n",
            "Epoch 594/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2844 - accuracy: 0.8634 - val_loss: 0.3776 - val_accuracy: 0.8444\n",
            "Epoch 595/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2116 - accuracy: 0.9177 - val_loss: 0.3664 - val_accuracy: 0.8389\n",
            "Epoch 596/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2247 - accuracy: 0.9224 - val_loss: 0.4076 - val_accuracy: 0.8278\n",
            "Epoch 597/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.1948 - accuracy: 0.9240 - val_loss: 0.3559 - val_accuracy: 0.8611\n",
            "Epoch 598/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.1841 - accuracy: 0.9368 - val_loss: 0.3809 - val_accuracy: 0.8278\n",
            "Epoch 599/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2184 - accuracy: 0.9040 - val_loss: 0.3455 - val_accuracy: 0.8444\n",
            "Epoch 600/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2515 - accuracy: 0.8865 - val_loss: 0.4104 - val_accuracy: 0.8222\n",
            "Epoch 601/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2613 - accuracy: 0.8828 - val_loss: 0.5462 - val_accuracy: 0.7889\n",
            "Epoch 602/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2658 - accuracy: 0.8864 - val_loss: 0.4915 - val_accuracy: 0.8000\n",
            "Epoch 603/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2668 - accuracy: 0.8833 - val_loss: 0.3485 - val_accuracy: 0.8556\n",
            "Epoch 604/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2265 - accuracy: 0.8958 - val_loss: 0.5231 - val_accuracy: 0.8111\n",
            "Epoch 605/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2857 - accuracy: 0.8971 - val_loss: 0.4783 - val_accuracy: 0.8444\n",
            "Epoch 606/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2789 - accuracy: 0.8725 - val_loss: 0.3872 - val_accuracy: 0.8556\n",
            "Epoch 607/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2323 - accuracy: 0.8941 - val_loss: 0.3768 - val_accuracy: 0.8556\n",
            "Epoch 608/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2603 - accuracy: 0.8844 - val_loss: 0.4011 - val_accuracy: 0.8222\n",
            "Epoch 609/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2481 - accuracy: 0.9004 - val_loss: 0.4985 - val_accuracy: 0.8222\n",
            "Epoch 610/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2513 - accuracy: 0.8805 - val_loss: 0.4865 - val_accuracy: 0.7944\n",
            "Epoch 611/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3148 - accuracy: 0.8599 - val_loss: 0.3777 - val_accuracy: 0.8500\n",
            "Epoch 612/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2948 - accuracy: 0.8742 - val_loss: 0.3751 - val_accuracy: 0.8556\n",
            "Epoch 613/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2592 - accuracy: 0.8802 - val_loss: 0.3899 - val_accuracy: 0.8389\n",
            "Epoch 614/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.1921 - accuracy: 0.9346 - val_loss: 0.3644 - val_accuracy: 0.8611\n",
            "Epoch 615/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2290 - accuracy: 0.9139 - val_loss: 0.4786 - val_accuracy: 0.8222\n",
            "Epoch 616/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2840 - accuracy: 0.8829 - val_loss: 0.4193 - val_accuracy: 0.8667\n",
            "Epoch 617/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2989 - accuracy: 0.8700 - val_loss: 0.3895 - val_accuracy: 0.8444\n",
            "Epoch 618/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2699 - accuracy: 0.8841 - val_loss: 0.3714 - val_accuracy: 0.8556\n",
            "Epoch 619/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2216 - accuracy: 0.9067 - val_loss: 0.3795 - val_accuracy: 0.8278\n",
            "Epoch 620/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2798 - accuracy: 0.8862 - val_loss: 0.5841 - val_accuracy: 0.7833\n",
            "Epoch 621/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2195 - accuracy: 0.9072 - val_loss: 0.3680 - val_accuracy: 0.8444\n",
            "Epoch 622/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2294 - accuracy: 0.8974 - val_loss: 0.4477 - val_accuracy: 0.8278\n",
            "Epoch 623/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2297 - accuracy: 0.8960 - val_loss: 0.3908 - val_accuracy: 0.8611\n",
            "Epoch 624/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.1852 - accuracy: 0.9394 - val_loss: 0.4129 - val_accuracy: 0.8333\n",
            "Epoch 625/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2522 - accuracy: 0.8907 - val_loss: 0.3787 - val_accuracy: 0.8500\n",
            "Epoch 626/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2405 - accuracy: 0.9084 - val_loss: 0.5456 - val_accuracy: 0.7833\n",
            "Epoch 627/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.3422 - accuracy: 0.8733 - val_loss: 0.3963 - val_accuracy: 0.8556\n",
            "Epoch 628/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2457 - accuracy: 0.8982 - val_loss: 0.3756 - val_accuracy: 0.8389\n",
            "Epoch 629/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2063 - accuracy: 0.9332 - val_loss: 0.4184 - val_accuracy: 0.8444\n",
            "Epoch 630/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2197 - accuracy: 0.9030 - val_loss: 0.3929 - val_accuracy: 0.8722\n",
            "Epoch 631/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2353 - accuracy: 0.9078 - val_loss: 0.4084 - val_accuracy: 0.8556\n",
            "Epoch 632/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2169 - accuracy: 0.9089 - val_loss: 0.6033 - val_accuracy: 0.8111\n",
            "Epoch 633/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3399 - accuracy: 0.8603 - val_loss: 0.5089 - val_accuracy: 0.8056\n",
            "Epoch 634/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2822 - accuracy: 0.8819 - val_loss: 0.4339 - val_accuracy: 0.8278\n",
            "Epoch 635/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2290 - accuracy: 0.8879 - val_loss: 0.3860 - val_accuracy: 0.8222\n",
            "Epoch 636/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2176 - accuracy: 0.9114 - val_loss: 0.3969 - val_accuracy: 0.8389\n",
            "Epoch 637/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2513 - accuracy: 0.9013 - val_loss: 0.5218 - val_accuracy: 0.8222\n",
            "Epoch 638/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2344 - accuracy: 0.9064 - val_loss: 0.4478 - val_accuracy: 0.8389\n",
            "Epoch 639/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2626 - accuracy: 0.8885 - val_loss: 0.4269 - val_accuracy: 0.8278\n",
            "Epoch 640/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2303 - accuracy: 0.9013 - val_loss: 0.4577 - val_accuracy: 0.8333\n",
            "Epoch 641/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2311 - accuracy: 0.8878 - val_loss: 0.5004 - val_accuracy: 0.7944\n",
            "Epoch 642/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2767 - accuracy: 0.8823 - val_loss: 0.3940 - val_accuracy: 0.8389\n",
            "Epoch 643/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2408 - accuracy: 0.9005 - val_loss: 0.4146 - val_accuracy: 0.8389\n",
            "Epoch 644/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2999 - accuracy: 0.8545 - val_loss: 0.5378 - val_accuracy: 0.8056\n",
            "Epoch 645/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2628 - accuracy: 0.9002 - val_loss: 0.4108 - val_accuracy: 0.8556\n",
            "Epoch 646/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2508 - accuracy: 0.8898 - val_loss: 0.4263 - val_accuracy: 0.8333\n",
            "Epoch 647/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3466 - accuracy: 0.8428 - val_loss: 0.3960 - val_accuracy: 0.8167\n",
            "Epoch 648/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2958 - accuracy: 0.8683 - val_loss: 0.4280 - val_accuracy: 0.8333\n",
            "Epoch 649/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2328 - accuracy: 0.8991 - val_loss: 0.3659 - val_accuracy: 0.8333\n",
            "Epoch 650/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2092 - accuracy: 0.9211 - val_loss: 0.3797 - val_accuracy: 0.8556\n",
            "Epoch 651/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2055 - accuracy: 0.9261 - val_loss: 0.4709 - val_accuracy: 0.8056\n",
            "Epoch 652/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2389 - accuracy: 0.8998 - val_loss: 0.4194 - val_accuracy: 0.8389\n",
            "Epoch 653/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2869 - accuracy: 0.8880 - val_loss: 0.4444 - val_accuracy: 0.8500\n",
            "Epoch 654/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2314 - accuracy: 0.8978 - val_loss: 0.3628 - val_accuracy: 0.8667\n",
            "Epoch 655/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2345 - accuracy: 0.9062 - val_loss: 0.3809 - val_accuracy: 0.8389\n",
            "Epoch 656/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2479 - accuracy: 0.8832 - val_loss: 0.4145 - val_accuracy: 0.8444\n",
            "Epoch 657/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2453 - accuracy: 0.8936 - val_loss: 0.4141 - val_accuracy: 0.8278\n",
            "Epoch 658/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2480 - accuracy: 0.8940 - val_loss: 0.4083 - val_accuracy: 0.8389\n",
            "Epoch 659/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.1977 - accuracy: 0.9266 - val_loss: 0.4095 - val_accuracy: 0.8500\n",
            "Epoch 660/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2181 - accuracy: 0.8988 - val_loss: 0.5894 - val_accuracy: 0.7778\n",
            "Epoch 661/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2935 - accuracy: 0.8672 - val_loss: 0.4582 - val_accuracy: 0.8056\n",
            "Epoch 662/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2947 - accuracy: 0.8937 - val_loss: 0.3783 - val_accuracy: 0.8667\n",
            "Epoch 663/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2420 - accuracy: 0.8975 - val_loss: 0.4054 - val_accuracy: 0.8333\n",
            "Epoch 664/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2984 - accuracy: 0.8759 - val_loss: 0.4262 - val_accuracy: 0.8389\n",
            "Epoch 665/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2536 - accuracy: 0.8808 - val_loss: 0.5233 - val_accuracy: 0.7944\n",
            "Epoch 666/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.3415 - accuracy: 0.8568 - val_loss: 0.3999 - val_accuracy: 0.8222\n",
            "Epoch 667/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2675 - accuracy: 0.8763 - val_loss: 0.4032 - val_accuracy: 0.8444\n",
            "Epoch 668/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2708 - accuracy: 0.8982 - val_loss: 0.4326 - val_accuracy: 0.8389\n",
            "Epoch 669/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.1973 - accuracy: 0.9187 - val_loss: 0.3633 - val_accuracy: 0.8611\n",
            "Epoch 670/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2237 - accuracy: 0.9088 - val_loss: 0.4590 - val_accuracy: 0.8167\n",
            "Epoch 671/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2737 - accuracy: 0.8859 - val_loss: 0.3808 - val_accuracy: 0.8500\n",
            "Epoch 672/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2290 - accuracy: 0.8886 - val_loss: 0.4367 - val_accuracy: 0.8167\n",
            "Epoch 673/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2327 - accuracy: 0.9076 - val_loss: 0.4052 - val_accuracy: 0.8333\n",
            "Epoch 674/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2183 - accuracy: 0.9000 - val_loss: 0.3761 - val_accuracy: 0.8500\n",
            "Epoch 675/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2221 - accuracy: 0.9092 - val_loss: 0.4062 - val_accuracy: 0.8278\n",
            "Epoch 676/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2498 - accuracy: 0.8853 - val_loss: 0.4199 - val_accuracy: 0.8389\n",
            "Epoch 677/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3384 - accuracy: 0.8509 - val_loss: 0.3549 - val_accuracy: 0.8556\n",
            "Epoch 678/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2134 - accuracy: 0.9162 - val_loss: 0.3925 - val_accuracy: 0.8500\n",
            "Epoch 679/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2598 - accuracy: 0.8976 - val_loss: 0.4516 - val_accuracy: 0.8333\n",
            "Epoch 680/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2702 - accuracy: 0.8800 - val_loss: 0.3964 - val_accuracy: 0.8333\n",
            "Epoch 681/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2021 - accuracy: 0.9292 - val_loss: 0.4385 - val_accuracy: 0.8167\n",
            "Epoch 682/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2783 - accuracy: 0.9054 - val_loss: 0.4199 - val_accuracy: 0.8222\n",
            "Epoch 683/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2644 - accuracy: 0.8976 - val_loss: 0.4581 - val_accuracy: 0.8333\n",
            "Epoch 684/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2915 - accuracy: 0.8748 - val_loss: 0.4262 - val_accuracy: 0.8111\n",
            "Epoch 685/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3019 - accuracy: 0.8676 - val_loss: 0.3865 - val_accuracy: 0.8333\n",
            "Epoch 686/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2603 - accuracy: 0.8811 - val_loss: 0.4120 - val_accuracy: 0.8611\n",
            "Epoch 687/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2377 - accuracy: 0.9022 - val_loss: 0.3924 - val_accuracy: 0.8333\n",
            "Epoch 688/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2168 - accuracy: 0.9128 - val_loss: 0.4246 - val_accuracy: 0.8444\n",
            "Epoch 689/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2388 - accuracy: 0.9109 - val_loss: 0.3619 - val_accuracy: 0.8500\n",
            "Epoch 690/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2166 - accuracy: 0.9295 - val_loss: 0.3903 - val_accuracy: 0.8611\n",
            "Epoch 691/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2057 - accuracy: 0.9034 - val_loss: 0.3888 - val_accuracy: 0.8500\n",
            "Epoch 692/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2565 - accuracy: 0.8986 - val_loss: 0.4217 - val_accuracy: 0.8389\n",
            "Epoch 693/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2174 - accuracy: 0.9178 - val_loss: 0.3860 - val_accuracy: 0.8444\n",
            "Epoch 694/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2361 - accuracy: 0.8978 - val_loss: 0.3906 - val_accuracy: 0.8500\n",
            "Epoch 695/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2469 - accuracy: 0.8985 - val_loss: 0.4456 - val_accuracy: 0.8500\n",
            "Epoch 696/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2533 - accuracy: 0.8921 - val_loss: 0.3749 - val_accuracy: 0.8500\n",
            "Epoch 697/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2391 - accuracy: 0.9011 - val_loss: 0.3899 - val_accuracy: 0.8556\n",
            "Epoch 698/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2024 - accuracy: 0.9131 - val_loss: 0.4027 - val_accuracy: 0.8389\n",
            "Epoch 699/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.1980 - accuracy: 0.9233 - val_loss: 0.5399 - val_accuracy: 0.8278\n",
            "Epoch 700/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2711 - accuracy: 0.8663 - val_loss: 0.4271 - val_accuracy: 0.8444\n",
            "Epoch 701/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2657 - accuracy: 0.8745 - val_loss: 0.4461 - val_accuracy: 0.8333\n",
            "Epoch 702/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3186 - accuracy: 0.8589 - val_loss: 0.4717 - val_accuracy: 0.8389\n",
            "Epoch 703/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2179 - accuracy: 0.9135 - val_loss: 0.4214 - val_accuracy: 0.8556\n",
            "Epoch 704/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2046 - accuracy: 0.9175 - val_loss: 0.4265 - val_accuracy: 0.8167\n",
            "Epoch 705/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2323 - accuracy: 0.9003 - val_loss: 0.4164 - val_accuracy: 0.8500\n",
            "Epoch 706/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2611 - accuracy: 0.8875 - val_loss: 0.5278 - val_accuracy: 0.7833\n",
            "Epoch 707/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2225 - accuracy: 0.8936 - val_loss: 0.5279 - val_accuracy: 0.7833\n",
            "Epoch 708/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3215 - accuracy: 0.8627 - val_loss: 0.3742 - val_accuracy: 0.8500\n",
            "Epoch 709/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2467 - accuracy: 0.9037 - val_loss: 0.4111 - val_accuracy: 0.8333\n",
            "Epoch 710/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2289 - accuracy: 0.9036 - val_loss: 0.3517 - val_accuracy: 0.8556\n",
            "Epoch 711/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2061 - accuracy: 0.9173 - val_loss: 0.3706 - val_accuracy: 0.8722\n",
            "Epoch 712/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2614 - accuracy: 0.8872 - val_loss: 0.3338 - val_accuracy: 0.8722\n",
            "Epoch 713/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2452 - accuracy: 0.9247 - val_loss: 0.3833 - val_accuracy: 0.8444\n",
            "Epoch 714/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2091 - accuracy: 0.8917 - val_loss: 0.3697 - val_accuracy: 0.8500\n",
            "Epoch 715/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2639 - accuracy: 0.8825 - val_loss: 0.5186 - val_accuracy: 0.8167\n",
            "Epoch 716/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3004 - accuracy: 0.8560 - val_loss: 0.4590 - val_accuracy: 0.8333\n",
            "Epoch 717/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2589 - accuracy: 0.8664 - val_loss: 0.3566 - val_accuracy: 0.8500\n",
            "Epoch 718/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2845 - accuracy: 0.8878 - val_loss: 0.3991 - val_accuracy: 0.8389\n",
            "Epoch 719/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2323 - accuracy: 0.9150 - val_loss: 0.3812 - val_accuracy: 0.8611\n",
            "Epoch 720/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2999 - accuracy: 0.8723 - val_loss: 0.3504 - val_accuracy: 0.8500\n",
            "Epoch 721/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2154 - accuracy: 0.9281 - val_loss: 0.4324 - val_accuracy: 0.8222\n",
            "Epoch 722/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2901 - accuracy: 0.8638 - val_loss: 0.4932 - val_accuracy: 0.8333\n",
            "Epoch 723/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2315 - accuracy: 0.9222 - val_loss: 0.4756 - val_accuracy: 0.8278\n",
            "Epoch 724/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2032 - accuracy: 0.9174 - val_loss: 0.4899 - val_accuracy: 0.8333\n",
            "Epoch 725/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2826 - accuracy: 0.8918 - val_loss: 0.3994 - val_accuracy: 0.8389\n",
            "Epoch 726/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2150 - accuracy: 0.9286 - val_loss: 0.4028 - val_accuracy: 0.8444\n",
            "Epoch 727/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2222 - accuracy: 0.9142 - val_loss: 0.4047 - val_accuracy: 0.8500\n",
            "Epoch 728/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2147 - accuracy: 0.9071 - val_loss: 0.4708 - val_accuracy: 0.8333\n",
            "Epoch 729/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2658 - accuracy: 0.8784 - val_loss: 0.3998 - val_accuracy: 0.8611\n",
            "Epoch 730/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2269 - accuracy: 0.9154 - val_loss: 0.4322 - val_accuracy: 0.8500\n",
            "Epoch 731/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2776 - accuracy: 0.8572 - val_loss: 0.3786 - val_accuracy: 0.8389\n",
            "Epoch 732/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2678 - accuracy: 0.8937 - val_loss: 0.3535 - val_accuracy: 0.8722\n",
            "Epoch 733/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2033 - accuracy: 0.9243 - val_loss: 0.4088 - val_accuracy: 0.8500\n",
            "Epoch 734/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2570 - accuracy: 0.8984 - val_loss: 0.3811 - val_accuracy: 0.8556\n",
            "Epoch 735/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2152 - accuracy: 0.9081 - val_loss: 0.5456 - val_accuracy: 0.8222\n",
            "Epoch 736/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2402 - accuracy: 0.8929 - val_loss: 0.4341 - val_accuracy: 0.8278\n",
            "Epoch 737/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.1743 - accuracy: 0.9243 - val_loss: 0.6112 - val_accuracy: 0.8333\n",
            "Epoch 738/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.3071 - accuracy: 0.8748 - val_loss: 0.4214 - val_accuracy: 0.8167\n",
            "Epoch 739/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2855 - accuracy: 0.8825 - val_loss: 0.3390 - val_accuracy: 0.8889\n",
            "Epoch 740/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.1917 - accuracy: 0.9225 - val_loss: 0.3798 - val_accuracy: 0.8500\n",
            "Epoch 741/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2371 - accuracy: 0.9085 - val_loss: 0.3926 - val_accuracy: 0.8444\n",
            "Epoch 742/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3182 - accuracy: 0.8765 - val_loss: 0.3676 - val_accuracy: 0.8444\n",
            "Epoch 743/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2474 - accuracy: 0.8880 - val_loss: 0.3711 - val_accuracy: 0.8667\n",
            "Epoch 744/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2434 - accuracy: 0.8936 - val_loss: 0.3622 - val_accuracy: 0.8556\n",
            "Epoch 745/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.1857 - accuracy: 0.9267 - val_loss: 0.3932 - val_accuracy: 0.8444\n",
            "Epoch 746/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2149 - accuracy: 0.9045 - val_loss: 0.4905 - val_accuracy: 0.8222\n",
            "Epoch 747/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2454 - accuracy: 0.8983 - val_loss: 0.3871 - val_accuracy: 0.8611\n",
            "Epoch 748/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2069 - accuracy: 0.9232 - val_loss: 0.5804 - val_accuracy: 0.8222\n",
            "Epoch 749/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3051 - accuracy: 0.8820 - val_loss: 0.5952 - val_accuracy: 0.7944\n",
            "Epoch 750/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.3043 - accuracy: 0.8441 - val_loss: 0.3948 - val_accuracy: 0.8556\n",
            "Epoch 751/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2300 - accuracy: 0.9145 - val_loss: 0.4007 - val_accuracy: 0.8500\n",
            "Epoch 752/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2587 - accuracy: 0.9021 - val_loss: 0.5227 - val_accuracy: 0.8111\n",
            "Epoch 753/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.1974 - accuracy: 0.9304 - val_loss: 0.3906 - val_accuracy: 0.8611\n",
            "Epoch 754/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2685 - accuracy: 0.8932 - val_loss: 0.3989 - val_accuracy: 0.8556\n",
            "Epoch 755/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2001 - accuracy: 0.9147 - val_loss: 0.5817 - val_accuracy: 0.8056\n",
            "Epoch 756/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2194 - accuracy: 0.9105 - val_loss: 0.3897 - val_accuracy: 0.8500\n",
            "Epoch 757/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.1986 - accuracy: 0.9388 - val_loss: 0.4252 - val_accuracy: 0.8278\n",
            "Epoch 758/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2146 - accuracy: 0.9035 - val_loss: 0.4125 - val_accuracy: 0.8500\n",
            "Epoch 759/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.1931 - accuracy: 0.9399 - val_loss: 0.4599 - val_accuracy: 0.8500\n",
            "Epoch 760/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2378 - accuracy: 0.9146 - val_loss: 0.3996 - val_accuracy: 0.8500\n",
            "Epoch 761/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2400 - accuracy: 0.8886 - val_loss: 0.4493 - val_accuracy: 0.8000\n",
            "Epoch 762/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2262 - accuracy: 0.8917 - val_loss: 0.3634 - val_accuracy: 0.8556\n",
            "Epoch 763/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2294 - accuracy: 0.9000 - val_loss: 0.4770 - val_accuracy: 0.8222\n",
            "Epoch 764/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2837 - accuracy: 0.9001 - val_loss: 0.4021 - val_accuracy: 0.8389\n",
            "Epoch 765/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2270 - accuracy: 0.9242 - val_loss: 0.4439 - val_accuracy: 0.8444\n",
            "Epoch 766/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2700 - accuracy: 0.8883 - val_loss: 0.3776 - val_accuracy: 0.8667\n",
            "Epoch 767/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.3116 - accuracy: 0.8606 - val_loss: 0.4011 - val_accuracy: 0.8278\n",
            "Epoch 768/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2483 - accuracy: 0.9001 - val_loss: 0.3910 - val_accuracy: 0.8389\n",
            "Epoch 769/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2397 - accuracy: 0.8979 - val_loss: 0.3823 - val_accuracy: 0.8611\n",
            "Epoch 770/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2258 - accuracy: 0.8985 - val_loss: 0.4324 - val_accuracy: 0.8111\n",
            "Epoch 771/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.1913 - accuracy: 0.9193 - val_loss: 0.4256 - val_accuracy: 0.8556\n",
            "Epoch 772/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2233 - accuracy: 0.9094 - val_loss: 0.3571 - val_accuracy: 0.8444\n",
            "Epoch 773/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2210 - accuracy: 0.9064 - val_loss: 0.4222 - val_accuracy: 0.8444\n",
            "Epoch 774/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.1786 - accuracy: 0.9328 - val_loss: 0.3932 - val_accuracy: 0.8611\n",
            "Epoch 775/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2153 - accuracy: 0.9083 - val_loss: 0.4358 - val_accuracy: 0.8222\n",
            "Epoch 776/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2547 - accuracy: 0.8824 - val_loss: 0.3418 - val_accuracy: 0.8722\n",
            "Epoch 777/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2190 - accuracy: 0.9104 - val_loss: 0.4392 - val_accuracy: 0.8500\n",
            "Epoch 778/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2027 - accuracy: 0.8967 - val_loss: 0.3899 - val_accuracy: 0.8333\n",
            "Epoch 779/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2759 - accuracy: 0.8912 - val_loss: 0.4553 - val_accuracy: 0.8111\n",
            "Epoch 780/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2338 - accuracy: 0.9050 - val_loss: 0.4639 - val_accuracy: 0.8333\n",
            "Epoch 781/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2126 - accuracy: 0.9144 - val_loss: 0.4050 - val_accuracy: 0.8500\n",
            "Epoch 782/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.1682 - accuracy: 0.9341 - val_loss: 0.3887 - val_accuracy: 0.8556\n",
            "Epoch 783/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2041 - accuracy: 0.9121 - val_loss: 0.4273 - val_accuracy: 0.8778\n",
            "Epoch 784/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2233 - accuracy: 0.9170 - val_loss: 0.4034 - val_accuracy: 0.8333\n",
            "Epoch 785/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2903 - accuracy: 0.8986 - val_loss: 0.3903 - val_accuracy: 0.8500\n",
            "Epoch 786/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.1933 - accuracy: 0.9233 - val_loss: 0.3515 - val_accuracy: 0.8778\n",
            "Epoch 787/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2098 - accuracy: 0.9121 - val_loss: 0.3726 - val_accuracy: 0.8333\n",
            "Epoch 788/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2769 - accuracy: 0.9116 - val_loss: 0.4067 - val_accuracy: 0.8167\n",
            "Epoch 789/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2267 - accuracy: 0.8961 - val_loss: 0.4129 - val_accuracy: 0.8278\n",
            "Epoch 790/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2680 - accuracy: 0.8928 - val_loss: 0.4114 - val_accuracy: 0.8667\n",
            "Epoch 791/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2227 - accuracy: 0.9121 - val_loss: 0.4707 - val_accuracy: 0.8389\n",
            "Epoch 792/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.1869 - accuracy: 0.9372 - val_loss: 0.4283 - val_accuracy: 0.8333\n",
            "Epoch 793/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2449 - accuracy: 0.8928 - val_loss: 0.3456 - val_accuracy: 0.8833\n",
            "Epoch 794/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2109 - accuracy: 0.9299 - val_loss: 0.3921 - val_accuracy: 0.8444\n",
            "Epoch 795/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2000 - accuracy: 0.9278 - val_loss: 0.4028 - val_accuracy: 0.8444\n",
            "Epoch 796/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2419 - accuracy: 0.9077 - val_loss: 0.4588 - val_accuracy: 0.8444\n",
            "Epoch 797/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2618 - accuracy: 0.8886 - val_loss: 0.4355 - val_accuracy: 0.8333\n",
            "Epoch 798/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2073 - accuracy: 0.9065 - val_loss: 0.3458 - val_accuracy: 0.8667\n",
            "Epoch 799/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2531 - accuracy: 0.8984 - val_loss: 0.4158 - val_accuracy: 0.8333\n",
            "Epoch 800/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2281 - accuracy: 0.9211 - val_loss: 0.4132 - val_accuracy: 0.8333\n",
            "Epoch 801/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2372 - accuracy: 0.8959 - val_loss: 0.3685 - val_accuracy: 0.8722\n",
            "Epoch 802/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.1944 - accuracy: 0.9272 - val_loss: 0.3910 - val_accuracy: 0.8667\n",
            "Epoch 803/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2028 - accuracy: 0.9336 - val_loss: 0.3765 - val_accuracy: 0.8556\n",
            "Epoch 804/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2036 - accuracy: 0.9068 - val_loss: 0.4243 - val_accuracy: 0.8389\n",
            "Epoch 805/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2219 - accuracy: 0.9041 - val_loss: 0.6456 - val_accuracy: 0.7778\n",
            "Epoch 806/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2854 - accuracy: 0.8976 - val_loss: 0.4019 - val_accuracy: 0.8611\n",
            "Epoch 807/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2357 - accuracy: 0.8964 - val_loss: 0.4722 - val_accuracy: 0.8222\n",
            "Epoch 808/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2344 - accuracy: 0.9070 - val_loss: 0.3769 - val_accuracy: 0.8444\n",
            "Epoch 809/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2795 - accuracy: 0.8804 - val_loss: 0.4175 - val_accuracy: 0.8722\n",
            "Epoch 810/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2175 - accuracy: 0.9067 - val_loss: 0.3950 - val_accuracy: 0.8556\n",
            "Epoch 811/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.1775 - accuracy: 0.9360 - val_loss: 0.4204 - val_accuracy: 0.8500\n",
            "Epoch 812/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2447 - accuracy: 0.8721 - val_loss: 0.4569 - val_accuracy: 0.8444\n",
            "Epoch 813/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2258 - accuracy: 0.9102 - val_loss: 0.4125 - val_accuracy: 0.8556\n",
            "Epoch 814/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2263 - accuracy: 0.8993 - val_loss: 0.4167 - val_accuracy: 0.8389\n",
            "Epoch 815/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2439 - accuracy: 0.9111 - val_loss: 0.4117 - val_accuracy: 0.8667\n",
            "Epoch 816/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2161 - accuracy: 0.9217 - val_loss: 0.3839 - val_accuracy: 0.8611\n",
            "Epoch 817/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2535 - accuracy: 0.9003 - val_loss: 0.4872 - val_accuracy: 0.8000\n",
            "Epoch 818/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3146 - accuracy: 0.8816 - val_loss: 0.4232 - val_accuracy: 0.8556\n",
            "Epoch 819/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2126 - accuracy: 0.9058 - val_loss: 0.3846 - val_accuracy: 0.8500\n",
            "Epoch 820/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2354 - accuracy: 0.9108 - val_loss: 0.4235 - val_accuracy: 0.8333\n",
            "Epoch 821/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.1902 - accuracy: 0.9148 - val_loss: 0.3551 - val_accuracy: 0.8556\n",
            "Epoch 822/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2033 - accuracy: 0.9160 - val_loss: 0.4079 - val_accuracy: 0.8611\n",
            "Epoch 823/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2156 - accuracy: 0.9234 - val_loss: 0.4442 - val_accuracy: 0.8389\n",
            "Epoch 824/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2686 - accuracy: 0.8962 - val_loss: 0.5815 - val_accuracy: 0.8111\n",
            "Epoch 825/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.3234 - accuracy: 0.8730 - val_loss: 0.4296 - val_accuracy: 0.8333\n",
            "Epoch 826/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.1788 - accuracy: 0.9455 - val_loss: 0.4414 - val_accuracy: 0.8333\n",
            "Epoch 827/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2209 - accuracy: 0.9138 - val_loss: 0.3953 - val_accuracy: 0.8611\n",
            "Epoch 828/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.1823 - accuracy: 0.9382 - val_loss: 0.3682 - val_accuracy: 0.8833\n",
            "Epoch 829/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2179 - accuracy: 0.9112 - val_loss: 0.4092 - val_accuracy: 0.8444\n",
            "Epoch 830/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2372 - accuracy: 0.9097 - val_loss: 0.4882 - val_accuracy: 0.8333\n",
            "Epoch 831/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.1875 - accuracy: 0.9270 - val_loss: 0.4370 - val_accuracy: 0.8667\n",
            "Epoch 832/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.1984 - accuracy: 0.9274 - val_loss: 0.3947 - val_accuracy: 0.8611\n",
            "Epoch 833/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2495 - accuracy: 0.9096 - val_loss: 0.3919 - val_accuracy: 0.8556\n",
            "Epoch 834/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.1816 - accuracy: 0.9343 - val_loss: 0.4410 - val_accuracy: 0.8389\n",
            "Epoch 835/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2007 - accuracy: 0.9196 - val_loss: 0.4027 - val_accuracy: 0.8611\n",
            "Epoch 836/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.1998 - accuracy: 0.9021 - val_loss: 0.3817 - val_accuracy: 0.8333\n",
            "Epoch 837/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2224 - accuracy: 0.9141 - val_loss: 0.4297 - val_accuracy: 0.8444\n",
            "Epoch 838/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.1939 - accuracy: 0.9112 - val_loss: 0.5024 - val_accuracy: 0.8278\n",
            "Epoch 839/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2251 - accuracy: 0.9179 - val_loss: 0.5131 - val_accuracy: 0.8389\n",
            "Epoch 840/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2526 - accuracy: 0.8729 - val_loss: 0.3877 - val_accuracy: 0.8444\n",
            "Epoch 841/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2674 - accuracy: 0.8935 - val_loss: 0.4038 - val_accuracy: 0.8333\n",
            "Epoch 842/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2285 - accuracy: 0.9184 - val_loss: 0.3840 - val_accuracy: 0.8556\n",
            "Epoch 843/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2924 - accuracy: 0.8620 - val_loss: 0.5311 - val_accuracy: 0.7833\n",
            "Epoch 844/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.3166 - accuracy: 0.8843 - val_loss: 0.3689 - val_accuracy: 0.8778\n",
            "Epoch 845/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.1911 - accuracy: 0.9336 - val_loss: 0.4116 - val_accuracy: 0.8444\n",
            "Epoch 846/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2990 - accuracy: 0.8706 - val_loss: 0.4356 - val_accuracy: 0.8556\n",
            "Epoch 847/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2306 - accuracy: 0.8995 - val_loss: 0.3927 - val_accuracy: 0.8611\n",
            "Epoch 848/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2040 - accuracy: 0.9294 - val_loss: 0.3805 - val_accuracy: 0.8667\n",
            "Epoch 849/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2557 - accuracy: 0.9029 - val_loss: 0.3400 - val_accuracy: 0.8722\n",
            "Epoch 850/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2197 - accuracy: 0.9200 - val_loss: 0.3901 - val_accuracy: 0.8444\n",
            "Epoch 851/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2288 - accuracy: 0.9072 - val_loss: 0.3745 - val_accuracy: 0.8556\n",
            "Epoch 852/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2170 - accuracy: 0.9137 - val_loss: 0.4386 - val_accuracy: 0.8611\n",
            "Epoch 853/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2131 - accuracy: 0.9239 - val_loss: 0.4777 - val_accuracy: 0.7667\n",
            "Epoch 854/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2247 - accuracy: 0.8983 - val_loss: 0.3738 - val_accuracy: 0.8500\n",
            "Epoch 855/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.1925 - accuracy: 0.9100 - val_loss: 0.4259 - val_accuracy: 0.8556\n",
            "Epoch 856/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2212 - accuracy: 0.8944 - val_loss: 0.5021 - val_accuracy: 0.7722\n",
            "Epoch 857/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2602 - accuracy: 0.8921 - val_loss: 0.4460 - val_accuracy: 0.8500\n",
            "Epoch 858/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2152 - accuracy: 0.9076 - val_loss: 0.3759 - val_accuracy: 0.8722\n",
            "Epoch 859/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2441 - accuracy: 0.8918 - val_loss: 0.3455 - val_accuracy: 0.8722\n",
            "Epoch 860/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2109 - accuracy: 0.9236 - val_loss: 0.4605 - val_accuracy: 0.8444\n",
            "Epoch 861/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2264 - accuracy: 0.9178 - val_loss: 0.3440 - val_accuracy: 0.8833\n",
            "Epoch 862/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2244 - accuracy: 0.9105 - val_loss: 0.3515 - val_accuracy: 0.8833\n",
            "Epoch 863/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.1765 - accuracy: 0.9325 - val_loss: 0.3875 - val_accuracy: 0.8722\n",
            "Epoch 864/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2046 - accuracy: 0.8976 - val_loss: 0.4048 - val_accuracy: 0.8389\n",
            "Epoch 865/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.1829 - accuracy: 0.9377 - val_loss: 0.3696 - val_accuracy: 0.8833\n",
            "Epoch 866/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2458 - accuracy: 0.9207 - val_loss: 0.3326 - val_accuracy: 0.8833\n",
            "Epoch 867/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2239 - accuracy: 0.9145 - val_loss: 0.4756 - val_accuracy: 0.8444\n",
            "Epoch 868/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2396 - accuracy: 0.9123 - val_loss: 0.4318 - val_accuracy: 0.8611\n",
            "Epoch 869/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2197 - accuracy: 0.8992 - val_loss: 0.4248 - val_accuracy: 0.8333\n",
            "Epoch 870/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.1605 - accuracy: 0.9558 - val_loss: 0.4138 - val_accuracy: 0.8778\n",
            "Epoch 871/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2087 - accuracy: 0.9204 - val_loss: 0.3894 - val_accuracy: 0.8222\n",
            "Epoch 872/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2000 - accuracy: 0.9302 - val_loss: 0.3678 - val_accuracy: 0.9000\n",
            "Epoch 873/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2738 - accuracy: 0.8606 - val_loss: 0.3562 - val_accuracy: 0.8667\n",
            "Epoch 874/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2775 - accuracy: 0.8885 - val_loss: 0.6104 - val_accuracy: 0.8056\n",
            "Epoch 875/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2407 - accuracy: 0.9251 - val_loss: 0.3509 - val_accuracy: 0.8889\n",
            "Epoch 876/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.1935 - accuracy: 0.9201 - val_loss: 0.5015 - val_accuracy: 0.8167\n",
            "Epoch 877/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2118 - accuracy: 0.9082 - val_loss: 0.3881 - val_accuracy: 0.8556\n",
            "Epoch 878/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.1742 - accuracy: 0.9272 - val_loss: 0.4546 - val_accuracy: 0.8556\n",
            "Epoch 879/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.1977 - accuracy: 0.9199 - val_loss: 0.3926 - val_accuracy: 0.8611\n",
            "Epoch 880/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.1840 - accuracy: 0.9212 - val_loss: 0.4059 - val_accuracy: 0.8500\n",
            "Epoch 881/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2127 - accuracy: 0.9089 - val_loss: 0.4550 - val_accuracy: 0.8333\n",
            "Epoch 882/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.1964 - accuracy: 0.9075 - val_loss: 0.4227 - val_accuracy: 0.8667\n",
            "Epoch 883/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2402 - accuracy: 0.9077 - val_loss: 0.4093 - val_accuracy: 0.8667\n",
            "Epoch 884/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.1647 - accuracy: 0.9393 - val_loss: 0.4491 - val_accuracy: 0.8500\n",
            "Epoch 885/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2110 - accuracy: 0.9331 - val_loss: 0.6514 - val_accuracy: 0.7778\n",
            "Epoch 886/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2848 - accuracy: 0.8927 - val_loss: 0.5913 - val_accuracy: 0.8222\n",
            "Epoch 887/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2088 - accuracy: 0.9128 - val_loss: 0.4314 - val_accuracy: 0.8500\n",
            "Epoch 888/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.1505 - accuracy: 0.9461 - val_loss: 0.4334 - val_accuracy: 0.8333\n",
            "Epoch 889/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2059 - accuracy: 0.9289 - val_loss: 0.4308 - val_accuracy: 0.8556\n",
            "Epoch 890/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.1950 - accuracy: 0.9088 - val_loss: 0.4702 - val_accuracy: 0.8167\n",
            "Epoch 891/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2545 - accuracy: 0.8579 - val_loss: 0.3835 - val_accuracy: 0.8667\n",
            "Epoch 892/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2430 - accuracy: 0.9086 - val_loss: 0.4224 - val_accuracy: 0.8611\n",
            "Epoch 893/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2224 - accuracy: 0.9211 - val_loss: 0.4128 - val_accuracy: 0.8500\n",
            "Epoch 894/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.1931 - accuracy: 0.9230 - val_loss: 0.3792 - val_accuracy: 0.8389\n",
            "Epoch 895/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2916 - accuracy: 0.8645 - val_loss: 0.4115 - val_accuracy: 0.8722\n",
            "Epoch 896/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2323 - accuracy: 0.9023 - val_loss: 0.4214 - val_accuracy: 0.8444\n",
            "Epoch 897/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.1950 - accuracy: 0.9254 - val_loss: 0.4165 - val_accuracy: 0.8500\n",
            "Epoch 898/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.1919 - accuracy: 0.9247 - val_loss: 0.4160 - val_accuracy: 0.8611\n",
            "Epoch 899/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2078 - accuracy: 0.9160 - val_loss: 0.5162 - val_accuracy: 0.8333\n",
            "Epoch 900/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2429 - accuracy: 0.8846 - val_loss: 0.3952 - val_accuracy: 0.8611\n",
            "Epoch 901/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2006 - accuracy: 0.9215 - val_loss: 0.4754 - val_accuracy: 0.8500\n",
            "Epoch 902/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.1979 - accuracy: 0.9165 - val_loss: 0.3841 - val_accuracy: 0.8722\n",
            "Epoch 903/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.1694 - accuracy: 0.9348 - val_loss: 0.5042 - val_accuracy: 0.8222\n",
            "Epoch 904/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2421 - accuracy: 0.9090 - val_loss: 0.4414 - val_accuracy: 0.8667\n",
            "Epoch 905/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.1758 - accuracy: 0.9289 - val_loss: 0.4188 - val_accuracy: 0.8500\n",
            "Epoch 906/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2071 - accuracy: 0.9207 - val_loss: 0.6320 - val_accuracy: 0.8000\n",
            "Epoch 907/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2674 - accuracy: 0.9018 - val_loss: 0.3966 - val_accuracy: 0.8556\n",
            "Epoch 908/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2180 - accuracy: 0.9110 - val_loss: 0.4315 - val_accuracy: 0.8444\n",
            "Epoch 909/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2341 - accuracy: 0.8864 - val_loss: 0.4349 - val_accuracy: 0.8333\n",
            "Epoch 910/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.1884 - accuracy: 0.9312 - val_loss: 0.4003 - val_accuracy: 0.8722\n",
            "Epoch 911/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2367 - accuracy: 0.9131 - val_loss: 0.4179 - val_accuracy: 0.8667\n",
            "Epoch 912/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.1960 - accuracy: 0.9266 - val_loss: 0.5516 - val_accuracy: 0.8278\n",
            "Epoch 913/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2279 - accuracy: 0.8914 - val_loss: 0.3627 - val_accuracy: 0.8667\n",
            "Epoch 914/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2132 - accuracy: 0.9059 - val_loss: 0.4015 - val_accuracy: 0.8500\n",
            "Epoch 915/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.1962 - accuracy: 0.9113 - val_loss: 0.4587 - val_accuracy: 0.8389\n",
            "Epoch 916/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2085 - accuracy: 0.9063 - val_loss: 0.4747 - val_accuracy: 0.8389\n",
            "Epoch 917/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2512 - accuracy: 0.9089 - val_loss: 0.4220 - val_accuracy: 0.8500\n",
            "Epoch 918/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2024 - accuracy: 0.9279 - val_loss: 0.4485 - val_accuracy: 0.8278\n",
            "Epoch 919/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2119 - accuracy: 0.9150 - val_loss: 0.4142 - val_accuracy: 0.8500\n",
            "Epoch 920/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2090 - accuracy: 0.9102 - val_loss: 0.4914 - val_accuracy: 0.8389\n",
            "Epoch 921/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2092 - accuracy: 0.9116 - val_loss: 0.4233 - val_accuracy: 0.8556\n",
            "Epoch 922/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2091 - accuracy: 0.9022 - val_loss: 0.4157 - val_accuracy: 0.8667\n",
            "Epoch 923/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.1851 - accuracy: 0.9302 - val_loss: 0.4519 - val_accuracy: 0.8667\n",
            "Epoch 924/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.1884 - accuracy: 0.9220 - val_loss: 0.4361 - val_accuracy: 0.8611\n",
            "Epoch 925/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.1921 - accuracy: 0.9334 - val_loss: 0.4288 - val_accuracy: 0.8389\n",
            "Epoch 926/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2262 - accuracy: 0.9073 - val_loss: 0.4418 - val_accuracy: 0.8444\n",
            "Epoch 927/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.1920 - accuracy: 0.9177 - val_loss: 0.4502 - val_accuracy: 0.8611\n",
            "Epoch 928/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.1899 - accuracy: 0.9297 - val_loss: 0.3903 - val_accuracy: 0.8444\n",
            "Epoch 929/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2034 - accuracy: 0.9202 - val_loss: 0.3766 - val_accuracy: 0.8611\n",
            "Epoch 930/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.1940 - accuracy: 0.9239 - val_loss: 0.4032 - val_accuracy: 0.8500\n",
            "Epoch 931/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2206 - accuracy: 0.9070 - val_loss: 0.4050 - val_accuracy: 0.8667\n",
            "Epoch 932/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2406 - accuracy: 0.9085 - val_loss: 0.4259 - val_accuracy: 0.8667\n",
            "Epoch 933/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2210 - accuracy: 0.9026 - val_loss: 0.6642 - val_accuracy: 0.8111\n",
            "Epoch 934/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.3816 - accuracy: 0.8619 - val_loss: 0.4926 - val_accuracy: 0.8556\n",
            "Epoch 935/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2496 - accuracy: 0.9028 - val_loss: 0.4889 - val_accuracy: 0.8444\n",
            "Epoch 936/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.1924 - accuracy: 0.9055 - val_loss: 0.4142 - val_accuracy: 0.8667\n",
            "Epoch 937/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2029 - accuracy: 0.9320 - val_loss: 0.3724 - val_accuracy: 0.8778\n",
            "Epoch 938/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.1819 - accuracy: 0.9273 - val_loss: 0.4057 - val_accuracy: 0.8611\n",
            "Epoch 939/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2121 - accuracy: 0.9150 - val_loss: 0.4708 - val_accuracy: 0.8611\n",
            "Epoch 940/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2019 - accuracy: 0.9130 - val_loss: 0.4331 - val_accuracy: 0.8722\n",
            "Epoch 941/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2592 - accuracy: 0.8947 - val_loss: 0.3782 - val_accuracy: 0.8667\n",
            "Epoch 942/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2051 - accuracy: 0.9212 - val_loss: 0.4322 - val_accuracy: 0.8556\n",
            "Epoch 943/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.1960 - accuracy: 0.9190 - val_loss: 0.3885 - val_accuracy: 0.8722\n",
            "Epoch 944/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.1721 - accuracy: 0.9340 - val_loss: 0.4135 - val_accuracy: 0.8889\n",
            "Epoch 945/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2130 - accuracy: 0.9147 - val_loss: 0.3890 - val_accuracy: 0.8611\n",
            "Epoch 946/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2813 - accuracy: 0.8972 - val_loss: 0.4687 - val_accuracy: 0.8389\n",
            "Epoch 947/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2056 - accuracy: 0.9188 - val_loss: 0.4128 - val_accuracy: 0.8833\n",
            "Epoch 948/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.1484 - accuracy: 0.9486 - val_loss: 0.5030 - val_accuracy: 0.8278\n",
            "Epoch 949/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2846 - accuracy: 0.8791 - val_loss: 0.4231 - val_accuracy: 0.8500\n",
            "Epoch 950/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2156 - accuracy: 0.9245 - val_loss: 0.4225 - val_accuracy: 0.8556\n",
            "Epoch 951/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.1770 - accuracy: 0.9275 - val_loss: 0.3984 - val_accuracy: 0.8667\n",
            "Epoch 952/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.1730 - accuracy: 0.9111 - val_loss: 0.3912 - val_accuracy: 0.8667\n",
            "Epoch 953/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.1953 - accuracy: 0.9249 - val_loss: 0.4904 - val_accuracy: 0.8333\n",
            "Epoch 954/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.1789 - accuracy: 0.9330 - val_loss: 0.4819 - val_accuracy: 0.8667\n",
            "Epoch 955/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2435 - accuracy: 0.8970 - val_loss: 0.4100 - val_accuracy: 0.8667\n",
            "Epoch 956/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.1957 - accuracy: 0.9140 - val_loss: 0.5037 - val_accuracy: 0.8167\n",
            "Epoch 957/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2111 - accuracy: 0.9085 - val_loss: 0.4645 - val_accuracy: 0.8667\n",
            "Epoch 958/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.1997 - accuracy: 0.9247 - val_loss: 0.3469 - val_accuracy: 0.8500\n",
            "Epoch 959/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.1836 - accuracy: 0.9120 - val_loss: 0.4757 - val_accuracy: 0.8444\n",
            "Epoch 960/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2391 - accuracy: 0.9110 - val_loss: 0.5345 - val_accuracy: 0.8167\n",
            "Epoch 961/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2487 - accuracy: 0.8994 - val_loss: 0.4861 - val_accuracy: 0.8667\n",
            "Epoch 962/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2104 - accuracy: 0.9028 - val_loss: 0.4176 - val_accuracy: 0.8722\n",
            "Epoch 963/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.1936 - accuracy: 0.9198 - val_loss: 0.6229 - val_accuracy: 0.7889\n",
            "Epoch 964/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.3070 - accuracy: 0.8853 - val_loss: 0.4023 - val_accuracy: 0.8778\n",
            "Epoch 965/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.1864 - accuracy: 0.9109 - val_loss: 0.3912 - val_accuracy: 0.8833\n",
            "Epoch 966/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2314 - accuracy: 0.9031 - val_loss: 0.3540 - val_accuracy: 0.8500\n",
            "Epoch 967/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.1907 - accuracy: 0.9155 - val_loss: 0.3870 - val_accuracy: 0.8778\n",
            "Epoch 968/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2271 - accuracy: 0.9053 - val_loss: 0.4144 - val_accuracy: 0.8556\n",
            "Epoch 969/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2293 - accuracy: 0.9116 - val_loss: 0.6070 - val_accuracy: 0.8000\n",
            "Epoch 970/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2174 - accuracy: 0.9080 - val_loss: 0.4070 - val_accuracy: 0.8556\n",
            "Epoch 971/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.1675 - accuracy: 0.9221 - val_loss: 0.3942 - val_accuracy: 0.8944\n",
            "Epoch 972/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2441 - accuracy: 0.8979 - val_loss: 0.3304 - val_accuracy: 0.8556\n",
            "Epoch 973/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.1986 - accuracy: 0.9307 - val_loss: 0.4422 - val_accuracy: 0.8500\n",
            "Epoch 974/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2195 - accuracy: 0.9157 - val_loss: 0.4467 - val_accuracy: 0.8556\n",
            "Epoch 975/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.1750 - accuracy: 0.9217 - val_loss: 0.4654 - val_accuracy: 0.8722\n",
            "Epoch 976/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2288 - accuracy: 0.9087 - val_loss: 0.4498 - val_accuracy: 0.8056\n",
            "Epoch 977/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2122 - accuracy: 0.9112 - val_loss: 0.4394 - val_accuracy: 0.8667\n",
            "Epoch 978/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.1897 - accuracy: 0.9203 - val_loss: 0.4124 - val_accuracy: 0.8611\n",
            "Epoch 979/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2164 - accuracy: 0.9067 - val_loss: 0.4103 - val_accuracy: 0.8889\n",
            "Epoch 980/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2368 - accuracy: 0.9034 - val_loss: 0.3842 - val_accuracy: 0.8722\n",
            "Epoch 981/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.1921 - accuracy: 0.9283 - val_loss: 0.4401 - val_accuracy: 0.8500\n",
            "Epoch 982/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2373 - accuracy: 0.9131 - val_loss: 0.3961 - val_accuracy: 0.8556\n",
            "Epoch 983/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.1804 - accuracy: 0.9272 - val_loss: 0.6493 - val_accuracy: 0.7944\n",
            "Epoch 984/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.1858 - accuracy: 0.9314 - val_loss: 0.4802 - val_accuracy: 0.8667\n",
            "Epoch 985/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.1810 - accuracy: 0.9430 - val_loss: 0.4561 - val_accuracy: 0.8611\n",
            "Epoch 986/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2093 - accuracy: 0.9206 - val_loss: 0.3855 - val_accuracy: 0.8722\n",
            "Epoch 987/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.1766 - accuracy: 0.9394 - val_loss: 0.4642 - val_accuracy: 0.8222\n",
            "Epoch 988/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.1972 - accuracy: 0.9120 - val_loss: 0.4412 - val_accuracy: 0.8556\n",
            "Epoch 989/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2436 - accuracy: 0.8759 - val_loss: 0.3512 - val_accuracy: 0.8722\n",
            "Epoch 990/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2345 - accuracy: 0.8921 - val_loss: 0.4414 - val_accuracy: 0.8556\n",
            "Epoch 991/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2035 - accuracy: 0.9288 - val_loss: 0.3737 - val_accuracy: 0.8667\n",
            "Epoch 992/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.1960 - accuracy: 0.9206 - val_loss: 0.4932 - val_accuracy: 0.8389\n",
            "Epoch 993/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2284 - accuracy: 0.9167 - val_loss: 0.3596 - val_accuracy: 0.8556\n",
            "Epoch 994/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.1894 - accuracy: 0.9174 - val_loss: 0.3797 - val_accuracy: 0.8611\n",
            "Epoch 995/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.1883 - accuracy: 0.9289 - val_loss: 0.4245 - val_accuracy: 0.8444\n",
            "Epoch 996/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2401 - accuracy: 0.9129 - val_loss: 0.5069 - val_accuracy: 0.8611\n",
            "Epoch 997/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2269 - accuracy: 0.9076 - val_loss: 0.3994 - val_accuracy: 0.8444\n",
            "Epoch 998/1000\n",
            "68/68 [==============================] - 0s 2ms/step - loss: 0.2499 - accuracy: 0.8902 - val_loss: 0.4427 - val_accuracy: 0.8556\n",
            "Epoch 999/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2654 - accuracy: 0.8762 - val_loss: 0.4470 - val_accuracy: 0.8667\n",
            "Epoch 1000/1000\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.2016 - accuracy: 0.9279 - val_loss: 0.3272 - val_accuracy: 0.8667\n",
            "INFO:tensorflow:Assets written to: models/model/assets\n"
          ]
        }
      ],
      "source": [
        "# Train the model on our training data while validating on our validation set\n",
        "history = model.fit(x_train, y_train, epochs=1000, batch_size=8,\n",
        "                        validation_data=(x_validate, y_validate))\n",
        "\n",
        "# Save the model to disk\n",
        "model.save(MODEL_TF)      # SavedModel format (to create the TensorFlow Lite models)\n",
        "model.save(MODEL_TF_H5)   # HDF5 format (to compare with the TensorFlow Lite models size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mc_CQu2_IvOP"
      },
      "source": [
        "### 3. Plot Performance Metrics (Categorical Crossentropy and Categorical Accuracy)\n",
        "\n",
        "The code in the following cell draws a graph of the loss (Categorical Crossentropy) and the performance metric (Categorical Accuracy) during training and validation (i.e. the distance between the predicted and actual values)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "id": "SYHGswAJJgrC",
        "outputId": "07ed86bf-a090-45fb-8488-5eced5ec58e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mFor training set:\u001b[0m\tLoss = 0.19842 \tAccuracy = 0.9185 \n",
            "\n",
            "\u001b[1mFor validation set:\u001b[0m\tLoss = 0.32720 \tAccuracy = 0.8667 \n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Calculate and print the loss on the training dataset\n",
        "train_loss, train_accuracy = model.evaluate(x_train, y_train, verbose=0)\n",
        "print('\\033[1m' + 'For training set:' + '\\033[0m' + '\\tLoss =',format(train_loss, \".5f\"),'\\tAccuracy =',format(train_accuracy, \".4f\"),'\\n')\n",
        "\n",
        "# Calculate and print the loss on the validation dataset\n",
        "val_loss, val_accuracy = model.evaluate(x_validate, y_validate, verbose=0)\n",
        "print('\\033[1m' + 'For validation set:' + '\\033[0m' + '\\tLoss =',format(val_loss, \".5f\"),'\\tAccuracy =',format(val_accuracy, \".4f\"),'\\n')\n",
        "\n",
        "#--- Plotting the graphs ---#\n",
        "# Exclude the first few epochs so the graph is easier to read\n",
        "SKIP = 200\n",
        "\n",
        "# Plotting the loss\n",
        "train_loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(train_loss) + 1)\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "\n",
        "plt.plot(epochs[SKIP:], train_loss[SKIP:], 'g.', label='Training loss')\n",
        "plt.plot(epochs[SKIP:], val_loss[SKIP:], 'b.', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Plotting the accuracy\n",
        "train_accuracy = history.history['accuracy']\n",
        "val_accuracy = history.history['val_accuracy']\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "\n",
        "plt.plot(epochs[SKIP:], train_accuracy[SKIP:], 'g.', label='Training Accuracy')\n",
        "plt.plot(epochs[SKIP:], val_accuracy[SKIP:], 'b.', label='Validation Accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4AuqJgSEoWH"
      },
      "source": [
        "### 4. Plot Predicted vs Actual values\n",
        "\n",
        "In the following cells, the metrics of the test set are measured and the predicted and actual values of the whole dataset are plotted for comparison."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCNO6aBmdIZK"
      },
      "outputs": [],
      "source": [
        "#Function to prepare y values for the plot. It separates the Gesture 1\n",
        "#labels from the Gesture 2 labels as they are mixed in vector y.\n",
        "def prepareY4Plot(y):\n",
        "  counter = -1\n",
        "  y_gest1 = np.zeros((int(len(y)/gest_num),gest_num))\n",
        "  y_gest2 = np.zeros((int(len(y)/gest_num),gest_num))\n",
        "  for n in range(0,int(len(y)/gest_num)):\n",
        "    counter = counter + 1\n",
        "    y_gest1[n] = y[counter]\n",
        "    counter = counter + 1\n",
        "    y_gest2[n] = y[counter]\n",
        "  return y_gest1, y_gest2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "lZfztKKyhLxX",
        "outputId": "735c7d71-29eb-40b0-b1e5-3e73a4c1eadc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mFor test set:\u001b[0m\tLoss = 0.24676 \tAccuracy = 0.9111 \n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " \u001b[1mFor whole dataset:\u001b[0m   Loss = 0.23384   Accuracy = 0.9067 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Calculate and print the loss on the test dataset\n",
        "test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('\\033[1m' + 'For test set:' + '\\033[0m' + '\\tLoss =',format(test_loss, \".5f\"),'\\tAccuracy =',format(test_accuracy, \".4f\"),'\\n')\n",
        "\n",
        "# Make predictions based on the whole dataset\n",
        "y_dataset_pred = model.predict(x_values)\n",
        "\n",
        "# Preparing variables for the plot\n",
        "y_dataset_gest1, y_dataset_gest2 = prepareY4Plot(y_values)\n",
        "y_dataset_pred_gest1, y_dataset_pred_gest2 = prepareY4Plot(y_dataset_pred)\n",
        "\n",
        "# Graph the predictions against the actual values\n",
        "x_plot_gest1 = range(0,int(len(x_values)/2))\n",
        "x_plot_gest2 = range(int(len(x_values)/2),len(x_values))\n",
        "plt.clf()\n",
        "plt.title('Comparison of predictions and actual values for the whole dataset')\n",
        "#plt.plot(x_plot_gest1, y_dataset_gest1[:,0], 'bo', label='Actual - Gesture 1')\n",
        "plt.plot(x_plot_gest1, y_dataset_pred_gest1[:,0], 'bx', label='Predicted - Gesture 1')\n",
        "#plt.plot(x_plot_gest2, y_dataset_gest2[:,1], 'ro', label='Actual - Gesture 2')\n",
        "plt.plot(x_plot_gest2, y_dataset_pred_gest2[:,1], 'rx', label='Predicted - Gesture 2')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Calculate and print the loss on the whole dataset\n",
        "dataset_loss, dataset_accuracy = model.evaluate(x_values, y_values, verbose=0)\n",
        "print('\\n','\\033[1m' + 'For whole dataset:' + '\\033[0m' + '   Loss =',format(dataset_loss, \".5f\"),'  Accuracy =',format(dataset_accuracy, \".4f\"),'\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3h7IcvuOOS4J"
      },
      "source": [
        "## Generate a TensorFlow Lite Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHe-Wv47rhm8"
      },
      "source": [
        "### 1. Generate Models with or without Quantization\n",
        "Now that we have a model, we will use the [TensorFlow Lite Converter](https://www.tensorflow.org/lite/convert) to convert it into a special, space-efficient format for use on memory-constrained devices (such as microcrocontrollers). Since the model is going to be deployed on a microcontroller, you want it to be as tiny as possible.\n",
        "\n",
        "One additional technique for reducing the size of the model is called [quantization](https://www.tensorflow.org/lite/performance/post_training_quantization). It reduces the precision of the model's _weights_, and possibly the activations (output of each layer) as well, which saves memory, often without much impact on accuracy. Quantized models also run faster, since the calculations required are simpler.\n",
        "\n",
        "In the following cell, the model will be converted to TensorFlow Lite format twice, once with quantization and once without."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1muAoUm8lSXL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73789f97-3065-4d0a-b633-4d89ab9a3a46"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3488"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "# Convert the model to the TensorFlow Lite format without quantization\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(MODEL_TF)\n",
        "model_no_quant_tflite = converter.convert()\n",
        "\n",
        "# Save the model to disk\n",
        "open(MODEL_NO_QUANT_TFLITE, \"wb\").write(model_no_quant_tflite)\n",
        "\n",
        "# Convert the model to the TensorFlow Lite format with quantization\n",
        "x_values = np.float32(x_values)   #It is required to use float data type format\n",
        "def representative_dataset():\n",
        "  for i in range(len(x_values)):\n",
        "    yield([x_values[i]])\n",
        "# Set the optimization flag.\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "# Provide a representative dataset to ensure we quantize correctly.\n",
        "converter.representative_dataset = representative_dataset\n",
        "model_tflite = converter.convert()\n",
        "\n",
        "# Save the model to disk\n",
        "open(MODEL_TFLITE, \"wb\").write(model_tflite)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0O7C5TJcUiEH"
      },
      "source": [
        "### 2. Compare Model Performance\n",
        "\n",
        "In the following cells, to prove these models are accurate even after conversion and quantization, we'll compare their predictions and performance metrics on our test dataset.\n",
        "\n",
        "***Note:***\n",
        "*The `predict` (for predictions) and `evaluate` (for loss and accuracy) functions are defined to be able to use them on the TFLite models. (These are already included in a TF model, but not in  a TFLite model.)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "NKtmxEhko1S1"
      },
      "outputs": [],
      "source": [
        "def predict_tflite(tflite_model, x_test):\n",
        "  # Prepare the test data\n",
        "  x_test = np.float32(x_test)   #It is required to use float data type format\n",
        "  x_test_ = x_test\n",
        "\n",
        "  # Initialize the TFLite interpreter\n",
        "  interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
        "  interpreter.allocate_tensors()\n",
        "\n",
        "  input_details = interpreter.get_input_details()[0]\n",
        "  output_details = interpreter.get_output_details()[0]\n",
        "\n",
        "  # If required, quantize the input layer (from float to integer)\n",
        "  input_scale, input_zero_point = input_details[\"quantization\"]\n",
        "  if (input_scale, input_zero_point) != (0.0, 0):\n",
        "    x_test_ = x_test_ / input_scale + input_zero_point\n",
        "    x_test_ = x_test_.astype(input_details[\"dtype\"])\n",
        "  \n",
        "  # Invoke the interpreter\n",
        "  y_pred = np.empty((len(x_test_),gest_num), dtype=output_details[\"dtype\"])\n",
        "  for i in range(len(x_test_)):\n",
        "    interpreter.set_tensor(input_details[\"index\"], [x_test_[i]])\n",
        "    interpreter.invoke()\n",
        "    y_pred[i] = interpreter.get_tensor(output_details[\"index\"])[0]\n",
        "  \n",
        "  # If required, dequantized the output layer (from integer to float)\n",
        "  output_scale, output_zero_point = output_details[\"quantization\"]\n",
        "  if (output_scale, output_zero_point) != (0.0, 0):\n",
        "    y_pred = y_pred.astype(np.float32)\n",
        "    y_pred = (y_pred - output_zero_point) * output_scale\n",
        "\n",
        "  return y_pred\n",
        "\n",
        "def evaluate_tflite(tflite_model, x_test, y_true):\n",
        "  global model\n",
        "  y_pred = predict_tflite(tflite_model, x_test)\n",
        "  #Loss\n",
        "  loss_function = tf.keras.losses.get(model.loss)\n",
        "  loss = loss_function(y_true, y_pred).numpy()\n",
        "  #Accuracy\n",
        "  accuracy_function = tf.keras.metrics.CategoricalAccuracy()\n",
        "  accuracy_function.update_state(y_true, y_pred)\n",
        "  accuracy = accuracy_function.result().numpy()\n",
        "  return loss, accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLZLY0D4gl6U"
      },
      "source": [
        "**1. Predictions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0RS3zni1gkrt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "382b708d-7807-4721-ff21-665d4cfc7dbc"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Calculate predictions\n",
        "y_test_pred_tf = model.predict(x_test)\n",
        "y_test_pred_no_quant_tflite = predict_tflite(model_no_quant_tflite, x_test)\n",
        "y_test_pred_tflite = predict_tflite(model_tflite, x_test)\n",
        "\n",
        "# Preparing variables for the plot\n",
        "y_test_gest1, y_test_gest2 = prepareY4Plot(y_test)\n",
        "y_test_pred_tf_gest1, y_test_pred_tf_gest2 = prepareY4Plot(y_test_pred_tf)\n",
        "y_test_pred_no_quant_tflite_gest1, y_test_pred_no_quant_tflite_gest2 = prepareY4Plot(y_test_pred_no_quant_tflite)\n",
        "y_test_pred_tflite_gest1, y_test_pred_tflite_gest2 = prepareY4Plot(y_test_pred_tflite)\n",
        "\n",
        "# Compare the predictions\n",
        "x_plot_gest1 = range(0,int(len(x_test)/2))\n",
        "x_plot_gest2 = range(int(len(x_test)/2),len(x_test))\n",
        "plt.clf()\n",
        "plt.title('Comparison of various models against actual values')\n",
        "#plt.plot(x_plot_gest1, y_test_gest1[:,0], 'ko', label='Actual values')\n",
        "#plt.plot(x_plot_gest2, y_test_gest2[:,1], 'ko')\n",
        "plt.plot(x_plot_gest1, y_test_pred_tf_gest1[:,0], 'ro', label='TF predictions')\n",
        "plt.plot(x_plot_gest2, y_test_pred_tf_gest2[:,1], 'ro')\n",
        "plt.plot(x_plot_gest1, y_test_pred_no_quant_tflite_gest1[:,0], 'bx', label='TFLite predictions')\n",
        "plt.plot(x_plot_gest2, y_test_pred_no_quant_tflite_gest2[:,1], 'bx')\n",
        "plt.plot(x_plot_gest1, y_test_pred_tflite_gest1[:,0], 'gx', label='TFLite quantized predictions')\n",
        "plt.plot(x_plot_gest2, y_test_pred_tflite_gest2[:,1], 'gx')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7vlfJqbiZMU"
      },
      "source": [
        "**2. Loss (Categorical Crossentropy) and Accuracy (Categorical)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IpHifyGZRhw8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "outputId": "212a0555-c17d-45b2-aea4-8992c1365921"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                               Loss  Accuracy\n",
              "Model                                        \n",
              "TensorFlow                 0.246759  0.911111\n",
              "TensorFlow Lite            0.246759  0.911111\n",
              "TensorFlow Lite Quantized  2.648698  0.716667"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-38274610-99cf-4b01-be49-aceb4fce2a41\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Model</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>TensorFlow</th>\n",
              "      <td>0.246759</td>\n",
              "      <td>0.911111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TensorFlow Lite</th>\n",
              "      <td>0.246759</td>\n",
              "      <td>0.911111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TensorFlow Lite Quantized</th>\n",
              "      <td>2.648698</td>\n",
              "      <td>0.716667</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-38274610-99cf-4b01-be49-aceb4fce2a41')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-38274610-99cf-4b01-be49-aceb4fce2a41 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-38274610-99cf-4b01-be49-aceb4fce2a41');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "# Calculate loss\n",
        "loss_tf, accuracy_tf = model.evaluate(x_test, y_test, verbose=0)\n",
        "loss_no_quant_tflite, accuracy_no_quant_tflite = evaluate_tflite(model_no_quant_tflite, x_test, y_test)\n",
        "loss_tflite, accuracy_tflite = evaluate_tflite(model_tflite, x_test, y_test)\n",
        "import statistics\n",
        "loss_no_quant_tflite = statistics.mean(loss_no_quant_tflite)\n",
        "loss_tflite = statistics.mean(loss_tflite)\n",
        "\n",
        "# Compare loss\n",
        "df = pd.DataFrame.from_records(\n",
        "    [[\"TensorFlow\", loss_tf, accuracy_tf],\n",
        "     [\"TensorFlow Lite\", loss_no_quant_tflite, accuracy_no_quant_tflite],\n",
        "     [\"TensorFlow Lite Quantized\", loss_tflite, accuracy_tflite]],\n",
        "     columns = [\"Model\", \"Loss\", \"Accuracy\"], index=\"Model\")\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7Vjw7VckLu1"
      },
      "source": [
        "**3. File Size**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wEXiJ8dFkL2R",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "outputId": "286fe9e0-cd81-45e2-8506-c03ff20852b7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                  Size                                \n",
              "Model                                                                 \n",
              "TensorFlow                 41144 bytes                                \n",
              "TensorFlow Lite            4384 bytes         (reduced by 36760 bytes)\n",
              "TensorFlow Lite Quantized   3488 bytes  (further reduced by 896 bytes)"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a5ecdf68-832d-45ea-be71-eb643eda9762\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Size</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Model</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>TensorFlow</th>\n",
              "      <td>41144 bytes</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TensorFlow Lite</th>\n",
              "      <td>4384 bytes</td>\n",
              "      <td>(reduced by 36760 bytes)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TensorFlow Lite Quantized</th>\n",
              "      <td>3488 bytes</td>\n",
              "      <td>(further reduced by 896 bytes)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a5ecdf68-832d-45ea-be71-eb643eda9762')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a5ecdf68-832d-45ea-be71-eb643eda9762 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a5ecdf68-832d-45ea-be71-eb643eda9762');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "# Calculate size\n",
        "size_tf = os.path.getsize(MODEL_TF_H5)\n",
        "size_no_quant_tflite = os.path.getsize(MODEL_NO_QUANT_TFLITE)\n",
        "size_tflite = os.path.getsize(MODEL_TFLITE)\n",
        "\n",
        "# Compare size\n",
        "pd.DataFrame.from_records(\n",
        "    [[\"TensorFlow\", f\"{size_tf} bytes\", \"\"],\n",
        "     [\"TensorFlow Lite\", f\"{size_no_quant_tflite} bytes \", f\"(reduced by {size_tf - size_no_quant_tflite} bytes)\"],\n",
        "     [\"TensorFlow Lite Quantized\", f\"{size_tflite} bytes\", f\"(further reduced by {size_no_quant_tflite - size_tflite} bytes)\"]],\n",
        "     columns = [\"Model\", \"Size\", \"\"], index=\"Model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPSFmDL7pv2L"
      },
      "source": [
        "## Generate a TensorFlow Lite for Microcontrollers Model\n",
        "The following cell converts the TensorFlow Lite model into a C source file that can be loaded by TensorFlow Lite for microcontrollers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j1FB4ieeg0lw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2866713-c932-4221-9ad6-157c6672a129"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.91.38)] [Connecting to security.ub\r                                                                               \rHit:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "\r                                                                               \rIgn:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [Waiting for headers] [Connecting to security.ubuntu.com (185.125.190.36)] [\r0% [1 InRelease gpgv 1,581 B] [Waiting for headers] [Connecting to security.ubu\r                                                                               \rHit:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "\r0% [1 InRelease gpgv 1,581 B] [Waiting for headers] [Connecting to security.ubu\r                                                                               \rHit:5 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "\r0% [1 InRelease gpgv 1,581 B] [Waiting for headers] [Waiting for headers] [Wait\r                                                                               \rHit:6 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "\r0% [1 InRelease gpgv 1,581 B] [Waiting for headers] [Waiting for headers] [Wait\r                                                                               \r0% [Waiting for headers] [Waiting for headers] [Waiting for headers]\r0% [2 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers] [Wait\r                                                                               \rHit:7 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "\r                                                                               \r0% [2 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers]\r                                                                         \rHit:8 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "\r0% [2 InRelease gpgv 3,626 B] [Waiting for headers] [Connecting to ppa.launchpa\r                                                                               \rHit:9 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "\r0% [2 InRelease gpgv 3,626 B] [Connecting to ppa.launchpad.net (185.125.190.52)\r                                                                               \rHit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:11 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:12 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Reading package lists... Done\n"
          ]
        }
      ],
      "source": [
        "# Install xxd if it is not available\n",
        "!apt-get update && apt-get -qq install xxd\n",
        "# Convert to a C source file, i.e, a TensorFlow Lite for Microcontrollers model\n",
        "!xxd -i {MODEL_NO_QUANT_TFLITE} > {MODEL_TFLITE_MICRO}\n",
        "# Update variable names\n",
        "REPLACE_TEXT = MODEL_TFLITE.replace('/', '_').replace('.', '_')\n",
        "!sed -i 's/'{REPLACE_TEXT}'/g_model/g' {MODEL_TFLITE_MICRO}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvRy0ZyMhQOX"
      },
      "source": [
        "## Deploy to a Microcontroller\n",
        "\n",
        "In our case, we are interested in deploying the model to an Arduino Nano 33 BLE. To do this, the array in the C source file is copied and pasted into one of the examples of the Arduino_TensorFlowLite library in Arduino, and the corresponding array length is substituted as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4-WhtGpvb-E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f093b14-b5c5-463c-88d4-b5c074a1de41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unsigned char models_model_no_quant_tflite[] = {\n",
            "  0x1c, 0x00, 0x00, 0x00, 0x54, 0x46, 0x4c, 0x33, 0x14, 0x00, 0x20, 0x00,\n",
            "  0x04, 0x00, 0x08, 0x00, 0x0c, 0x00, 0x10, 0x00, 0x14, 0x00, 0x00, 0x00,\n",
            "  0x18, 0x00, 0x1c, 0x00, 0x14, 0x00, 0x00, 0x00, 0x03, 0x00, 0x00, 0x00,\n",
            "  0x18, 0x00, 0x00, 0x00, 0x20, 0x00, 0x00, 0x00, 0x40, 0x01, 0x00, 0x00,\n",
            "  0x20, 0x00, 0x00, 0x00, 0xe8, 0x00, 0x00, 0x00, 0x50, 0x00, 0x00, 0x00,\n",
            "  0x02, 0x00, 0x00, 0x00, 0xd4, 0x03, 0x00, 0x00, 0x00, 0x02, 0x00, 0x00,\n",
            "  0x01, 0x00, 0x00, 0x00, 0x44, 0x01, 0x00, 0x00, 0x0d, 0x00, 0x00, 0x00,\n",
            "  0xc8, 0x10, 0x00, 0x00, 0xc4, 0x10, 0x00, 0x00, 0xf8, 0x0f, 0x00, 0x00,\n",
            "  0x64, 0x0f, 0x00, 0x00, 0xb8, 0x0e, 0x00, 0x00, 0x00, 0x0c, 0x00, 0x00,\n",
            "  0xd0, 0x04, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0xa8, 0x10, 0x00, 0x00,\n",
            "  0xa4, 0x10, 0x00, 0x00, 0xa0, 0x10, 0x00, 0x00, 0x9c, 0x10, 0x00, 0x00,\n",
            "  0xcc, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x10, 0x00, 0x00, 0x00,\n",
            "  0x0c, 0x00, 0x14, 0x00, 0x10, 0x00, 0x0c, 0x00, 0x08, 0x00, 0x04, 0x00,\n",
            "  0x0c, 0x00, 0x00, 0x00, 0x10, 0x00, 0x00, 0x00, 0x18, 0x00, 0x00, 0x00,\n",
            "  0x28, 0x00, 0x00, 0x00, 0x2c, 0x00, 0x00, 0x00, 0x05, 0x00, 0x00, 0x00,\n",
            "  0x73, 0x65, 0x72, 0x76, 0x65, 0x00, 0x00, 0x00, 0x0f, 0x00, 0x00, 0x00,\n",
            "  0x73, 0x65, 0x72, 0x76, 0x69, 0x6e, 0x67, 0x5f, 0x64, 0x65, 0x66, 0x61,\n",
            "  0x75, 0x6c, 0x74, 0x00, 0x01, 0x00, 0x00, 0x00, 0x14, 0x00, 0x00, 0x00,\n",
            "  0x01, 0x00, 0x00, 0x00, 0x24, 0x00, 0x00, 0x00, 0x08, 0x00, 0x0c, 0x00,\n",
            "  0x08, 0x00, 0x04, 0x00, 0x08, 0x00, 0x00, 0x00, 0x0a, 0x00, 0x00, 0x00,\n",
            "  0x04, 0x00, 0x00, 0x00, 0x07, 0x00, 0x00, 0x00, 0x64, 0x65, 0x6e, 0x73,\n",
            "  0x65, 0x5f, 0x38, 0x00, 0xb2, 0xf0, 0xff, 0xff, 0x04, 0x00, 0x00, 0x00,\n",
            "  0x0d, 0x00, 0x00, 0x00, 0x64, 0x65, 0x6e, 0x73, 0x65, 0x5f, 0x36, 0x5f,\n",
            "  0x69, 0x6e, 0x70, 0x75, 0x74, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00,\n",
            "  0x0c, 0x00, 0x00, 0x00, 0x08, 0x00, 0x0c, 0x00, 0x04, 0x00, 0x08, 0x00,\n",
            "  0x08, 0x00, 0x00, 0x00, 0x08, 0x00, 0x00, 0x00, 0x0c, 0x00, 0x00, 0x00,\n",
            "  0x13, 0x00, 0x00, 0x00, 0x6d, 0x69, 0x6e, 0x5f, 0x72, 0x75, 0x6e, 0x74,\n",
            "  0x69, 0x6d, 0x65, 0x5f, 0x76, 0x65, 0x72, 0x73, 0x69, 0x6f, 0x6e, 0x00,\n",
            "  0x02, 0xf1, 0xff, 0xff, 0x04, 0x00, 0x00, 0x00, 0x10, 0x00, 0x00, 0x00,\n",
            "  0x31, 0x2e, 0x35, 0x2e, 0x30, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n",
            "  0x00, 0x00, 0x00, 0x00, 0x0f, 0x00, 0x00, 0x00, 0x4d, 0x4c, 0x49, 0x52,\n",
            "  0x20, 0x43, 0x6f, 0x6e, 0x76, 0x65, 0x72, 0x74, 0x65, 0x64, 0x2e, 0x00,\n",
            "  0x00, 0x00, 0x0e, 0x00, 0x18, 0x00, 0x04, 0x00, 0x08, 0x00, 0x0c, 0x00,\n",
            "  0x10, 0x00, 0x14, 0x00, 0x0e, 0x00, 0x00, 0x00, 0x14, 0x00, 0x00, 0x00,\n",
            "  0x40, 0x00, 0x00, 0x00, 0x44, 0x00, 0x00, 0x00, 0x48, 0x00, 0x00, 0x00,\n",
            "  0x58, 0x00, 0x00, 0x00, 0x0b, 0x00, 0x00, 0x00, 0x14, 0x0f, 0x00, 0x00,\n",
            "  0xc8, 0x0e, 0x00, 0x00, 0x60, 0x0e, 0x00, 0x00, 0xd8, 0x0d, 0x00, 0x00,\n",
            "  0x28, 0x0d, 0x00, 0x00, 0x70, 0x0a, 0x00, 0x00, 0x40, 0x03, 0x00, 0x00,\n",
            "  0x5c, 0x02, 0x00, 0x00, 0xa0, 0x01, 0x00, 0x00, 0x08, 0x01, 0x00, 0x00,\n",
            "  0x80, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n",
            "  0x01, 0x00, 0x00, 0x00, 0x0a, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00,\n",
            "  0xe0, 0x01, 0x00, 0x00, 0x48, 0x01, 0x00, 0x00, 0xb4, 0x00, 0x00, 0x00,\n",
            "  0x1c, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0x6d, 0x61, 0x69, 0x6e,\n",
            "  0x00, 0x00, 0x0e, 0x00, 0x18, 0x00, 0x08, 0x00, 0x0c, 0x00, 0x10, 0x00,\n",
            "  0x07, 0x00, 0x14, 0x00, 0x0e, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x09,\n",
            "  0x01, 0x00, 0x00, 0x00, 0x1c, 0x00, 0x00, 0x00, 0x10, 0x00, 0x00, 0x00,\n",
            "  0x04, 0x00, 0x00, 0x00, 0xde, 0xf1, 0xff, 0xff, 0x00, 0x00, 0x80, 0x3f,\n",
            "  0x01, 0x00, 0x00, 0x00, 0x0a, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00,\n",
            "  0x09, 0x00, 0x00, 0x00, 0x3c, 0xfe, 0xff, 0xff, 0x19, 0x00, 0x00, 0x00,\n",
            "  0x00, 0x00, 0x00, 0x19, 0x01, 0x00, 0x00, 0x00, 0xa8, 0xf1, 0xff, 0xff,\n",
            "  0x14, 0x00, 0x00, 0x00, 0x0b, 0x00, 0x00, 0x00, 0x18, 0x00, 0x00, 0x00,\n",
            "  0x40, 0x00, 0x00, 0x00, 0x30, 0x00, 0x00, 0x00, 0x02, 0x00, 0x00, 0x00,\n",
            "  0x01, 0x00, 0x00, 0x00, 0x02, 0x00, 0x00, 0x00, 0x19, 0x00, 0x00, 0x00,\n",
            "  0x53, 0x74, 0x61, 0x74, 0x65, 0x66, 0x75, 0x6c, 0x50, 0x61, 0x72, 0x74,\n",
            "  0x69, 0x74, 0x69, 0x6f, 0x6e, 0x65, 0x64, 0x43, 0x61, 0x6c, 0x6c, 0x3a,\n",
            "  0x30, 0x00, 0x00, 0x00, 0x02, 0x00, 0x00, 0x00, 0xff, 0xff, 0xff, 0xff,\n",
            "  0x02, 0x00, 0x00, 0x00, 0x8c, 0xf1, 0xff, 0xff, 0x7e, 0xff, 0xff, 0xff,\n",
            "  0x00, 0x00, 0x00, 0x08, 0x18, 0x00, 0x00, 0x00, 0x0c, 0x00, 0x00, 0x00,\n",
            "  0x04, 0x00, 0x00, 0x00, 0xa4, 0xf1, 0xff, 0xff, 0x01, 0x00, 0x00, 0x00,\n",
            "  0x09, 0x00, 0x00, 0x00, 0x03, 0x00, 0x00, 0x00, 0x08, 0x00, 0x00, 0x00,\n",
            "  0x06, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x2c, 0xf2, 0xff, 0xff,\n",
            "  0x14, 0x00, 0x00, 0x00, 0x0a, 0x00, 0x00, 0x00, 0x18, 0x00, 0x00, 0x00,\n",
            "  0x3c, 0x00, 0x00, 0x00, 0x2c, 0x00, 0x00, 0x00, 0x02, 0x00, 0x00, 0x00,\n",
            "  0x01, 0x00, 0x00, 0x00, 0x02, 0x00, 0x00, 0x00, 0x14, 0x00, 0x00, 0x00,\n",
            "  0x74, 0x66, 0x6c, 0x2e, 0x66, 0x75, 0x6c, 0x6c, 0x79, 0x5f, 0x63, 0x6f,\n",
            "  0x6e, 0x6e, 0x65, 0x63, 0x74, 0x65, 0x64, 0x32, 0x00, 0x00, 0x00, 0x00,\n",
            "  0x02, 0x00, 0x00, 0x00, 0xff, 0xff, 0xff, 0xff, 0x02, 0x00, 0x00, 0x00,\n",
            "  0xe0, 0xf2, 0xff, 0xff, 0x00, 0x00, 0x0e, 0x00, 0x14, 0x00, 0x00, 0x00,\n",
            "  0x08, 0x00, 0x0c, 0x00, 0x07, 0x00, 0x10, 0x00, 0x0e, 0x00, 0x00, 0x00,\n",
            "  0x00, 0x00, 0x00, 0x08, 0x1c, 0x00, 0x00, 0x00, 0x10, 0x00, 0x00, 0x00,\n",
            "  0x04, 0x00, 0x00, 0x00, 0x6a, 0xff, 0xff, 0xff, 0x00, 0x00, 0x00, 0x01,\n",
            "  0x01, 0x00, 0x00, 0x00, 0x08, 0x00, 0x00, 0x00, 0x03, 0x00, 0x00, 0x00,\n",
            "  0x07, 0x00, 0x00, 0x00, 0x05, 0x00, 0x00, 0x00, 0x02, 0x00, 0x00, 0x00,\n",
            "  0xc0, 0xf2, 0xff, 0xff, 0x14, 0x00, 0x00, 0x00, 0x09, 0x00, 0x00, 0x00,\n",
            "  0x18, 0x00, 0x00, 0x00, 0x3c, 0x00, 0x00, 0x00, 0x2c, 0x00, 0x00, 0x00,\n",
            "  0x02, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x11, 0x00, 0x00, 0x00,\n",
            "  0x14, 0x00, 0x00, 0x00, 0x74, 0x66, 0x6c, 0x2e, 0x66, 0x75, 0x6c, 0x6c,\n",
            "  0x79, 0x5f, 0x63, 0x6f, 0x6e, 0x6e, 0x65, 0x63, 0x74, 0x65, 0x64, 0x31,\n",
            "  0x00, 0x00, 0x00, 0x00, 0x02, 0x00, 0x00, 0x00, 0xff, 0xff, 0xff, 0xff,\n",
            "  0x11, 0x00, 0x00, 0x00, 0x74, 0xf3, 0xff, 0xff, 0x00, 0x00, 0x0e, 0x00,\n",
            "  0x16, 0x00, 0x00, 0x00, 0x08, 0x00, 0x0c, 0x00, 0x07, 0x00, 0x10, 0x00,\n",
            "  0x0e, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x08, 0x24, 0x00, 0x00, 0x00,\n",
            "  0x18, 0x00, 0x00, 0x00, 0x0c, 0x00, 0x00, 0x00, 0x00, 0x00, 0x06, 0x00,\n",
            "  0x08, 0x00, 0x07, 0x00, 0x06, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x01,\n",
            "  0x01, 0x00, 0x00, 0x00, 0x07, 0x00, 0x00, 0x00, 0x03, 0x00, 0x00, 0x00,\n",
            "  0x00, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0x03, 0x00, 0x00, 0x00,\n",
            "  0x0c, 0x00, 0x10, 0x00, 0x0b, 0x00, 0x00, 0x00, 0x0c, 0x00, 0x04, 0x00,\n",
            "  0x0c, 0x00, 0x00, 0x00, 0x09, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x09,\n",
            "  0x01, 0x00, 0x00, 0x00, 0x78, 0xf3, 0xff, 0xff, 0x14, 0x00, 0x00, 0x00,\n",
            "  0x08, 0x00, 0x00, 0x00, 0x18, 0x00, 0x00, 0x00, 0x38, 0x00, 0x00, 0x00,\n",
            "  0x28, 0x00, 0x00, 0x00, 0x02, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00,\n",
            "  0x1a, 0x00, 0x00, 0x00, 0x13, 0x00, 0x00, 0x00, 0x74, 0x66, 0x6c, 0x2e,\n",
            "  0x66, 0x75, 0x6c, 0x6c, 0x79, 0x5f, 0x63, 0x6f, 0x6e, 0x6e, 0x65, 0x63,\n",
            "  0x74, 0x65, 0x64, 0x00, 0x02, 0x00, 0x00, 0x00, 0xff, 0xff, 0xff, 0xff,\n",
            "  0x1a, 0x00, 0x00, 0x00, 0x54, 0xf3, 0xff, 0xff, 0x22, 0xf4, 0xff, 0xff,\n",
            "  0x04, 0x00, 0x00, 0x00, 0x88, 0x00, 0x00, 0x00, 0xd8, 0xa3, 0x81, 0x3e,\n",
            "  0x35, 0xf2, 0x05, 0xbf, 0x09, 0xd1, 0x09, 0x3f, 0xa9, 0x05, 0x9a, 0xbd,\n",
            "  0xc1, 0x39, 0x86, 0x3d, 0xb7, 0xe9, 0xd8, 0xbe, 0xb5, 0xee, 0xb7, 0xbb,\n",
            "  0x25, 0x08, 0x5e, 0x3c, 0xde, 0x36, 0xc9, 0x3e, 0xb0, 0xdd, 0xcb, 0x3d,\n",
            "  0x1b, 0xb3, 0x95, 0xbe, 0x85, 0x5b, 0x38, 0xbd, 0x2a, 0x3e, 0xfd, 0xbe,\n",
            "  0xb9, 0xbc, 0xc2, 0xbe, 0xce, 0x15, 0x0c, 0xbe, 0x27, 0x9f, 0xa1, 0x3d,\n",
            "  0x28, 0xab, 0xca, 0x3e, 0xb2, 0x8a, 0x57, 0xbd, 0x79, 0xca, 0x11, 0xbf,\n",
            "  0x90, 0x92, 0xb4, 0x3d, 0x1a, 0xd3, 0x3a, 0x3d, 0xda, 0xa6, 0xc2, 0xbc,\n",
            "  0xde, 0x14, 0x82, 0x3e, 0x71, 0x95, 0x8a, 0xbe, 0x5e, 0xf7, 0x08, 0x3e,\n",
            "  0x34, 0x43, 0x59, 0x3e, 0x50, 0x36, 0x5e, 0x3e, 0x82, 0xae, 0x4e, 0x3d,\n",
            "  0xff, 0x0a, 0x2b, 0xbe, 0x20, 0xd5, 0x8d, 0xbc, 0x1d, 0x17, 0x91, 0xbe,\n",
            "  0xb1, 0xae, 0xc6, 0x3e, 0x70, 0xff, 0x3b, 0x3e, 0x61, 0x96, 0xac, 0xbe,\n",
            "  0x9a, 0xf4, 0xff, 0xff, 0x10, 0x00, 0x00, 0x00, 0x07, 0x00, 0x00, 0x00,\n",
            "  0x14, 0x00, 0x00, 0x00, 0x24, 0x00, 0x00, 0x00, 0x02, 0x00, 0x00, 0x00,\n",
            "  0x02, 0x00, 0x00, 0x00, 0x11, 0x00, 0x00, 0x00, 0x0d, 0x00, 0x00, 0x00,\n",
            "  0x73, 0x74, 0x64, 0x2e, 0x63, 0x6f, 0x6e, 0x73, 0x74, 0x61, 0x6e, 0x74,\n",
            "  0x32, 0x00, 0x00, 0x00, 0x20, 0xf4, 0xff, 0xff, 0xee, 0xf4, 0xff, 0xff,\n",
            "  0x04, 0x00, 0x00, 0x00, 0xe8, 0x06, 0x00, 0x00, 0xc6, 0xfb, 0xe9, 0xbd,\n",
            "  0x44, 0x76, 0xcf, 0x3d, 0x7e, 0x7b, 0xb3, 0x3e, 0x8a, 0xce, 0x1d, 0xbe,\n",
            "  0xec, 0x2f, 0xb1, 0x3d, 0x32, 0x90, 0x85, 0x3e, 0x76, 0x21, 0x90, 0xbd,\n",
            "  0x40, 0xc5, 0x93, 0xbe, 0x05, 0x74, 0x0a, 0xbe, 0x62, 0x81, 0x38, 0xbe,\n",
            "  0x12, 0xe2, 0x04, 0xbe, 0xc1, 0xa0, 0x8e, 0xbd, 0x80, 0xfe, 0x61, 0xbb,\n",
            "  0xa0, 0xe0, 0xb9, 0xbe, 0xe6, 0x84, 0xf8, 0x3c, 0xf1, 0xfb, 0xdf, 0x3d,\n",
            "  0x2a, 0x2e, 0xb7, 0xbd, 0x28, 0xa9, 0x98, 0xbe, 0x70, 0x43, 0x46, 0x3d,\n",
            "  0xc0, 0xc0, 0x03, 0xbc, 0x5a, 0x57, 0x52, 0xbe, 0x60, 0x02, 0x7b, 0x3d,\n",
            "  0x18, 0x45, 0xfa, 0xbd, 0x9b, 0xe8, 0x32, 0x3d, 0x43, 0xac, 0x2b, 0x3e,\n",
            "  0xb9, 0x28, 0xf4, 0x3d, 0x29, 0xda, 0x9e, 0x3e, 0x70, 0x6e, 0x5e, 0x3d,\n",
            "  0xdc, 0x9f, 0xa1, 0xbe, 0xe4, 0x5c, 0x1d, 0x3e, 0xa8, 0x4f, 0x28, 0xbe,\n",
            "  0x69, 0xe8, 0x6b, 0xbe, 0x69, 0x94, 0x50, 0xbe, 0xc8, 0xce, 0x4b, 0xbe,\n",
            "  0x48, 0x0f, 0x8f, 0x3e, 0x54, 0x6d, 0x28, 0x3e, 0x09, 0xe1, 0x84, 0xbe,\n",
            "  0x89, 0x7b, 0xc0, 0x3e, 0xa2, 0x1c, 0x66, 0xbe, 0xf8, 0xc6, 0x2b, 0x3e,\n",
            "  0xaa, 0x05, 0x8d, 0xbe, 0x10, 0xad, 0x95, 0xbe, 0x8a, 0x2c, 0x19, 0xbe,\n",
            "  0xdc, 0x28, 0xab, 0x3e, 0x42, 0x26, 0xad, 0xbe, 0x3c, 0x7a, 0xa3, 0x3e,\n",
            "  0xbe, 0x7e, 0xb3, 0x3e, 0xa1, 0x06, 0x68, 0xbe, 0xa2, 0x57, 0x49, 0xbe,\n",
            "  0xf4, 0xe8, 0x97, 0xbe, 0x7a, 0x2d, 0x90, 0xbe, 0xfa, 0xcd, 0x75, 0xbe,\n",
            "  0x2f, 0xbf, 0x00, 0xbe, 0x7b, 0x37, 0x4a, 0xbe, 0x03, 0x26, 0x53, 0xbe,\n",
            "  0x50, 0x10, 0xc6, 0xbd, 0xa8, 0x12, 0x42, 0x3e, 0xe0, 0xdf, 0x72, 0x3d,\n",
            "  0xe0, 0x77, 0xe1, 0xbd, 0xb7, 0x80, 0xb7, 0xbe, 0xe7, 0xd6, 0x14, 0xbe,\n",
            "  0x96, 0x12, 0x9f, 0xbe, 0x8e, 0x91, 0xa9, 0x3e, 0x8f, 0x6d, 0x9b, 0xbe,\n",
            "  0x07, 0xcd, 0x1c, 0xbe, 0xf7, 0x8f, 0x32, 0xbe, 0x08, 0x62, 0x73, 0x3e,\n",
            "  0x28, 0x0f, 0x3f, 0xbd, 0x02, 0x77, 0x9a, 0xbe, 0x66, 0xaf, 0xa1, 0x3e,\n",
            "  0x8c, 0x8a, 0x19, 0x3e, 0xf8, 0x32, 0x34, 0x3d, 0xb8, 0x6c, 0x4f, 0x3d,\n",
            "  0x6c, 0x2b, 0xf5, 0x3d, 0x1c, 0xf9, 0xe6, 0x3d, 0xf4, 0x1e, 0x93, 0x3e,\n",
            "  0x9a, 0xf0, 0x41, 0xbe, 0x1c, 0xf5, 0x0a, 0x3e, 0x83, 0x75, 0x8e, 0x3c,\n",
            "  0x85, 0x2a, 0xc7, 0xbe, 0x5d, 0xce, 0x4b, 0xbe, 0x46, 0x72, 0xa9, 0x3e,\n",
            "  0x18, 0x00, 0x97, 0x3e, 0x76, 0xd1, 0x0b, 0x3e, 0x59, 0x17, 0x21, 0x3e,\n",
            "  0x03, 0xdd, 0x5e, 0xbe, 0x35, 0xdd, 0x94, 0xbe, 0xb8, 0xe9, 0x41, 0x3e,\n",
            "  0xa4, 0x16, 0xbc, 0xbe, 0xc7, 0x8c, 0xdb, 0xbe, 0xda, 0xc5, 0xa4, 0x3d,\n",
            "  0xff, 0x12, 0x0a, 0x3e, 0xea, 0x65, 0x96, 0x3e, 0x4f, 0x08, 0xd3, 0xbe,\n",
            "  0x21, 0xf9, 0xcd, 0xbe, 0x30, 0x66, 0xaa, 0x3e, 0x10, 0x47, 0xe8, 0x3d,\n",
            "  0x40, 0xb4, 0xac, 0xbd, 0xe8, 0x08, 0x2e, 0x3e, 0x12, 0x90, 0xba, 0xbe,\n",
            "  0x66, 0x17, 0xbf, 0x3e, 0x23, 0xc4, 0x8c, 0xbd, 0x81, 0xb6, 0x65, 0x3d,\n",
            "  0x5a, 0x1d, 0xfb, 0x3c, 0xb8, 0x7b, 0xef, 0x3d, 0x19, 0xb6, 0xc0, 0x3e,\n",
            "  0xec, 0xbb, 0xda, 0xbd, 0xc8, 0x75, 0xc4, 0xbd, 0xc0, 0x44, 0xa5, 0x3e,\n",
            "  0x5f, 0x7b, 0xc7, 0xbe, 0xb2, 0xaf, 0x52, 0xbc, 0xd9, 0xe2, 0x87, 0x3e,\n",
            "  0xb8, 0xf5, 0x65, 0x3e, 0xe7, 0xdc, 0x8e, 0xbe, 0x2e, 0x01, 0x5f, 0x3e,\n",
            "  0x30, 0x3e, 0x9c, 0x3e, 0x17, 0x93, 0x86, 0x3e, 0x1a, 0x8d, 0xbe, 0xbd,\n",
            "  0x3c, 0xe0, 0xc2, 0x3d, 0x17, 0xad, 0x97, 0x39, 0x68, 0xa3, 0x42, 0x3e,\n",
            "  0xc4, 0x17, 0x28, 0xbb, 0xb4, 0xff, 0x7d, 0x3e, 0x40, 0xc5, 0xb5, 0x3c,\n",
            "  0xbc, 0x2a, 0x34, 0xbe, 0x40, 0x68, 0x74, 0x3c, 0xe0, 0x1c, 0xbc, 0xbc,\n",
            "  0x35, 0x1f, 0x4f, 0x3e, 0x3b, 0xf2, 0x0a, 0xbe, 0x7a, 0xd7, 0x1f, 0xbe,\n",
            "  0xc2, 0x4a, 0x65, 0x3e, 0x99, 0xb8, 0xb7, 0xbe, 0x40, 0xfc, 0xa5, 0xbc,\n",
            "  0xf3, 0x06, 0xb5, 0xbe, 0x04, 0x33, 0x57, 0xbe, 0xdc, 0xc1, 0x78, 0xbe,\n",
            "  0x8d, 0x79, 0x89, 0xbe, 0x70, 0xd7, 0x15, 0xbd, 0xd4, 0x6c, 0x6f, 0x3e,\n",
            "  0xd4, 0x1e, 0xa2, 0xbe, 0x05, 0x13, 0x3d, 0xbe, 0xa6, 0x41, 0x1c, 0x3e,\n",
            "  0x52, 0x87, 0x8a, 0xbe, 0xf4, 0xeb, 0xb1, 0x3e, 0x68, 0xcd, 0xfc, 0x3d,\n",
            "  0xaf, 0xf9, 0x1c, 0xbe, 0x24, 0x8f, 0x9c, 0x3e, 0xf0, 0xc2, 0xf1, 0xbc,\n",
            "  0x96, 0x43, 0x28, 0xbe, 0x40, 0x06, 0x0a, 0xbd, 0x48, 0xe1, 0x31, 0xbd,\n",
            "  0x60, 0xe0, 0x22, 0x3e, 0x5c, 0x69, 0x94, 0x3d, 0x2f, 0xea, 0xa3, 0xbd,\n",
            "  0x58, 0x0c, 0x26, 0x3e, 0xf0, 0xa9, 0x7d, 0xbd, 0xca, 0x20, 0xd0, 0xbe,\n",
            "  0x84, 0x0d, 0x43, 0x3e, 0xc4, 0x21, 0x0b, 0x3e, 0xfc, 0xd6, 0xa6, 0x3d,\n",
            "  0xa7, 0x36, 0x40, 0xbe, 0xb6, 0xeb, 0x32, 0x3d, 0x00, 0xcf, 0xb9, 0x3d,\n",
            "  0x99, 0x2d, 0x2c, 0x3e, 0x42, 0xd1, 0x07, 0xbe, 0x80, 0x64, 0x11, 0xbe,\n",
            "  0xe0, 0x13, 0xa2, 0xbd, 0x1b, 0x62, 0x07, 0xbe, 0xcf, 0x81, 0x4f, 0xbe,\n",
            "  0x78, 0x3a, 0x23, 0x3e, 0x60, 0x44, 0xec, 0x3c, 0x2e, 0xbb, 0xc9, 0xbe,\n",
            "  0x04, 0xb2, 0x5c, 0xbe, 0x04, 0x1a, 0xb9, 0x3e, 0xa0, 0x91, 0x5f, 0x3c,\n",
            "  0xfb, 0x98, 0x79, 0xbe, 0x9d, 0x25, 0x25, 0x3e, 0x92, 0xf9, 0xb6, 0xbe,\n",
            "  0x74, 0x98, 0xb5, 0x3e, 0x78, 0xca, 0x90, 0xbe, 0xa0, 0x80, 0x12, 0x3d,\n",
            "  0x39, 0x90, 0xbb, 0xbe, 0x3f, 0x1f, 0xa1, 0xba, 0xbe, 0xc2, 0xd2, 0xbe,\n",
            "  0x48, 0xe5, 0xd4, 0x3d, 0xe2, 0x71, 0x00, 0xbe, 0x90, 0x18, 0xb7, 0x3e,\n",
            "  0x25, 0xa6, 0x72, 0x3e, 0xe9, 0x68, 0x3d, 0xbf, 0xfb, 0xce, 0xb3, 0x3d,\n",
            "  0x65, 0x9c, 0x4c, 0xbe, 0x80, 0x79, 0x66, 0x3d, 0x1d, 0x19, 0x0e, 0x3d,\n",
            "  0x09, 0xcb, 0xfc, 0x3d, 0x3f, 0x44, 0xef, 0xbe, 0xc9, 0x9d, 0x2e, 0x3d,\n",
            "  0x20, 0x26, 0x23, 0xbe, 0x52, 0x3b, 0x2f, 0xbc, 0x21, 0x2e, 0xa4, 0x3d,\n",
            "  0x0c, 0x81, 0xa7, 0xbe, 0x2a, 0x6e, 0x83, 0xbe, 0x3c, 0xb9, 0x1c, 0x3e,\n",
            "  0x20, 0xa1, 0xef, 0xbe, 0xf4, 0xd2, 0xad, 0xbd, 0xe7, 0x5a, 0x51, 0xbe,\n",
            "  0x77, 0x3d, 0x1d, 0xbe, 0xf2, 0x5c, 0x00, 0x3e, 0x58, 0xe3, 0xd8, 0x3d,\n",
            "  0x70, 0x0c, 0x5b, 0x3e, 0xb2, 0xe0, 0x8a, 0x3d, 0x44, 0xa0, 0x75, 0xbe,\n",
            "  0x20, 0x74, 0xb7, 0x3c, 0xe0, 0xd6, 0xc0, 0x3d, 0xc9, 0x39, 0xbc, 0xbe,\n",
            "  0xe9, 0x33, 0xac, 0x3e, 0x65, 0x09, 0x92, 0x3e, 0x89, 0x5e, 0x37, 0xbe,\n",
            "  0x7a, 0x1c, 0x9b, 0x3e, 0x04, 0x55, 0x60, 0x3e, 0x50, 0x2b, 0xc9, 0xbd,\n",
            "  0x0b, 0xe9, 0x1a, 0xbf, 0x2a, 0xb6, 0xdb, 0xbd, 0x28, 0xde, 0x3a, 0x3d,\n",
            "  0x8c, 0x44, 0x0f, 0xbc, 0x12, 0xad, 0xb5, 0xbf, 0xc4, 0x53, 0x47, 0xbe,\n",
            "  0x8a, 0x85, 0x90, 0xbe, 0xd4, 0x99, 0x58, 0x3e, 0xfe, 0xef, 0x0b, 0xbf,\n",
            "  0xc8, 0x79, 0xdb, 0xbd, 0xc6, 0x61, 0x86, 0x3e, 0x0e, 0xa2, 0x75, 0xbe,\n",
            "  0x32, 0xa4, 0xad, 0xbe, 0x1f, 0xef, 0x86, 0x3d, 0xc0, 0xda, 0x17, 0x3c,\n",
            "  0x94, 0x83, 0x85, 0x3d, 0xe8, 0x56, 0xb6, 0xbe, 0xa6, 0xf2, 0xb2, 0xbe,\n",
            "  0xc1, 0xff, 0xaf, 0xbe, 0x40, 0x45, 0xbd, 0xbe, 0x08, 0x8f, 0x3a, 0x3e,\n",
            "  0x8c, 0x58, 0x31, 0xbe, 0x1c, 0x4b, 0x73, 0x3e, 0x70, 0xe9, 0x87, 0x3e,\n",
            "  0x26, 0x2a, 0xaa, 0x3e, 0x44, 0xd4, 0xae, 0x3e, 0xb9, 0x2a, 0x80, 0xbe,\n",
            "  0x78, 0x81, 0x55, 0x3e, 0x88, 0x6c, 0xcc, 0x3d, 0xc0, 0x9e, 0xa5, 0x3e,\n",
            "  0x90, 0x6f, 0x91, 0xbd, 0xf3, 0xf1, 0x16, 0xbe, 0x40, 0xd1, 0x9e, 0x3e,\n",
            "  0x15, 0x1f, 0x5a, 0xbe, 0xe8, 0xb9, 0x1b, 0x3e, 0x4c, 0x93, 0x42, 0x3e,\n",
            "  0x22, 0x96, 0xab, 0x3e, 0xd4, 0x39, 0x44, 0x3e, 0x1a, 0x23, 0xbf, 0x3e,\n",
            "  0x1c, 0x03, 0x5f, 0xbe, 0x54, 0x0f, 0xa5, 0x3d, 0xf6, 0x9e, 0x60, 0xbe,\n",
            "  0xd4, 0x60, 0x84, 0x3e, 0x79, 0x4a, 0x88, 0xbe, 0xb9, 0x41, 0x1f, 0xbe,\n",
            "  0x0d, 0x42, 0x37, 0x3e, 0x9b, 0x86, 0x36, 0xbe, 0xe0, 0xf8, 0x02, 0xbe,\n",
            "  0xbd, 0x0a, 0x1c, 0xbe, 0x94, 0x57, 0xa1, 0x3e, 0xed, 0xe9, 0x99, 0xbe,\n",
            "  0xce, 0xf9, 0x0d, 0xbe, 0x89, 0xb2, 0xf4, 0xbc, 0x73, 0xad, 0x22, 0x3e,\n",
            "  0x67, 0x99, 0x87, 0x3e, 0x30, 0xab, 0xaa, 0x3e, 0x37, 0x37, 0x72, 0x3e,\n",
            "  0x05, 0x82, 0x0d, 0x3d, 0x1e, 0xa1, 0x9b, 0x3e, 0x90, 0xae, 0xb4, 0xbd,\n",
            "  0x08, 0x8d, 0x3d, 0xbd, 0x08, 0x0f, 0x0d, 0x3e, 0xa3, 0x4a, 0x82, 0xbe,\n",
            "  0x67, 0x3a, 0xae, 0xbd, 0x8e, 0xcd, 0x45, 0xbd, 0xaf, 0x8b, 0x33, 0x3d,\n",
            "  0x13, 0x98, 0x44, 0xbf, 0x55, 0x08, 0x81, 0x3f, 0x48, 0x55, 0x39, 0x3e,\n",
            "  0xc0, 0xe1, 0x6d, 0x3e, 0xd0, 0xef, 0x9a, 0x3e, 0x82, 0xac, 0x82, 0xbf,\n",
            "  0x55, 0x33, 0x9c, 0xbe, 0x5c, 0x8c, 0x8a, 0x3e, 0x1c, 0x5a, 0x1e, 0x3e,\n",
            "  0xa4, 0xd2, 0x35, 0x3e, 0x43, 0x65, 0x74, 0x3e, 0x75, 0x92, 0xb2, 0xbe,\n",
            "  0x3d, 0xe6, 0x91, 0xbd, 0xaf, 0x91, 0xb9, 0xbd, 0x8b, 0x4f, 0x88, 0xbd,\n",
            "  0xf9, 0x27, 0x7c, 0xbc, 0x2e, 0xe3, 0x6d, 0x3e, 0xe8, 0x84, 0xcd, 0x3d,\n",
            "  0x14, 0xae, 0x16, 0x3e, 0xb0, 0x5b, 0x6d, 0xbd, 0xad, 0x97, 0x32, 0x3e,\n",
            "  0x7b, 0x19, 0x1e, 0xbe, 0xe4, 0xee, 0x4e, 0x3e, 0x58, 0xd8, 0x32, 0x3e,\n",
            "  0xdb, 0xa3, 0x5a, 0x3e, 0x53, 0x52, 0x4f, 0xbe, 0xe3, 0x14, 0x83, 0xbe,\n",
            "  0xd0, 0xe1, 0x1b, 0x3e, 0xd8, 0x88, 0x19, 0x3e, 0x00, 0x5d, 0xf3, 0x3b,\n",
            "  0x14, 0xe8, 0x8f, 0xbe, 0x10, 0xf0, 0xd1, 0xbd, 0x0a, 0xed, 0x5b, 0xbe,\n",
            "  0x38, 0xe0, 0x88, 0xbd, 0xf4, 0x1a, 0x1e, 0xbe, 0x53, 0xbc, 0x8a, 0xbe,\n",
            "  0x30, 0xae, 0x2c, 0x3d, 0x38, 0x65, 0x42, 0x3e, 0x60, 0xd7, 0x29, 0xbc,\n",
            "  0x42, 0x41, 0x95, 0x3e, 0x24, 0xd7, 0x87, 0xbd, 0xaf, 0x8f, 0xaa, 0xbe,\n",
            "  0x20, 0x45, 0x9c, 0xbe, 0x60, 0xbf, 0x97, 0xbe, 0x46, 0x1d, 0x30, 0xbe,\n",
            "  0xad, 0x36, 0x92, 0xbe, 0x0a, 0x00, 0xaf, 0xbe, 0x73, 0xb0, 0x28, 0xbe,\n",
            "  0x80, 0x00, 0x94, 0xbe, 0x58, 0x4b, 0xf4, 0xbd, 0x10, 0x44, 0xad, 0x3c,\n",
            "  0xf0, 0xc7, 0x46, 0xbd, 0xbc, 0x66, 0x2a, 0xbc, 0x9e, 0x5e, 0x33, 0xbd,\n",
            "  0xa5, 0xc7, 0x46, 0xbe, 0x98, 0x1e, 0xeb, 0x3d, 0x89, 0xc6, 0x24, 0xbe,\n",
            "  0x35, 0xbc, 0x92, 0x3e, 0x14, 0xc3, 0x7c, 0xbb, 0x1c, 0x34, 0xb0, 0xbe,\n",
            "  0xfc, 0x31, 0x09, 0x3e, 0x14, 0x22, 0x1d, 0x3e, 0xa7, 0x1f, 0x8e, 0xbe,\n",
            "  0x4d, 0xbb, 0x4f, 0xbd, 0xdf, 0x34, 0x74, 0xbe, 0xa2, 0x88, 0x04, 0x3e,\n",
            "  0xc8, 0xde, 0x6c, 0x3e, 0xe4, 0x9e, 0x97, 0xbe, 0x0c, 0xd6, 0x9c, 0x3e,\n",
            "  0x44, 0x6a, 0x7d, 0x3e, 0x0c, 0x76, 0xf3, 0xbd, 0x98, 0xee, 0x95, 0xbd,\n",
            "  0xf9, 0x9a, 0xaa, 0xbe, 0xc4, 0x8b, 0x8b, 0x3e, 0x3f, 0x8c, 0x89, 0xbe,\n",
            "  0xfc, 0x03, 0x23, 0x3e, 0xd2, 0x4e, 0x87, 0xbe, 0x4b, 0xec, 0xf9, 0x3d,\n",
            "  0xd9, 0xf7, 0x59, 0xbd, 0x87, 0xd2, 0x61, 0xbe, 0x80, 0x2e, 0xb6, 0xbb,\n",
            "  0x80, 0xc2, 0xe4, 0x3c, 0x10, 0xd6, 0x8e, 0x3e, 0x37, 0xba, 0xab, 0xbe,\n",
            "  0x2e, 0xfb, 0x09, 0x3e, 0xdb, 0x3d, 0x41, 0xbe, 0xf0, 0x42, 0x34, 0x3e,\n",
            "  0xc0, 0x28, 0x43, 0xbc, 0xc0, 0x8f, 0x0b, 0x3e, 0x89, 0x12, 0xae, 0xbe,\n",
            "  0xb6, 0x05, 0x85, 0x3e, 0x6d, 0x8a, 0xce, 0xbe, 0xcf, 0x80, 0x13, 0xbe,\n",
            "  0x08, 0x9d, 0xb5, 0xbe, 0x7b, 0x87, 0x5f, 0x3e, 0x68, 0x2d, 0x24, 0x3e,\n",
            "  0xda, 0xcd, 0x2f, 0xbe, 0x98, 0xc7, 0x6b, 0xbe, 0x0a, 0x36, 0x97, 0x3e,\n",
            "  0x98, 0x22, 0x52, 0xbd, 0x70, 0xdc, 0x0b, 0xbd, 0xb0, 0xed, 0x2f, 0xbd,\n",
            "  0xd4, 0x85, 0xa6, 0x3d, 0x85, 0x4d, 0xa7, 0xbe, 0x57, 0xc9, 0x88, 0x3d,\n",
            "  0x00, 0x07, 0x87, 0xbf, 0x19, 0x4b, 0xad, 0xbe, 0xce, 0x4d, 0xbb, 0x3e,\n",
            "  0x7f, 0xa8, 0xad, 0xbe, 0x63, 0x74, 0x84, 0x3f, 0x98, 0x3f, 0x7a, 0xbf,\n",
            "  0x32, 0xc0, 0x1b, 0x3d, 0x84, 0x05, 0x88, 0xbd, 0xf0, 0xe2, 0x06, 0x3e,\n",
            "  0x3c, 0xac, 0x53, 0xbe, 0x3b, 0x0b, 0xf6, 0xbd, 0x98, 0x56, 0xb3, 0xbd,\n",
            "  0x11, 0x8e, 0xf9, 0x3c, 0x0b, 0x6a, 0x39, 0xbe, 0xc6, 0x88, 0xea, 0x3e,\n",
            "  0x5b, 0x2b, 0x2a, 0xbf, 0x57, 0x20, 0xbc, 0x3e, 0x78, 0xf1, 0x41, 0x3e,\n",
            "  0x0c, 0x58, 0x9d, 0x3e, 0xcc, 0x84, 0x6a, 0xbe, 0xc8, 0x0f, 0x60, 0xbe,\n",
            "  0xb8, 0x67, 0x69, 0x3e, 0x13, 0x03, 0x95, 0xbe, 0x2e, 0xf4, 0xb7, 0x3e,\n",
            "  0xc2, 0x73, 0x54, 0x3e, 0xae, 0x27, 0x7d, 0x3d, 0xa8, 0xb7, 0xe7, 0x3c,\n",
            "  0xb2, 0x38, 0xa8, 0x3e, 0xde, 0x9f, 0x9d, 0xbe, 0x30, 0x74, 0xaf, 0xbc,\n",
            "  0x91, 0xa2, 0x50, 0x3e, 0xff, 0x95, 0xa0, 0x3e, 0x87, 0x2c, 0x6c, 0x3e,\n",
            "  0x40, 0x07, 0x79, 0x3e, 0xc0, 0x90, 0x46, 0xbd, 0x96, 0x59, 0x0a, 0x3e,\n",
            "  0x3b, 0x96, 0xf6, 0xbd, 0x4c, 0xb9, 0x2d, 0x3e, 0x72, 0xfb, 0x27, 0xbe,\n",
            "  0x06, 0xf7, 0xf3, 0xbb, 0xdf, 0x21, 0x19, 0xbd, 0xff, 0xe5, 0xc5, 0xbe,\n",
            "  0xce, 0x0b, 0x75, 0x3c, 0x98, 0xa7, 0xb4, 0x3e, 0x6a, 0xb6, 0xa5, 0x3e,\n",
            "  0xb7, 0xbf, 0xed, 0xbc, 0x18, 0x16, 0xbc, 0xbe, 0x08, 0x49, 0x2d, 0x3d,\n",
            "  0xcd, 0x7e, 0x80, 0x3e, 0x03, 0x51, 0xa8, 0x3e, 0x31, 0x5d, 0xe0, 0xbe,\n",
            "  0xc6, 0xfb, 0xff, 0xff, 0x10, 0x00, 0x00, 0x00, 0x06, 0x00, 0x00, 0x00,\n",
            "  0x14, 0x00, 0x00, 0x00, 0x24, 0x00, 0x00, 0x00, 0x02, 0x00, 0x00, 0x00,\n",
            "  0x11, 0x00, 0x00, 0x00, 0x1a, 0x00, 0x00, 0x00, 0x0d, 0x00, 0x00, 0x00,\n",
            "  0x73, 0x74, 0x64, 0x2e, 0x63, 0x6f, 0x6e, 0x73, 0x74, 0x61, 0x6e, 0x74,\n",
            "  0x31, 0x00, 0x00, 0x00, 0x4c, 0xfb, 0xff, 0xff, 0x1a, 0xfc, 0xff, 0xff,\n",
            "  0x04, 0x00, 0x00, 0x00, 0x70, 0x02, 0x00, 0x00, 0x3f, 0x17, 0x02, 0x41,\n",
            "  0xd6, 0x1a, 0xcd, 0x3e, 0xbc, 0xe1, 0xbf, 0xbe, 0x3d, 0x12, 0x97, 0x40,\n",
            "  0x64, 0x48, 0xae, 0xbe, 0x0f, 0x8d, 0xb1, 0x3e, 0x9e, 0x8d, 0x81, 0x3f,\n",
            "  0x5d, 0x05, 0x2f, 0xbf, 0x05, 0x9c, 0x49, 0x3e, 0xe6, 0x13, 0x86, 0x40,\n",
            "  0xe7, 0x0e, 0xc2, 0x3c, 0xc7, 0x26, 0x22, 0x3e, 0xf0, 0xc3, 0xd0, 0x3c,\n",
            "  0xd2, 0x46, 0x0f, 0x3e, 0x9a, 0x88, 0x45, 0xbe, 0x30, 0x91, 0x34, 0x3d,\n",
            "  0x62, 0xdd, 0x6a, 0x3e, 0x2a, 0xc9, 0xa9, 0xbe, 0xd3, 0x38, 0xcb, 0x3e,\n",
            "  0xc4, 0x7d, 0x82, 0xbe, 0x61, 0xaf, 0xaa, 0xbe, 0xec, 0xd7, 0xc2, 0x3d,\n",
            "  0x1e, 0xb2, 0xae, 0xbe, 0x94, 0x1c, 0xc7, 0x3d, 0xca, 0x35, 0x45, 0x3e,\n",
            "  0x20, 0x43, 0x7f, 0x3d, 0xf4, 0xce, 0x9d, 0x3d, 0xc0, 0x49, 0x99, 0x3c,\n",
            "  0x00, 0xf6, 0x75, 0xbe, 0xca, 0x17, 0x91, 0xbe, 0x4b, 0xac, 0x12, 0x40,\n",
            "  0x20, 0x39, 0xa5, 0xbe, 0xe7, 0x05, 0xb9, 0x3e, 0xf6, 0x57, 0x01, 0x41,\n",
            "  0xdc, 0xed, 0xcc, 0xbe, 0x80, 0x59, 0x86, 0x3e, 0xb5, 0x57, 0x40, 0xc0,\n",
            "  0x60, 0x8e, 0x8c, 0x3e, 0xa3, 0x8e, 0x10, 0xbf, 0x8f, 0xff, 0x21, 0x40,\n",
            "  0x93, 0x56, 0x58, 0xbe, 0x72, 0x12, 0xc6, 0x3e, 0x80, 0x4b, 0x4b, 0xc1,\n",
            "  0x66, 0xdb, 0x8b, 0x3d, 0x03, 0xb3, 0x62, 0x3d, 0xc9, 0x9a, 0x0d, 0xc1,\n",
            "  0x8d, 0xd4, 0x88, 0xbc, 0x0b, 0xf9, 0x48, 0x3e, 0xf2, 0xd7, 0x3f, 0x3e,\n",
            "  0x90, 0x17, 0x71, 0xbd, 0x04, 0xdd, 0xb1, 0xbd, 0x23, 0xaf, 0xa5, 0xbe,\n",
            "  0xf8, 0xb9, 0xae, 0xbe, 0x4c, 0xa3, 0xdc, 0x3d, 0x4b, 0x68, 0xc6, 0xbe,\n",
            "  0xb8, 0x11, 0x03, 0xbe, 0x0a, 0xb9, 0x95, 0xbe, 0xb0, 0x79, 0x80, 0xbc,\n",
            "  0xa0, 0xc7, 0x9b, 0xbc, 0x6c, 0x48, 0x02, 0xbe, 0xc5, 0xfc, 0xdf, 0xbe,\n",
            "  0x8d, 0x41, 0xdf, 0xbd, 0xae, 0x44, 0x87, 0xbc, 0x2b, 0x66, 0xda, 0x3d,\n",
            "  0x2c, 0xc6, 0xe1, 0xbe, 0x03, 0xfd, 0xa2, 0x3e, 0xc9, 0x63, 0x11, 0x41,\n",
            "  0x88, 0xd6, 0xdb, 0x3e, 0xb6, 0x75, 0x9a, 0xbe, 0x2b, 0x29, 0x15, 0x41,\n",
            "  0xbf, 0xec, 0xaa, 0x3e, 0x9f, 0xf8, 0xb0, 0xbe, 0xca, 0x80, 0x13, 0x41,\n",
            "  0x21, 0x7b, 0xba, 0xbe, 0xa6, 0x07, 0x04, 0xbe, 0xd8, 0xb9, 0xa3, 0x3f,\n",
            "  0x1a, 0x41, 0xb6, 0x3e, 0x35, 0xd4, 0x8b, 0xbb, 0x4c, 0x8b, 0xe8, 0xbf,\n",
            "  0x10, 0x30, 0xa8, 0x3c, 0x96, 0x19, 0xbc, 0xbe, 0xca, 0x98, 0x5b, 0xbf,\n",
            "  0x85, 0x57, 0x18, 0xbe, 0x6e, 0x93, 0x2a, 0x3e, 0x95, 0xd7, 0x2b, 0xbe,\n",
            "  0x0e, 0x29, 0x25, 0x3c, 0xb1, 0x48, 0x27, 0xbe, 0xdf, 0x74, 0x13, 0x3e,\n",
            "  0xfc, 0x85, 0xb0, 0xbe, 0xf5, 0x36, 0x8e, 0x3e, 0x86, 0x48, 0xe0, 0xbd,\n",
            "  0x41, 0x88, 0xa3, 0xbd, 0x1c, 0xfd, 0xfb, 0x3d, 0xe4, 0xb0, 0xb5, 0x3f,\n",
            "  0x75, 0xa5, 0xa7, 0xbe, 0x59, 0x50, 0x43, 0x3e, 0xca, 0xf1, 0x49, 0xbf,\n",
            "  0xec, 0xb1, 0x05, 0xbf, 0xa7, 0x21, 0xb8, 0xbe, 0x67, 0xb1, 0x83, 0xbd,\n",
            "  0x68, 0xff, 0x86, 0x3e, 0x9c, 0x9c, 0xf1, 0x3d, 0x01, 0xee, 0x51, 0xbe,\n",
            "  0x49, 0xf5, 0xbd, 0x3e, 0x79, 0xe5, 0x95, 0xbe, 0xec, 0x38, 0x5c, 0x3e,\n",
            "  0xe4, 0xa9, 0x0c, 0xbe, 0x9d, 0x30, 0x34, 0x3d, 0x02, 0xd1, 0x31, 0x3e,\n",
            "  0x46, 0x01, 0x50, 0x3e, 0x66, 0x37, 0x8f, 0xbe, 0xd1, 0xfb, 0xd0, 0x3e,\n",
            "  0x90, 0xce, 0xc8, 0xbe, 0xbc, 0x29, 0xc5, 0x3d, 0x30, 0x36, 0x1f, 0xbe,\n",
            "  0xca, 0x6a, 0x10, 0x3e, 0x08, 0x0d, 0xe9, 0xbd, 0x80, 0xfc, 0x88, 0xbd,\n",
            "  0x00, 0xa2, 0x8b, 0xbb, 0xc4, 0xda, 0x1b, 0xbe, 0x1b, 0x6d, 0xd6, 0x40,\n",
            "  0x37, 0xc5, 0x44, 0x3e, 0x17, 0x2b, 0x51, 0x3e, 0xed, 0x54, 0xc7, 0x40,\n",
            "  0xb3, 0x1e, 0x67, 0x3e, 0x8f, 0x74, 0xd0, 0xbe, 0x66, 0xa3, 0x65, 0x3e,\n",
            "  0x45, 0xe7, 0x69, 0xbe, 0xfa, 0xd3, 0x18, 0x3e, 0x6c, 0x19, 0x01, 0xbe,\n",
            "  0x18, 0x4e, 0x58, 0x3d, 0xb8, 0x22, 0x80, 0xbd, 0xd2, 0x46, 0x6c, 0x3e,\n",
            "  0xd8, 0xf0, 0xb9, 0xbe, 0xb8, 0x4c, 0x70, 0x3d, 0x26, 0xfb, 0x4a, 0xbe,\n",
            "  0x02, 0xbf, 0x13, 0x3e, 0x88, 0xe0, 0xae, 0xbe, 0xf1, 0x2c, 0x76, 0xbe,\n",
            "  0xe4, 0x16, 0xb6, 0xbe, 0xac, 0xd4, 0x3e, 0x3e, 0xcf, 0x90, 0x59, 0x3e,\n",
            "  0x1f, 0xc0, 0xe2, 0xbe, 0x66, 0x5d, 0xb3, 0x3e, 0x65, 0x8a, 0xdc, 0xbe,\n",
            "  0xc9, 0xa6, 0x03, 0x3d, 0x41, 0xf3, 0x4c, 0x3d, 0x28, 0xb5, 0x32, 0x3d,\n",
            "  0xcd, 0x17, 0xa9, 0xbe, 0x96, 0x3b, 0x30, 0x3e, 0xef, 0xbe, 0x3e, 0x41,\n",
            "  0x66, 0x64, 0xab, 0xbe, 0x8e, 0xf6, 0x95, 0x3e, 0xa6, 0x7d, 0xfc, 0x40,\n",
            "  0xa9, 0x57, 0x3a, 0x3a, 0x59, 0x4b, 0x86, 0x3e, 0x7a, 0xfe, 0xff, 0xff,\n",
            "  0x10, 0x00, 0x00, 0x00, 0x05, 0x00, 0x00, 0x00, 0x14, 0x00, 0x00, 0x00,\n",
            "  0x24, 0x00, 0x00, 0x00, 0x02, 0x00, 0x00, 0x00, 0x1a, 0x00, 0x00, 0x00,\n",
            "  0x06, 0x00, 0x00, 0x00, 0x0c, 0x00, 0x00, 0x00, 0x73, 0x74, 0x64, 0x2e,\n",
            "  0x63, 0x6f, 0x6e, 0x73, 0x74, 0x61, 0x6e, 0x74, 0x00, 0x00, 0x00, 0x00,\n",
            "  0x00, 0xfe, 0xff, 0xff, 0xce, 0xfe, 0xff, 0xff, 0x04, 0x00, 0x00, 0x00,\n",
            "  0x68, 0x00, 0x00, 0x00, 0x9d, 0x39, 0xa5, 0x3f, 0x4e, 0x5e, 0x5a, 0x40,\n",
            "  0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n",
            "  0xa2, 0xa8, 0xd4, 0x3f, 0x22, 0x77, 0x4e, 0x40, 0x85, 0x5a, 0x22, 0xc0,\n",
            "  0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x7d, 0x24, 0x4b, 0xbd,\n",
            "  0xa4, 0x7c, 0x62, 0x40, 0x88, 0x69, 0xeb, 0x3e, 0x0c, 0x98, 0xa1, 0x3d,\n",
            "  0x71, 0xa6, 0x21, 0xbd, 0x85, 0xe9, 0x19, 0x3e, 0xdc, 0x56, 0x5e, 0xbe,\n",
            "  0xe0, 0x8d, 0x87, 0xbc, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n",
            "  0x57, 0xc1, 0xe8, 0x3f, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n",
            "  0xb0, 0xd4, 0x91, 0xbd, 0x09, 0xc3, 0x23, 0xbc, 0xed, 0xf1, 0x05, 0x40,\n",
            "  0x26, 0xff, 0xff, 0xff, 0x10, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00,\n",
            "  0x10, 0x00, 0x00, 0x00, 0x20, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00,\n",
            "  0x1a, 0x00, 0x00, 0x00, 0x0c, 0x00, 0x00, 0x00, 0x64, 0x65, 0x6e, 0x73,\n",
            "  0x65, 0x5f, 0x36, 0x2f, 0x62, 0x69, 0x61, 0x73, 0x00, 0x00, 0x00, 0x00,\n",
            "  0xa8, 0xfe, 0xff, 0xff, 0x76, 0xff, 0xff, 0xff, 0x04, 0x00, 0x00, 0x00,\n",
            "  0x44, 0x00, 0x00, 0x00, 0xa4, 0x55, 0x43, 0xbd, 0xfd, 0xb3, 0x11, 0xbc,\n",
            "  0x00, 0x00, 0x00, 0x00, 0xd3, 0xd5, 0x21, 0xbe, 0x44, 0xeb, 0xf8, 0xbf,\n",
            "  0x7d, 0xda, 0xaf, 0xbb, 0x1f, 0x52, 0xec, 0xbd, 0x2a, 0x5a, 0x02, 0x40,\n",
            "  0xf6, 0x7b, 0x0d, 0xc0, 0x00, 0x00, 0x00, 0x00, 0x57, 0x5e, 0x0c, 0x3f,\n",
            "  0x72, 0xe8, 0x2c, 0xc0, 0x00, 0x00, 0x00, 0x00, 0x74, 0x5b, 0x14, 0xbd,\n",
            "  0xab, 0xf8, 0x8f, 0xbc, 0xce, 0x01, 0x08, 0x40, 0xef, 0xe1, 0x8b, 0xbd,\n",
            "  0xaa, 0xff, 0xff, 0xff, 0x10, 0x00, 0x00, 0x00, 0x03, 0x00, 0x00, 0x00,\n",
            "  0x10, 0x00, 0x00, 0x00, 0x24, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00,\n",
            "  0x11, 0x00, 0x00, 0x00, 0x0c, 0x00, 0x00, 0x00, 0x64, 0x65, 0x6e, 0x73,\n",
            "  0x65, 0x5f, 0x37, 0x2f, 0x62, 0x69, 0x61, 0x73, 0x00, 0x00, 0x00, 0x00,\n",
            "  0x04, 0x00, 0x06, 0x00, 0x04, 0x00, 0x00, 0x00, 0x00, 0x00, 0x06, 0x00,\n",
            "  0x08, 0x00, 0x04, 0x00, 0x06, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00,\n",
            "  0x08, 0x00, 0x00, 0x00, 0x70, 0xe3, 0xee, 0xbf, 0x70, 0xe3, 0xee, 0x3f,\n",
            "  0x00, 0x00, 0x0e, 0x00, 0x14, 0x00, 0x04, 0x00, 0x00, 0x00, 0x08, 0x00,\n",
            "  0x0c, 0x00, 0x10, 0x00, 0x0e, 0x00, 0x00, 0x00, 0x10, 0x00, 0x00, 0x00,\n",
            "  0x02, 0x00, 0x00, 0x00, 0x10, 0x00, 0x00, 0x00, 0x20, 0x00, 0x00, 0x00,\n",
            "  0x01, 0x00, 0x00, 0x00, 0x02, 0x00, 0x00, 0x00, 0x0c, 0x00, 0x00, 0x00,\n",
            "  0x64, 0x65, 0x6e, 0x73, 0x65, 0x5f, 0x38, 0x2f, 0x62, 0x69, 0x61, 0x73,\n",
            "  0x00, 0x00, 0x00, 0x00, 0x90, 0xff, 0xff, 0xff, 0x14, 0x00, 0x18, 0x00,\n",
            "  0x04, 0x00, 0x00, 0x00, 0x08, 0x00, 0x0c, 0x00, 0x10, 0x00, 0x00, 0x00,\n",
            "  0x00, 0x00, 0x14, 0x00, 0x14, 0x00, 0x00, 0x00, 0x14, 0x00, 0x00, 0x00,\n",
            "  0x01, 0x00, 0x00, 0x00, 0x18, 0x00, 0x00, 0x00, 0x44, 0x00, 0x00, 0x00,\n",
            "  0x34, 0x00, 0x00, 0x00, 0x02, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00,\n",
            "  0x06, 0x00, 0x00, 0x00, 0x1f, 0x00, 0x00, 0x00, 0x73, 0x65, 0x72, 0x76,\n",
            "  0x69, 0x6e, 0x67, 0x5f, 0x64, 0x65, 0x66, 0x61, 0x75, 0x6c, 0x74, 0x5f,\n",
            "  0x64, 0x65, 0x6e, 0x73, 0x65, 0x5f, 0x36, 0x5f, 0x69, 0x6e, 0x70, 0x75,\n",
            "  0x74, 0x3a, 0x30, 0x00, 0x02, 0x00, 0x00, 0x00, 0xff, 0xff, 0xff, 0xff,\n",
            "  0x06, 0x00, 0x00, 0x00, 0xfc, 0xff, 0xff, 0xff, 0x04, 0x00, 0x04, 0x00,\n",
            "  0x04, 0x00, 0x00, 0x00\n",
            "};\n",
            "unsigned int models_model_no_quant_tflite_len = 4384;\n"
          ]
        }
      ],
      "source": [
        "# Print the C source file\n",
        "!cat {MODEL_TFLITE_MICRO}"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}